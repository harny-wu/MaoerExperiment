{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042e0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||--------开始时间： 2024-03-13 12:10:10.373156 -------------\n"
     ]
    }
   ],
   "source": [
    "# maoer_data深度学习模型 双层注意力机制\n",
    "# 加上付费label作为输入的gpu版  # 消融实验 去除QOEji及FUFEI部分\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score\n",
    "\n",
    "print('||--------开始时间：',datetime.datetime.now(),'-------------')\n",
    "# data input\n",
    "data_time_windows = '0101_0131'\n",
    "path = './Dataset/' + data_time_windows + '_user_pay_pred_feature_deal.csv'\n",
    "dataset_spilt_path = './Dataset/' + data_time_windows + '_user_pay_pred_feature_spilt.csv'\n",
    "output_weight_result_path = './Dataset/' + data_time_windows + '_user_pay_pred_result_weight.csv'\n",
    "data_feature_continue_discrete_namelist_path = './Dataset/maoer_timewindows_continue_discrete_feature_column.csv'    # 连续与离散划分表\n",
    "\n",
    "# 参数设置\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "num_heads = 10\n",
    "feature_dim = 200\n",
    "max_history_len = 15\n",
    "num_experts = 3\n",
    "num_tasks = 2\n",
    "# 设置嵌入维度\n",
    "continue_embedding_dim = 200\n",
    "discrete_embedding_dim = 200\n",
    "lr = 0.1\n",
    "batch_size = 128\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "# 获取时间窗内连续与离散特征名的列表\n",
    "def get_continue_discrete_feature_namelist(time_windows, datapath):\n",
    "    data = pd.read_csv(datapath)\n",
    "    time_windows_data = data[(data['DataSet'] == time_windows)]\n",
    "    user_history_pay_QOE_continue_column = eval([time_windows_data['QOE_continue'].values.tolist()][0][0])\n",
    "    user_history_pay_CHONGHE_continue_column = eval([time_windows_data['CHONGHE_continue'].values.tolist()][0][0])\n",
    "    user_history_pay_FUFEI_continue_column = eval([time_windows_data['FUFEI_continue'].values.tolist()][0][0])\n",
    "    user_history_pay_QOE_discrete_column = eval([time_windows_data['QOE_discrete'].values.tolist()][0][0])\n",
    "    user_history_pay_CHONGHE_discrete_column = eval([time_windows_data['CHONGHE_discrete'].values.tolist()][0][0])\n",
    "    user_history_pay_FUFEI_discrete_column = eval([time_windows_data['FUFEI_discrete'].values.tolist()][0][0])\n",
    "\n",
    "\n",
    "    return user_history_pay_QOE_continue_column, user_history_pay_CHONGHE_continue_column,user_history_pay_FUFEI_continue_column,\\\n",
    "            user_history_pay_QOE_discrete_column,user_history_pay_CHONGHE_discrete_column,user_history_pay_FUFEI_discrete_column\n",
    "\n",
    "user_feature_continue_column = []\n",
    "user_feature_discrete_column = []\n",
    "# user history bav\n",
    "# user_history_pay_QOE_continue_column = ['user_in_sound_submit_review_num','user_in_sound_submit_danmu_total_len','sound_view_num','sound_danmu_num','sound_review_avg_len','drama_total_sound_num','drama_sound_has_max_cv_num_sound_point_num','drama_upuser_submit_sound_max_view_num','drama_sound_min_time_sound_view_num','pcm_RMSenergy_sma_range numeric','pcm_fftMag_mfcc_sma[1]_maxPos numeric','pcm_fftMag_mfcc_sma[2]_max numeric','pcm_fftMag_mfcc_sma[5]_min numeric','pcm_fftMag_mfcc_sma[8]_maxPos numeric','pcm_fftMag_mfcc_sma[10]_max numeric','voiceProb_sma_stddev numeric','F0_sma_stddev numeric','pcm_fftMag_mfcc_sma_de[4]_min numeric','pcm_fftMag_mfcc_sma_de[6]_maxPos numeric','pcm_fftMag_mfcc_sma_de[8]_kurtosis numeric','pcm_fftMag_mfcc_sma_de[9]_min numeric','pcm_fftMag_mfcc_sma_de[10]_linregc1 numeric']\n",
    "# user_history_pay_CHONGHE_continue_column = ['user_name_len','user_intro_len','user_fish_num','user_follower_num','user_subscribe_drama_num','user_submit_danmu_drama_total_view_num','user_submit_danmu_drama_max_view_num','user_submit_danmu_drama_avg_view_num','user_submit_danmu_drama_total_danmu_num','user_submit_danmu_drama_max_danmu_num','user_submit_danmu_drama_min_danmu_num','user_submit_danmu_drama_avg_danmu_num','user_submit_danmu_drama_min_review_num','user_in_sound_submit_danmu_max_len','user_in_sound_submit_danmu_min_len','user_in_sound_submit_danmu_avg_len','user_in_sound_danmu_around_15s_total_danmu_max_num','user_in_sound_danmu_around_15s_total_danmu_min_num','user_in_sound_danmu_around_15s_total_danmu_avg_num','drama_upuser_submit_sound_avg_danmu_num','pcm_fftMag_mfcc_sma[5]_maxPos numeric','pcm_fftMag_mfcc_sma_de[10]_minPos numeric']\n",
    "# user_history_pay_FUFEI_continue_column = ['user_submit_danmu_drama_min_view_num','user_submit_danmu_drama_max_review_num','user_submit_danmu_drama_avg_review_num','drama_sound_has_min_view_num_sound_favorite_num','drama_sound_max_time_sound_view_num','drama_sound_min_traffic_position_in_sound_avg','pcm_fftMag_mfcc_sma[1]_range numeric','pcm_fftMag_mfcc_sma[1]_minPos numeric','pcm_fftMag_mfcc_sma[2]_min numeric','pcm_fftMag_mfcc_sma[2]_skewness numeric','pcm_fftMag_mfcc_sma[5]_range numeric','pcm_fftMag_mfcc_sma[6]_linregc1 numeric','pcm_fftMag_mfcc_sma[11]_kurtosis numeric','pcm_RMSenergy_sma_de_minPos numeric','pcm_fftMag_mfcc_sma_de[2]_max numeric','pcm_fftMag_mfcc_sma_de[4]_stddev numeric','pcm_fftMag_mfcc_sma_de[4]_skewness numeric','pcm_fftMag_mfcc_sma_de[5]_linregc1 numeric','pcm_fftMag_mfcc_sma_de[8]_amean numeric','pcm_fftMag_mfcc_sma_de[8]_skewness numeric','pcm_fftMag_mfcc_sma_de[9]_max numeric','pcm_fftMag_mfcc_sma_de[10]_linregerrQ numeric','pcm_fftMag_mfcc_sma_de[11]_maxPos numeric','voiceProb_sma_de_max numeric','F0_sma_de_linregerrQ numeric']\n",
    "# user_history_pay_QOE_discrete_column = ['user_name_has_english','user_in_sound_is_submit_review','drama_danmu_time_between_sound_time_in_7days_num_min']\n",
    "# user_history_pay_CHONGHE_discrete_column = ['user_name_has_chinese','user_intro_has_chinese','user_intro_has_english','user_submit_danmu_drama_completed_num_now','sound_title_len','sound_intro_len','sound_danmu_15s_max_traffic_position_in_sound']\n",
    "# user_history_pay_FUFEI_discrete_column = ['drama_intro_len','drama_upuser_subscriptions_num','drama_sound_max_traffic_position_in_sound_avg','label1']\n",
    "\n",
    "# 获取时间窗内连续与离散特征名的列表\n",
    "user_history_pay_QOE_continue_column, user_history_pay_CHONGHE_continue_column, \\\n",
    "        user_history_pay_FUFEI_continue_column, user_history_pay_QOE_discrete_column,\\\n",
    "        user_history_pay_CHONGHE_discrete_column, user_history_pay_FUFEI_discrete_column = get_continue_discrete_feature_namelist(data_time_windows, data_feature_continue_discrete_namelist_path)\n",
    "\n",
    "# total continue feature\n",
    "total_continue_feature = user_feature_continue_column+user_history_pay_QOE_continue_column+user_history_pay_CHONGHE_continue_column+user_history_pay_FUFEI_continue_column\n",
    "total_discrete_feature = user_feature_discrete_column+user_history_pay_QOE_discrete_column+user_history_pay_CHONGHE_discrete_column+user_history_pay_FUFEI_discrete_column\n",
    "total_discrete_feature_add_D = user_feature_discrete_column+user_history_pay_QOE_discrete_column+user_history_pay_CHONGHE_discrete_column+user_history_pay_FUFEI_discrete_column\n",
    "total_discrete_feature_add_D.append('user_in_drama_is_pay_for_drama_in_next_time')\n",
    "user_history_pay_CHONGHE_discrete_column_add_D = copy.deepcopy(user_history_pay_CHONGHE_discrete_column)\n",
    "user_history_pay_CHONGHE_discrete_column_add_D.append('user_in_drama_is_pay_for_drama_in_next_time')\n",
    "tensor_dict_idx = ['pay_QOE_continue','pay_QOE_discrete','pay_CHONGHE_continue','pay_CHONGHE_discrete','pay_FUFEI_continue','pay_FUFEI_discrete','target_QOE_continue','target_QOE_discrete','target_CHONGHE_continue','target_CHONGHE_discrete','target_FUFEI_continue','target_FUFEI_discrete']\n",
    "# print(len(user_history_pay_QOE_continue_column),len(user_history_pay_CHONGHE_continue_column),len(user_history_pay_FUFEI_continue_column))\n",
    "# 形成对应需要的特征名称列表\n",
    "feature_column_dict = {\n",
    "    'user_info_continue': user_feature_continue_column,\n",
    "    'user_info_discrete': user_feature_discrete_column,\n",
    "    # 'history_QOE_continue': user_history_pay_QOE_continue_column,\n",
    "    # 'history_QOE_discrete': user_history_pay_QOE_discrete_column,\n",
    "    'history_CHONGHE_continue': user_history_pay_CHONGHE_continue_column,\n",
    "    'history_CHONGHE_discrete': user_history_pay_CHONGHE_discrete_column,\n",
    "    # 'history_FUFEI_continue': user_history_pay_FUFEI_continue_column,\n",
    "    # 'history_FUFEI_discrete': user_history_pay_FUFEI_discrete_column,\n",
    "    'history_CHONGHE_discrete_add_D': user_history_pay_CHONGHE_discrete_column_add_D\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db025e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分文件已存在，不再进行数据划分\n",
      "数据预处理结束\n",
      "数据划分完成\n"
     ]
    }
   ],
   "source": [
    "# 1.数据处理+划分训练、验证、测试集\n",
    "\n",
    "# 划分数据集 给定输出后固定结果 输出形式定为存储user_id 形成train_dataset,val_dataset,test_dataset\n",
    "def split_data_unique(input_file, output_file, train_ratio, val_ratio, test_ratio):\n",
    "    df = pd.read_csv(input_file)\n",
    "    data = df[df.columns[0]].unique()  # 提取第一列数据并去重\n",
    "    # print(data)\n",
    "    np.random.shuffle(data)  # 随机打乱数据\n",
    "    # 划分数据\n",
    "    total_len = len(data)\n",
    "    x_end = int(total_len * train_ratio)\n",
    "    y_end = x_end + int(total_len * val_ratio)\n",
    "    train_data = data[:x_end]\n",
    "    val_data = data[x_end:y_end]\n",
    "    test_data = data[y_end:]\n",
    "    # 存储结果是去重的user_id\n",
    "    result = {\n",
    "        'Train': train_data,\n",
    "        'Val': val_data,\n",
    "        'Test': test_data\n",
    "    }   \n",
    "    # 创建每个子集的DataFrame  \n",
    "    train_df = pd.DataFrame(train_data, columns=['Train'])\n",
    "    val_df = pd.DataFrame(val_data, columns=['Val'])\n",
    "    test_df = pd.DataFrame(test_data, columns=['Test'])\n",
    "    # 将每个DataFrame转换为一列Series  \n",
    "    train_series = train_df.iloc[:, 0]\n",
    "    val_series = val_df.iloc[:, 0]\n",
    "    test_series = test_df.iloc[:, 0]\n",
    "    # 为了确保所有Series有相同的长度，我们需要找到最大长度并截断较短的Series  \n",
    "    max_len = max(len(train_series), len(val_series), len(test_series))\n",
    "    train_series = train_series.reindex(range(max_len)).fillna(value=pd.NA)\n",
    "    val_series = val_series.reindex(range(max_len)).fillna(value=pd.NA)\n",
    "    test_series = test_series.reindex(range(max_len)).fillna(value=pd.NA)\n",
    "    # 创建一个新的DataFrame，将Series作为列  \n",
    "    combined_df = pd.DataFrame({\n",
    "        'Train': train_series,\n",
    "        'Val': val_series,\n",
    "        'Test': test_series\n",
    "    })\n",
    "    # 写入CSV文件，不包含索引和列名  \n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print('已输出划分数据集结果')\n",
    "\n",
    "# 数据预处理 将连续特征变离散特征 分桶 不处理user_id、sound_id、drama_id、time\n",
    "def data_pre_deal(input_path,continue_feature_list):\n",
    "    df = pd.read_csv(input_path)\n",
    "    deal_data_df = [] # 待修改********\n",
    "    # # 获取离散特征的类别数量，并存储为字典\n",
    "    # category_counts = {}\n",
    "    # for column in deal_data_df.columns:\n",
    "    #     unique_values = deal_data_df[column].nunique()  # 获取列的唯一值数量\n",
    "    #     category_counts[column] = unique_values\n",
    "    print('数据预处理结束')\n",
    "    return df\n",
    "\n",
    "# 根据划分好的数据集中user_id 找到对应csv文件中对应user_id的所有行数据取出，即包含了历史数据（付费+非付费）+目标数据（最后一次行为）\n",
    "# def find_data_by_list(user_list, intput_data_df, data_hash):\n",
    "#     df = intput_data_df\n",
    "#     # result_list = []\n",
    "#     # 遍历列表中的值，在CSV文件中找到所有匹配的行数据并加入结果列表\n",
    "#     for user_id in user_list:\n",
    "#         result_df = df[df[df.columns[0]] == user_id]\n",
    "#         # result_list.append(result_df)\n",
    "#         if user_id in data_hash:\n",
    "#             data_hash[user_id].update({col: result_df for col in df.columns})  # 使用列名作为键\n",
    "#         else:\n",
    "#             data_hash[user_id] = {col: result_df for col in df.columns}\n",
    "#     #result = pd.concat(result_list)  # 合并所有匹配的行数据\n",
    "#     return data_hash\n",
    "\n",
    "def find_data_by_list(user_list, intput_data_df, data_hash):  \n",
    "    # 遍历列表中的值，在DataFrame中找到所有匹配的行数据并加入data_hash  \n",
    "    for user_id in user_list:  \n",
    "        result_df = intput_data_df[intput_data_df[intput_data_df.columns[0]] == user_id]  \n",
    "        data_hash[user_id] = result_df  # 直接存储DataFrame对象  \n",
    "    return data_hash\n",
    "    \n",
    "# 获取列唯一值数量表，并对离散特征的值转化为从0开始的索引\n",
    "def get_unique_feature_num_and_discrete_valueChange(datadf,discrete_feature_column_list):\n",
    "    # 获取离散特征的类别数量，并存储为字典\n",
    "    feature_category_num_dict = {}\n",
    "    for column in datadf.columns:\n",
    "        unique_values_len = datadf[column].nunique()  # 获取列的唯一值数量\n",
    "        feature_category_num_dict[column] = unique_values_len\n",
    "        if column in discrete_feature_column_list:\n",
    "            unique_values = datadf[column].unique()\n",
    "            value_mapping_dict = {value: index for index, value in enumerate(unique_values) if\n",
    "                              value != -1 and value != '' and value is not None}\n",
    "            datadf[column] = datadf[column].map(value_mapping_dict)\n",
    "    return feature_category_num_dict,datadf\n",
    "\n",
    "# 总的特征输入，生成划分后数据集及其输入\n",
    "def data_input(data_time_windows, path, spilt_outpath, train_ratio, val_ratio, test_ratio, total_continue_feature):\n",
    "    dataset_path = path  # 待修改********\n",
    "    dataset_spilt_path = spilt_outpath  # 待修改********\n",
    "    if os.path.exists(dataset_spilt_path):  # 划分训练、验证、测试集\n",
    "        print(\"划分文件已存在，不再进行数据划分\")\n",
    "    else:\n",
    "        split_data_unique(dataset_path, dataset_spilt_path, train_ratio, val_ratio, test_ratio)\n",
    "    deal_data_df = data_pre_deal(dataset_path, total_continue_feature)  # 数据预处理\n",
    "    # 获取离散特征的类别数量，并存储为字典\n",
    "    feature_category_num_dict,deal_data_df  = get_unique_feature_num_and_discrete_valueChange(deal_data_df,total_discrete_feature)\n",
    "    # 读取划分文件的结果\n",
    "    spilt_data_df = pd.read_csv(dataset_spilt_path)\n",
    "    # 输出每一列数据为列表\n",
    "    train_list = spilt_data_df['Train'].tolist()\n",
    "    val_list = spilt_data_df['Val'].tolist()\n",
    "    test_list = spilt_data_df['Test'].tolist()\n",
    "    train_list = [x for x in train_list if not math.isnan(x)]\n",
    "    val_list = [x for x in val_list if not math.isnan(x)]\n",
    "    test_list = [x for x in test_list if not math.isnan(x)]\n",
    "    # print('训练集、验证集、测试集大小=', len(train_list),len(val_list),len(test_list))\n",
    "    # 根据划分好的生成以user_id为key的hash（特征集合）将最后一行看做目标数据\n",
    "    data_hash = {}  # 存成一个hash形式\n",
    "    find_data_by_list(train_list, deal_data_df, data_hash)\n",
    "    find_data_by_list(val_list, deal_data_df, data_hash)\n",
    "    find_data_by_list(test_list, deal_data_df, data_hash)\n",
    "    print('数据划分完成')\n",
    "    # print(feature_category_num_dict)\n",
    "    return train_list, val_list, test_list, data_hash, feature_category_num_dict\n",
    "\n",
    "# test\n",
    "# 数据集 train、val、test划分及总数据hash表(以user_id为key的存储对应对应行的hash表)及不同类特征数存储的字典\n",
    "train_list, val_list, test_list, data_hash, feature_category_num_dict = data_input(data_time_windows, path,dataset_spilt_path, train_ratio, val_ratio, test_ratio, total_continue_feature)\n",
    "# print(data_hash[3617476])\n",
    "# print(feature_category_num_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59420a1d-a80d-4242-80c7-bc15ea0497dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 形成张量矩阵 目标特征为：（batch,1,feature_num; 用户历史行为特征为（batch,max_history_len(固定长度的历史记录数),feature_num）\n",
    "\n",
    "# mask 对用户历史行为长度的固定\n",
    "# 转换 history 列为长度为max_history_len的数组\n",
    "def process_history(history, max_history_len):\n",
    "    if len(history) >= max_history_len:\n",
    "        processed_history = history[-max_history_len:]\n",
    "    else:\n",
    "        processed_history = [-1] * (max_history_len - len(history)) + history\n",
    "    return processed_history\n",
    "# 将填充-1的位置标记为True\n",
    "def create_mask(history):\n",
    "    mask = [True if item == -1 else False for item in history]\n",
    "    return mask\n",
    "# 将历史行为记录处理为固定长度并进行mask\n",
    "def history_feature_mask(user_history_feature_index, data_matrix_user_history, max_history_len):\n",
    "    mask_history_feature_matrix = []\n",
    "    origin_history_feature_matrix = []\n",
    "    for feature_index in range(len(user_history_feature_index)):\n",
    "        feature_data = [data_row[feature_index] for data_row in data_matrix_user_history]  # 获取一列特征值\n",
    "        processed_feature_data = process_history(feature_data, max_history_len)  # 处理为固定长度 max_history_len\n",
    "        origin_history_feature_matrix.append(processed_feature_data)\n",
    "        mask_feature_data = create_mask(processed_feature_data)  # 将空的mask\n",
    "        mask_history_feature_matrix.append(mask_feature_data)\n",
    "        \n",
    "    # print('mask',len(origin_history_feature_matrix),len(origin_history_feature_matrix[0]))\n",
    "    return origin_history_feature_matrix, mask_history_feature_matrix\n",
    "\n",
    "# 将输入形成的data_hash和连续、离散特征列名,按照划分的训练或测试的user_id的列表，提取用户特征形成张量矩阵存储到data_tensor_hash中，以user_id为key，多个张量矩阵为value\n",
    "def get_feature_to_matrix(train_or_val_or_test_list, data_hash, feature_column_dict):\n",
    "    # 存储新的张量hash\n",
    "    data_tensor_hash = {}\n",
    "    # 存储历史记录的掩码矩阵\n",
    "    data_tensor_history_mask_hash = {}\n",
    "    target_label = []  # 预测目标值的标签\n",
    "\n",
    "    for user_id in train_or_val_or_test_list:\n",
    "        user_data = data_hash[user_id]\n",
    "        # 创建空的二维矩阵\n",
    "        # data_matrix_user_info_continue = []\n",
    "        # data_matrix_user_info_discrete = []\n",
    "        data_matrix_pay_QOE_continue = []\n",
    "        data_matrix_pay_QOE_discrete = []\n",
    "        data_matrix_pay_CHONGHE_continue = []\n",
    "        data_matrix_pay_CHONGHE_discrete = []\n",
    "        data_matrix_pay_FUFEI_continue = []\n",
    "        data_matrix_pay_FUFEI_discrete = []\n",
    "        data_matrix_target_QOE_continue = []\n",
    "        data_matrix_target_CHONGHE_continue = []\n",
    "        data_matrix_target_FUFEI_continue = []\n",
    "        data_matrix_target_QOE_discrete = []\n",
    "        data_matrix_target_CHONGHE_discrete = []\n",
    "        data_matrix_target_FUFEI_discrete = []\n",
    "        # 提取特征列对应的索引\n",
    "        # user_feature_continue_index = [user_data.columns.get_loc(col) for col in feature_column_dict['user_info_continue'] if col in user_data.columns]\n",
    "        # user_feature_discrete_index = [user_data.columns.get_loc(col) for col in feature_column_dict['user_info_discrete'] if\n",
    "        #                                col in user_data.columns]\n",
    "        # user_history_QOE_continue_index = [user_data.columns.get_loc(col) for col in feature_column_dict['history_QOE_continue'] if\n",
    "        #                                col in user_data.columns]\n",
    "        # user_history_QOE_discrete_index = [user_data.columns.get_loc(col) for col in feature_column_dict['history_QOE_discrete'] if\n",
    "        #                                col in user_data.columns]\n",
    "        user_history_CHONGHE_continue_index = [user_data.columns.get_loc(col) for col in\n",
    "                                           feature_column_dict['history_CHONGHE_continue'] if\n",
    "                                           col in user_data.columns]\n",
    "        user_history_CHONGHE_discrete_index = [user_data.columns.get_loc(col) for col in\n",
    "                                           feature_column_dict['history_CHONGHE_discrete'] if\n",
    "                                           col in user_data.columns]\n",
    "        # user_history_FUFEI_continue_index = [user_data.columns.get_loc(col) for col in\n",
    "        #                                    feature_column_dict['history_FUFEI_continue'] if\n",
    "        #                                    col in user_data.columns]\n",
    "        # user_history_FUFEI_discrete_index = [user_data.columns.get_loc(col) for col in\n",
    "        #                                    feature_column_dict['history_FUFEI_discrete'] if\n",
    "        #                                    col in user_data.columns]\n",
    "        user_history_CHONGHE_discrete_add_D_index = [user_data.columns.get_loc(col) for col in feature_column_dict['history_CHONGHE_discrete_add_D'] if col in user_data.columns]\n",
    "        \n",
    "        # 填充数据矩阵\n",
    "        for i in range(len(user_data)):\n",
    "            if i != (len(user_data) - 1):  # 除最后一行即所有历史记录，不包括目标记录\n",
    "                    # data_matrix_pay_QOE_continue.append(\n",
    "                    #     [user_data.iloc[i, col] for col in user_history_QOE_continue_index])  # 用户历史QOE连续特征\n",
    "                    # data_matrix_pay_QOE_discrete.append(\n",
    "                    #     [user_data.iloc[i, col] for col in user_history_QOE_discrete_index])  # 用户历史QOE离散特征\n",
    "                    data_matrix_pay_CHONGHE_continue.append(\n",
    "                        [user_data.iloc[i, col] for col in user_history_CHONGHE_continue_index])  # 用户历史CHONGHE连续特征\n",
    "                    data_matrix_pay_CHONGHE_discrete.append(\n",
    "                        [user_data.iloc[i, col] for col in user_history_CHONGHE_discrete_add_D_index])  # 用户历史CHONGHE离散特征\n",
    "                    # data_matrix_pay_FUFEI_continue.append(\n",
    "                    #     [user_data.iloc[i, col] for col in user_history_FUFEI_continue_index])  # 用户历史FUFEI连续特征\n",
    "                    # data_matrix_pay_FUFEI_discrete.append(\n",
    "                    #     [user_data.iloc[i, col] for col in user_history_FUFEI_discrete_index])  # 用户历史FUFEI离散特征\n",
    "            else:   # 目标记录\n",
    "                # data_matrix_user_info_continue.append([user_data.iloc[i, col] for col in user_feature_continue_index])  # 用户连续特征\n",
    "                # data_matrix_user_info_discrete.append([user_data.iloc[i, col] for col in user_feature_discrete_index])  # 用户离散特征\n",
    "                target_label.append(user_data.iloc[i, -1])  # 预测目标的y值\n",
    "                # data_matrix_target_QOE_continue.append([user_data.iloc[i, col] for col in user_history_QOE_continue_index])  # 目标QOE连续特征\n",
    "                # data_matrix_target_QOE_discrete.append([user_data.iloc[i, col] for col in user_history_QOE_discrete_index])  # 目标QOE离散特征\n",
    "                data_matrix_target_CHONGHE_continue.append([user_data.iloc[i, col] for col in user_history_CHONGHE_continue_index])  # 目标CHONGHE连续特征\n",
    "                data_matrix_target_CHONGHE_discrete.append([user_data.iloc[i, col] for col in user_history_CHONGHE_discrete_index])  # 目标CHONGHE离散特征\n",
    "                # data_matrix_target_FUFEI_continue.append([user_data.iloc[i, col] for col in user_history_FUFEI_continue_index])  # 目标FUFEI连续特征\n",
    "                # data_matrix_target_FUFEI_discrete.append([user_data.iloc[i, col] for col in user_history_FUFEI_discrete_index])  # 目标FUFEI离散特征\n",
    "        # print('data_matrix_pay_QOE_continue:', len(data_matrix_pay_QOE_continue),len(data_matrix_pay_QOE_continue[0]))\n",
    "        # print(len(data_matrix_target_QOE_continue),len(data_matrix_target_QOE_continue[0]))\n",
    "        # 将历史行为记录处理为固定长度并进行mask\n",
    "        # data_matrix_pay_QOE_continue,data_matrix_pay_QOE_continue_mask = history_feature_mask(user_history_QOE_continue_index, data_matrix_pay_QOE_continue, max_history_len)\n",
    "        # data_matrix_pay_QOE_discrete,data_matrix_pay_QOE_discrete_mask = history_feature_mask(user_history_QOE_discrete_index, data_matrix_pay_QOE_discrete, max_history_len)\n",
    "        data_matrix_pay_CHONGHE_continue,data_matrix_pay_CHONGHE_continue_mask = history_feature_mask(user_history_CHONGHE_continue_index, data_matrix_pay_CHONGHE_continue,max_history_len)\n",
    "        data_matrix_pay_CHONGHE_discrete,data_matrix_pay_CHONGHE_discrete_mask = history_feature_mask(user_history_CHONGHE_discrete_add_D_index, data_matrix_pay_CHONGHE_discrete,max_history_len)\n",
    "        # data_matrix_pay_FUFEI_continue,data_matrix_pay_FUFEI_continue_mask = history_feature_mask(user_history_FUFEI_continue_index, data_matrix_pay_FUFEI_continue,max_history_len)\n",
    "        # data_matrix_pay_FUFEI_discrete,data_matrix_pay_FUFEI_discrete_mask = history_feature_mask(user_history_FUFEI_discrete_index, data_matrix_pay_FUFEI_discrete,max_history_len)\n",
    "        # print('data_matrix_pay_QOE_discrete',len(data_matrix_pay_QOE_discrete),len(data_matrix_pay_QOE_discrete[0]))\n",
    "        # print('(ata_matrix_pay_QOE_discrete',data_matrix_pay_QOE_discrete[0])\n",
    "\n",
    "        # 将numpy数组转换为PyTorch张量       # history   得到的data_matrix_user_history及data_tensor_pay_QOE_continue维度是(feature_num,history_len)需要转成tensor后转置\n",
    "        # data_tensor_pay_QOE_continue = torch.transpose(torch.tensor(np.array(data_matrix_pay_QOE_continue), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_QOE_discrete = torch.tensor(np.array(data_matrix_pay_QOE_discrete), dtype=torch.float32)\n",
    "        # print('data_tensor_pay_QOE_discrete1',data_tensor_pay_QOE_discrete[0,:])\n",
    "        # data_tensor_pay_QOE_discrete = torch.transpose(torch.tensor(np.array(data_matrix_pay_QOE_discrete), dtype=torch.float32),-2,-1)\n",
    "        # print('data_tensor_pay_QOE_discrete2',data_tensor_pay_QOE_discrete[0,:])\n",
    "        # print('data_tensor_pay_QOE_discrete3',data_tensor_pay_QOE_discrete[:,0])\n",
    "        data_tensor_pay_CHONGHE_continue = torch.transpose(torch.tensor(np.array(data_matrix_pay_CHONGHE_continue), dtype=torch.float32),-2,-1)\n",
    "        data_tensor_pay_CHONGHE_discrete = torch.transpose(torch.tensor(np.array(data_matrix_pay_CHONGHE_discrete), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_FUFEI_continue = torch.transpose(torch.tensor(np.array(data_matrix_pay_FUFEI_continue), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_FUFEI_discrete = torch.transpose(torch.tensor(np.array(data_matrix_pay_FUFEI_discrete), dtype=torch.float32),-2,-1)\n",
    "        #  mask矩阵   得到的data_matrix_user_history及data_tensor_pay_QOE_continue维度是(feature_num,history_len)需要转成tensor后转置\n",
    "        # data_tensor_pay_QOE_continue_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_QOE_continue_mask), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_QOE_discrete_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_QOE_discrete_mask), dtype=torch.float32),-2,-1)\n",
    "        data_tensor_pay_CHONGHE_continue_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_CHONGHE_continue_mask), dtype=torch.float32),-2,-1)\n",
    "        data_tensor_pay_CHONGHE_discrete_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_CHONGHE_discrete_mask), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_FUFEI_continue_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_FUFEI_continue_mask), dtype=torch.float32),-2,-1)\n",
    "        # data_tensor_pay_FUFEI_discrete_mask = torch.transpose(torch.tensor(np.array(data_matrix_pay_FUFEI_discrete_mask), dtype=torch.float32),-2,-1)\n",
    "        # user + target   输出维度为（1，feature_num）,一处第一个为1的维度变为（feature_num）\n",
    "        # data_tensor_user_info_continue = torch.tensor(np.array(data_matrix_user_info_continue), dtype=torch.float32)\n",
    "        # data_tensor_user_info_discrete = torch.tensor(np.array(data_matrix_user_info_discrete), dtype=torch.float32)\n",
    "        # data_tensor_target_QOE_continue = torch.squeeze(torch.tensor(np.array(data_matrix_target_QOE_continue), dtype=torch.float32),dim=0)\n",
    "        # data_tensor_target_QOE_discrete = torch.squeeze(torch.tensor(np.array(data_matrix_target_QOE_discrete), dtype=torch.float32),dim=0)\n",
    "        data_tensor_target_CHONGHE_continue = torch.squeeze(torch.tensor(np.array(data_matrix_target_CHONGHE_continue),dtype=torch.float32),dim=0)\n",
    "        data_tensor_target_CHONGHE_discrete = torch.squeeze(torch.tensor(np.array(data_matrix_target_CHONGHE_discrete),dtype=torch.float32),dim=0)\n",
    "        # data_tensor_target_FUFEI_continue = torch.squeeze(torch.tensor(np.array(data_matrix_target_FUFEI_continue), dtype=torch.float32),dim=0)\n",
    "        # data_tensor_target_FUFEI_discrete = torch.squeeze(torch.tensor(np.array(data_matrix_target_FUFEI_discrete), dtype=torch.float32),dim=0)\n",
    "        \n",
    "        # 生成hash值，按user_id为key存储成hash\n",
    "        tensor_hash_value = {\n",
    "            # 'pay_QOE_continue': data_tensor_pay_QOE_continue,\n",
    "            # 'pay_QOE_discrete': data_tensor_pay_QOE_discrete,\n",
    "            'pay_CHONGHE_continue': data_tensor_pay_CHONGHE_continue,\n",
    "            'pay_CHONGHE_discrete': data_tensor_pay_CHONGHE_discrete,\n",
    "            # 'pay_FUFEI_continue': data_tensor_pay_FUFEI_continue,\n",
    "            # 'pay_FUFEI_discrete': data_tensor_pay_FUFEI_discrete,\n",
    "            # 'target_QOE_continue': data_tensor_target_QOE_continue,\n",
    "            # 'target_QOE_discrete': data_tensor_target_QOE_discrete,\n",
    "            'target_CHONGHE_continue': data_tensor_target_CHONGHE_continue,\n",
    "            'target_CHONGHE_discrete': data_tensor_target_CHONGHE_discrete,\n",
    "            # 'target_FUFEI_continue': data_tensor_target_FUFEI_continue,\n",
    "            # 'target_FUFEI_discrete': data_tensor_target_FUFEI_discrete\n",
    "        }\n",
    "        tensor_hash_value_history_mask = {\n",
    "            # 'pay_QOE_continue': data_tensor_pay_QOE_continue_mask,\n",
    "            # 'pay_QOE_discrete': data_tensor_pay_QOE_discrete_mask,\n",
    "            'pay_CHONGHE_continue': data_tensor_pay_CHONGHE_continue_mask,\n",
    "            'pay_CHONGHE_discrete': data_tensor_pay_CHONGHE_discrete_mask,\n",
    "            # 'pay_FUFEI_continue': data_tensor_pay_FUFEI_continue_mask,\n",
    "            # 'pay_FUFEI_discrete': data_tensor_pay_FUFEI_discrete_mask,\n",
    "        }\n",
    "        if user_id in data_tensor_hash:\n",
    "            data_tensor_hash[user_id].update(tensor_hash_value)\n",
    "            data_tensor_history_mask_hash[user_id].update(tensor_hash_value_history_mask)\n",
    "        else:\n",
    "            data_tensor_hash[user_id] = tensor_hash_value\n",
    "            data_tensor_history_mask_hash[user_id] = tensor_hash_value_history_mask\n",
    "    \n",
    "    # 如果需要合并成一个张量，可以使用torch.cat方法\n",
    "    # combined_tensor = torch.cat((data_matrix_1_tensor, data_matrix_2_tensor), dim=1)\n",
    "    # data_tensor_hash中用户历史的输出维度(max_history_len,feature_num)，目标的输出维度是（feature_num）\n",
    "    return data_tensor_hash, target_label, data_tensor_history_mask_hash   \n",
    "\n",
    "\n",
    "# 张量矩阵添加一个batch维度，并在用户特征与目标特征的张量中再添加一维使其与用户历史行为张量对齐， 形成两种：\n",
    "# 原数据为：1.用户特征与目标特征都为：（1,feature_num）; 2.用户历史行为特征为（max_history_len(固定长度的历史记录数),feature_num）\n",
    "# 新数据为：1.用户特征与目标特征都为：（batch,1,1,feature_num); 2.用户历史行为特征为（batch,max_history_len(固定长度的历史记录数),feature_num）\n",
    "# 形成batch维度的特征\n",
    "def generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, feature_category):  # 例:feature_category = 'user_info_continue' 就是上面生成的tensor_hash_value字典的键\n",
    "    tensor_list = []\n",
    "    for user_id in train_or_val_or_test_list:  # 遍历data_tensor_hash的所有key (user_id)\n",
    "        if feature_category in data_tensor_hash[user_id]:\n",
    "            tensor = data_tensor_hash[user_id][feature_category]  # 获取feature_category对应的张量\n",
    "            tensor_list.append(tensor)  # 添加到tensor_list中\n",
    "    #  print(tensor_list)\n",
    "    batch_feature_tensor = torch.stack(tensor_list, dim=0)  # 在第一个维度上合并所有张量(其实相当于生成一个新维度)\n",
    "    return batch_feature_tensor\n",
    "# 生成batch再添加维度对齐张量（三个维度）\n",
    "def generate_user_feature_alignment_tensor(train_or_val_or_test_list,data_tensor_hash,is_mask=False):\n",
    "    # 用户历史行为矩阵（max_history_len(固定长度的历史记录数),feature_num）->（batch,max_history_len(固定长度的历史记录数),feature_num）\n",
    "    # pay_QOE_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_QOE_continue')\n",
    "    # pay_QOE_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_QOE_discrete')\n",
    "    pay_CHONGHE_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_CHONGHE_continue')\n",
    "    pay_CHONGHE_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_CHONGHE_discrete')\n",
    "    # pay_FUFEI_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_FUFEI_continue')\n",
    "    # pay_FUFEI_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'pay_FUFEI_discrete')\n",
    "    # print('pay_QOE_discrete_batch_feature_tensor1',pay_QOE_discrete_batch_feature_tensor[0,:,0])\n",
    "    # 看是否是掩码矩阵，不是则xxx，是则没有user+target\n",
    "    if is_mask==False:\n",
    "        # 用户矩阵 (feature_num) ->(batch,feature_num)\n",
    "        # user_info_continue_batch_feature_tensor = generate_batch_feature(data_tensor_hash, 'user_info_continue')\n",
    "        # user_info_discrete_batch_feature_tensor = generate_batch_feature(data_tensor_hash, 'user_info_discrete')\n",
    "        # 目标矩阵 (feature_num) ->(batch,feature_num)\n",
    "        # target_QOE_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_QOE_continue')\n",
    "        # target_QOE_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_QOE_discrete')\n",
    "        target_CHONGHE_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_CHONGHE_continue')\n",
    "        target_CHONGHE_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_CHONGHE_discrete')\n",
    "        # target_FUFEI_continue_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_FUFEI_continue')\n",
    "        # target_FUFEI_discrete_batch_feature_tensor = generate_batch_feature(train_or_val_or_test_list,data_tensor_hash, 'target_FUFEI_discrete')\n",
    "\n",
    "        # 假设原始张量矩阵为 tensor，形状为 (batch_size, feature_num)将其加一个维度变为 (batch_size, 1, feature_num)\n",
    "        # user_info_continue_batch_feature_tensor = torch.unsqueeze(user_info_continue_batch_feature_tensor, dim=1)\n",
    "        # user_info_discrete_batch_feature_tensor = torch.unsqueeze(user_info_discrete_batch_feature_tensor, dim=1)\n",
    "        # target_QOE_continue_batch_feature_tensor = torch.unsqueeze(target_QOE_continue_batch_feature_tensor, dim=1)\n",
    "        # target_QOE_discrete_batch_feature_tensor = torch.unsqueeze(target_QOE_discrete_batch_feature_tensor, dim=1)\n",
    "        target_CHONGHE_continue_batch_feature_tensor = torch.unsqueeze(target_CHONGHE_continue_batch_feature_tensor, dim=1)\n",
    "        target_CHONGHE_discrete_batch_feature_tensor = torch.unsqueeze(target_CHONGHE_discrete_batch_feature_tensor, dim=1)\n",
    "        # target_FUFEI_continue_batch_feature_tensor = torch.unsqueeze(target_FUFEI_continue_batch_feature_tensor, dim=1)\n",
    "        # target_FUFEI_discrete_batch_feature_tensor = torch.unsqueeze(target_FUFEI_discrete_batch_feature_tensor, dim=1)\n",
    "\n",
    "        batch_feature_tensor_dict = {\n",
    "            # 'pay_QOE_discrete': pay_QOE_discrete_batch_feature_tensor,\n",
    "            'pay_CHONGHE_discrete': pay_CHONGHE_discrete_batch_feature_tensor,\n",
    "            # 'pay_FUFEI_discrete': pay_FUFEI_discrete_batch_feature_tensor,\n",
    "            # 'pay_QOE_continue': pay_QOE_continue_batch_feature_tensor,\n",
    "            'pay_CHONGHE_continue': pay_CHONGHE_continue_batch_feature_tensor,\n",
    "            # 'pay_FUFEI_continue': pay_FUFEI_continue_batch_feature_tensor,\n",
    "            # 'target_QOE_discrete': target_QOE_discrete_batch_feature_tensor,\n",
    "            'target_CHONGHE_discrete': target_CHONGHE_discrete_batch_feature_tensor,\n",
    "            # 'target_FUFEI_discrete': target_FUFEI_discrete_batch_feature_tensor,       \n",
    "            # 'target_QOE_continue': target_QOE_continue_batch_feature_tensor,\n",
    "            'target_CHONGHE_continue': target_CHONGHE_continue_batch_feature_tensor,\n",
    "            # 'target_FUFEI_continue': target_FUFEI_continue_batch_feature_tensor,\n",
    "            \n",
    "        }\n",
    "    else:\n",
    "        batch_feature_tensor_dict = {\n",
    "            # 'pay_QOE_discrete': pay_QOE_discrete_batch_feature_tensor,\n",
    "            'pay_CHONGHE_discrete': pay_CHONGHE_discrete_batch_feature_tensor,\n",
    "            # 'pay_FUFEI_discrete': pay_FUFEI_discrete_batch_feature_tensor,\n",
    "            # 'pay_QOE_continue': pay_QOE_continue_batch_feature_tensor,\n",
    "            'pay_CHONGHE_continue': pay_CHONGHE_continue_batch_feature_tensor,\n",
    "            # 'pay_FUFEI_continue': pay_FUFEI_continue_batch_feature_tensor,\n",
    "        }\n",
    "    return batch_feature_tensor_dict  # 这里张量输出的全是三维 (batch_size, 1 or max_history_len, feature_num)\n",
    "\n",
    "\n",
    "# 由于模型输入得是张量，因此在之前将字典转化为了张量，现在将它转换回去\n",
    "class TensorDatasettoDict(Dataset):\n",
    "    def __init__(self, dataset, keys):\n",
    "        self.dataset = dataset\n",
    "        self.keys = keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        sample = {key: data[i] for i, key in enumerate(self.keys)}\n",
    "        return sample\n",
    "\n",
    "# test\n",
    "# 获取训练、验证、测试集对应的数据形成的向量hash存储及label\n",
    "# print(data_hash[3617476])\n",
    "# train_data_tensor_hash, train_label, train_data_tensor_hash_history_mask = get_feature_to_matrix(train_list, data_hash, feature_column_dict)\n",
    "# first_key = list(train_data_tensor_hash.keys())[0]\n",
    "# print(train_data_tensor_hash[first_key]['pay_QOE_discrete'][:,0])\n",
    "# print(train_label)\n",
    "# # print(train_data_tensor_hash[3617476])\n",
    "# dimensions1 = train_data_tensor_hash[3617476]['pay_QOE_continue'].size()\n",
    "# dimensions2 = train_data_tensor_hash[3617476]['pay_QOE_discrete'].size()\n",
    "# dimensions3 = train_data_tensor_hash[3617476]['pay_CHONGHE_continue'].size()\n",
    "# dimensions4 = train_data_tensor_hash[3617476]['target_QOE_continue'].size()\n",
    "# dimensions5 = train_data_tensor_hash[3617476]['target_QOE_discrete'].size()\n",
    "# dimensions6 = train_data_tensor_hash[3617476]['target_CHONGHE_continue'].size()\n",
    "# print(\"PyTorch张量的维度：\", dimensions1,dimensions2,dimensions3,dimensions4,dimensions5,dimensions6)\n",
    "# train_batch_feature_tensor_dict = generate_user_feature_alignment_tensor(train_list,train_data_tensor_hash)\n",
    "# train_batch_feature_tensor_history_mask_dict = generate_user_feature_alignment_tensor(train_data_tensor_hash_history_mask,is_mask=True)\n",
    "# print(train_batch_feature_tensor_dict['pay_QOE_discrete'][0,:,0])\n",
    "# dimensions1 = train_data_tensor_hash[3617476]['pay_QOE_continue'].size()\n",
    "# dimensions2 = train_data_tensor_hash[3617476]['pay_QOE_discrete'].size()\n",
    "# dimensions3 = train_data_tensor_hash[3617476]['pay_CHONGHE_continue'].size()\n",
    "# dimensions4 = train_data_tensor_hash[3617476]['target_QOE_continue'].size()\n",
    "# dimensions5 = train_data_tensor_hash[3617476]['target_QOE_discrete'].size()\n",
    "# dimensions6 = train_data_tensor_hash[3617476]['target_CHONGHE_continue'].size()\n",
    "# print(\"PyTorch添加batch后张量的维度：\", dimensions1,dimensions2,dimensions3,dimensions4,dimensions5,dimensions6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6535f848-5c74-494a-84e9-884c8f6d42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.基础模型 embedding、attention\n",
    "# 构建离散特征的embedding\n",
    "def discrete_embedding(feature_category_num_dict, feature_column_name_list, embedding_dim): # 输入特征取值大小的集合,特征数,维度\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    embeddings = []\n",
    "    for i in range(0, len(feature_column_name_list)):\n",
    "        # print(feature_column_name_list[i], feature_category_num_dict[feature_column_name_list[i]])\n",
    "        embedding_layer1 = nn.Embedding(feature_category_num_dict[feature_column_name_list[i]]+2, embedding_dim)\n",
    "        embeddings.append(embedding_layer1)\n",
    "    #     print('embedding维度',feature_category_num_dict[feature_column_name_list[i]]+1)\n",
    "    # print('本轮embedding层：',len(feature_column_name_list))\n",
    "    return embeddings\n",
    "\n",
    "# 全连接层 MLP\n",
    "def dense_layer(in_features, out_features):\n",
    "    # in_features=hidden_size,out_features=1\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features, bias=True),\n",
    "        nn.ReLU())\n",
    "# 全连接层 MLP\n",
    "def dense_layer_noReLu(in_features, out_features):\n",
    "    # in_features=hidden_size,out_features=1\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features, bias=True))\n",
    "\n",
    "# 连续特征离散化\n",
    "def continuous_embedding(num_continuous_features, out_features):\n",
    "    continuous_embedding_layers = []\n",
    "    for i in range(0,len(num_continuous_features)):\n",
    "        num_continuous_feature = num_continuous_features[i]\n",
    "        embedding_layer = dense_layer(1, out_features)\n",
    "        continuous_embedding_layers.append(embedding_layer)\n",
    "    return continuous_embedding_layers\n",
    "\n",
    "# 根据全特征数量表及类别，得到类别下的对应特征数量  feature_column_name_list = feature_column_dict['user_info_continue']\n",
    "def category_feature_num(feature_category_num_dict, feature_column_name_list):\n",
    "    category_feature_num_list = []\n",
    "    for i in range(len(feature_column_name_list)):\n",
    "        category_feature_num_list.append(feature_category_num_dict[feature_column_name_list[i]])\n",
    "    # print('category_feature_num',len(category_feature_num_list))\n",
    "    return category_feature_num_list \n",
    "\n",
    "# SE层中找到合适的reduction使channel // reduction得到整数\n",
    "def find_reduction(channel, min_reduction=2, max_reduction=19):  \n",
    "    # 对于质数，直接取自己作为reduction  \n",
    "    if is_prime(channel):  \n",
    "        return channel  \n",
    "      \n",
    "    # 计算介于min_reduction和max_reduction之间的候选reduction值  \n",
    "    candidates = [i for i in range(min_reduction, max_reduction + 1) if channel % i == 0]  \n",
    "      \n",
    "    # 如果候选列表为空，则至少取2作为reduction  \n",
    "    if not candidates:  \n",
    "        return min_reduction  \n",
    "      \n",
    "    # 尝试找到最大的候选值，使得channel // reduction的结果尽可能大  \n",
    "    reduction = max(candidates)  \n",
    "      \n",
    "    return reduction  \n",
    "def is_prime(n):  \n",
    "    \"\"\"判断一个数是否为质数\"\"\"  \n",
    "    if n < 2:  \n",
    "        return False  \n",
    "    for i in range(2, int(math.sqrt(n)) + 1):  \n",
    "        if n % i == 0:  \n",
    "            return False  \n",
    "    return True  \n",
    "    \n",
    "# 输入(batch,feature_num,embedding_dim,1) ->(batch,feature_num,embedding_dim,1)->输出特征权重及权重乘后的(batch,embedding_dim) \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.reduction = reduction\n",
    "        self.reduction = find_reduction(channel)\n",
    "        self.fc = nn.Sequential(       \n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # print('b, c, h, w',b, c, h, w)\n",
    "        y = self.avg_pool(x).view(b, c)   \n",
    "        # print('y',y)\n",
    "        weight = self.fc(y).view(b, c, 1, 1)\n",
    "        new_x = x * weight.expand_as(x)  # 利用了 PyTorch 的广播机制，使得张量 weight 被复制成与输入 x 相同的形状，然后进行逐元素相乘 \n",
    "        # 加权平均 (batch, embedding_dim)\n",
    "        weighted_avg_out_x = new_x.mean(dim=1, keepdim=True)  # 在 feature_num维度上取平均，保持维度\n",
    "        # 调整维度\n",
    "        weighted_avg_out_x = weighted_avg_out_x.view(b, 1, h, w)\n",
    "        # 去除最后一维\n",
    "        new_x = new_x.squeeze(dim=3)\n",
    "        weighted_avg_out_x = weighted_avg_out_x.squeeze(dim=3)\n",
    "        \n",
    "        return  weight, weighted_avg_out_x,new_x\n",
    "# 旧 弃用\n",
    "# class SELayer(nn.Module):\n",
    "#     def __init__(self, feature_dim, feature_num, reduction=16):\n",
    "#         super(SELayer, self).__init__()\n",
    "#         self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(in_features=feature_dim, out_features=feature_dim // reduction, bias=False),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(in_features=feature_dim // reduction, out_features=feature_num, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         # Apply average pooling along the feature_dim dimension  x(batch, embedding_dim, feature_num)\n",
    "#         b, c, h = x.size()\n",
    "#         print('b, c, h', b, c, h)\n",
    "#         y = self.pool(x.unsqueeze(-1)).view(b, c, -1)  # (batch, embedding_dim, 1)\n",
    "#         print('y', y)\n",
    "#         print('b, h', b, h)\n",
    "#         # Generate attention weights for each feature\n",
    "#         attention_weights = self.fc(y).view(b, h, -1)  # 权重batch, 1, feature_num\n",
    "#         print(attention_weights.shape)\n",
    "#         # Apply attention weights to the original input\n",
    "#         weighted_x = x * attention_weights.unsqueeze(1)\n",
    "#         # 输出的是一个形状为(batch, embedding_dim, feature_num)的张量。\n",
    "#         # 这个张量是对原始输入x进行加权后的结果，其中每个特征都被相应的注意力权重所乘。\n",
    "\n",
    "#         # Sum over the feature_num dimension to get (batch, embedding_dim)\n",
    "#         weighted_sum = torch.sum(weighted_x, dim=2)\n",
    "\n",
    "#         return attention_weights, weighted_sum, weighted_x\n",
    "    # def forward(self, x):\n",
    "    #     # Apply average pooling along the feature_dim dimension  x(batch, feature_dim, feature_num)\n",
    "    #     b, c, h, w = x.size()\n",
    "    #     print('b, c, h, w',b, c, h, w)\n",
    "    #     y = self.pool(x).view(b, c, -1)  # (batch, feature_dim, 1, 1)\n",
    "    #     print('y',y)\n",
    "    #     print('b, h * w',b, h * w)\n",
    "    #     # Generate attention weights for each feature\n",
    "    #     attention_weights = self.fc(y).view(b, h * w, -1)  # 权重batch, 1, 1, feature_num)\n",
    "    #     print(attention_weights.shape)\n",
    "    #     # Apply attention weights to the original input\n",
    "    #     weighted_x = x.view(b, c, -1) * attention_weights\n",
    "    #     # 输出的是一个形状为(batch, feature_dim, 1, feature_num)的张量。\n",
    "    #     # 这个张量是对原始输入x进行加权后的结果，其中每个特征都被相应的注意力权重所乘。\n",
    "    #     weighted_x = weighted_x.view(b, c, h, w)\n",
    "\n",
    "    #     # Sum over the feature_num dimension to get (batch, feature_dim, 1)\n",
    "    #     # weighted_sum = torch.sum(weighted_x, dim=2, keepdim=True)的具体意思是沿着feature_num维度\n",
    "    #     # （即第三个维度，索引为2）对weighted_x进行求和。由于keepdim=True，求和后的结果保持了一个额外的维度，\n",
    "    #     # 形状为(batch, feature_dim, 1)。这一步实现了对每个样本的所有特征进行加权求和，得到一个新的特征表示。\n",
    "    #     weighted_sum = torch.sum(weighted_x, dim=2, keepdim=True)\n",
    "    #     # 转置最后两维\n",
    "    #     weighted_sum = torch.transpose(weighted_sum, -2, -1)\n",
    "\n",
    "    #     return attention_weights, weighted_sum, weighted_x\n",
    "   \n",
    "    # 多头自注意力\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, feature_dim, max_history_len):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads  #10\n",
    "        self.feature_dim = feature_dim   #200\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        self.max_history_len = max_history_len\n",
    "\n",
    "        self.WQ = nn.Linear(feature_dim, feature_dim)\n",
    "        self.WK = nn.Linear(feature_dim, feature_dim)\n",
    "        self.WV = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, history_matrix, mask=None):\n",
    "        batch_size, history_len, _ = history_matrix.size()\n",
    "\n",
    "        Q = self.WQ(history_matrix)\n",
    "        K = self.WK(history_matrix)\n",
    "        V = self.WV(history_matrix)\n",
    "\n",
    "        Q = Q.view(batch_size, history_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  #(batch,num_heads,history_len,head_dim)\n",
    "        K = K.view(batch_size, history_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, history_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.permute(0, 2, 1)  # 二、三维度互换  变为(batch, feature_num, history)\n",
    "            temp_dim=mask.shape[1]\n",
    "            #（样本数*特征数,历史数）\n",
    "            mask=mask.reshape(-1,max_history_len)\n",
    "            attention_scores = attention_scores.masked_fill(mask.unsqueeze(1).unsqueeze(2).bool(), float('-inf'))  #()\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)  #shape(batch,head,history_len,history_len)\n",
    "        #(batch,history_len,200)\n",
    "        weighted_sum = torch.matmul(attention_weights, V).permute(0, 2, 1, 3).contiguous().view(batch_size, history_len,\n",
    "                                                                                                self.feature_dim)\n",
    "        # 计算加权平均\n",
    "        weighted_avg_out = weighted_sum.mean(dim=1, keepdim=True)  # 在 history_len 维度上取平均，保持维度\n",
    "        # 调整维度\n",
    "        weighted_avg_out = weighted_avg_out.view(batch_size, 1,self.feature_dim)\n",
    "        # print('weighted_sum',weighted_avg_out.shape)\n",
    "        \n",
    "        return attention_weights, weighted_avg_out, weighted_sum\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(self, num_heads, feature_dim):\n",
    "#         super(MultiHeadSelfAttention, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.feature_dim = feature_dim\n",
    "#         self.head_dim = feature_dim // num_heads\n",
    "\n",
    "#         # 线性变换的权重\n",
    "#         self.wq = nn.Parameter(torch.Tensor(feature_dim, self.num_heads * self.head_dim))\n",
    "#         self.wk = nn.Parameter(torch.Tensor(feature_dim, self.num_heads * self.head_dim))\n",
    "#         self.wv = nn.Parameter(torch.Tensor(feature_dim, self.num_heads * self.head_dim))\n",
    "\n",
    "#         # 初始化权重\n",
    "#         nn.init.normal_(self.wq, std=0.02)\n",
    "#         nn.init.normal_(self.wk, std=0.02)\n",
    "#         nn.init.normal_(self.wv, std=0.02)\n",
    "\n",
    "#     def forward(self, history_embedding_vec, mask=None):\n",
    "#         batch_size, history_len, feature_num, feature_dim = history_embedding_vec.size()\n",
    "#         # 将feature_num和batch_size合并\n",
    "#         x = history_embedding_vec.view(batch_size * feature_num, history_len, feature_dim)\n",
    "#         # 线性变换\n",
    "#         q = torch.matmul(x, self.wq).view(batch_size * feature_num, history_len, self.num_heads,self.head_dim).transpose(1, 2)\n",
    "#         k = torch.matmul(x, self.wk).view(batch_size * feature_num, history_len, self.num_heads,self.head_dim).transpose(1, 2)\n",
    "#         v = torch.matmul(x, self.wv).view(batch_size * feature_num, history_len, self.num_heads,self.head_dim).transpose(1, 2)\n",
    "#         # 缩放点积注意力\n",
    "#         scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "#         if mask is not None:\n",
    "#             mask = mask.view(batch_size * feature_num, history_len)\n",
    "#             scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2).bool(), float('-inf'))\n",
    "#         attention_weights = nn.Softmax(dim=-1)(scores)\n",
    "#         out = torch.matmul(attention_weights, v).transpose(1, 2).contiguous().view(batch_size, feature_num, history_len,self.num_heads * self.head_dim)\n",
    "#         # 合并多头\n",
    "#         out = torch.matmul(out, self.wq.view(self.num_heads * self.head_dim, feature_dim)).view(batch_size, feature_num,history_len,feature_dim)\n",
    "#         # 恢复到原始形状\n",
    "#         # out = out.view(batch_size, feature_num, history_len, feature_dim)\n",
    "        \n",
    "#           # 计算加权平均后的结果\n",
    "#         # 计算加权平均\n",
    "#         weighted_avg_out = out.mean(dim=2, keepdim=True)  # 在 history_len 维度上取平均，保持维度\n",
    "#         # 调整维度\n",
    "#         weighted_avg_out = weighted_avg_out.view(batch_size, 1, feature_num, feature_dim)\n",
    "        \n",
    "#         return attention_weights, weighted_avg_out\n",
    "\n",
    "# 注意力机制 关于用\n",
    "class MultiHeadHistory_TargetAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, dropout=0.1):\n",
    "        super(MultiHeadHistory_TargetAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by the number of heads ({num_heads}).\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # 定义权重矩阵\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        batch_size = query.size(0)       \n",
    "        # 进行线性投影并分离成多个头\n",
    "        q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # 计算注意力得分\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
    "        if attn_mask is not None:\n",
    "            scores.masked_fill_(attn_mask.unsqueeze(1), float('-inf'))\n",
    "        # 应用softmax函数\n",
    "        attn_weights = self.softmax(scores)\n",
    "        # 应用dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # 进行值的加权求和\n",
    "        context = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        # 输出层的线性变换\n",
    "        output = self.out_proj(context)\n",
    "        return attn_weights, output\n",
    "\n",
    "# class MultiHeadHistory_TargetAttention(nn.Module):\n",
    "#     def __init__(self, num_heads, feature_dim):\n",
    "#         super(MultiHeadHistory_TargetAttention, self).__init__()\n",
    "#         self.feature_dim = feature_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = feature_dim // num_heads\n",
    "\n",
    "#         assert (\n",
    "#                 self.head_dim * num_heads == feature_dim\n",
    "#         ), \"Embedding dimension must be divisible by num_heads.\"\n",
    "\n",
    "#         self.values = nn.Linear(self.num_heads * self.head_dim, self.num_heads * self.head_dim, bias=False)\n",
    "#         # self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "#         self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "#         self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "#         # 其他部分保持不变\n",
    "\n",
    "#     def forward(self, student_embeddings, unit_embeddings, mask=None):\n",
    "#         batch_size = student_embeddings.size(0)\n",
    "\n",
    "#         # Split the embedding into self.num_heads different pieces\n",
    "#         student_values = student_embeddings.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "#         student_keys = student_embeddings.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "#         student_queries = student_embeddings.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "#         unit_values = unit_embeddings.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "#         unit_keys = unit_embeddings.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "#         # print('student_queries',student_queries.shape)\n",
    "#         # print('unit_keys',unit_keys.shape)\n",
    "\n",
    "#         # Compute the attention weights\n",
    "#         energy = torch.matmul(student_queries, unit_keys.transpose(-2, -1)) / torch.sqrt(\n",
    "#             torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "#         if mask is not None:\n",
    "#             attention_weights = energy.masked_fill(mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "#         attention_weights = torch.softmax(energy, dim=-1)\n",
    "\n",
    "#         # Apply attention weights to the values\n",
    "#         out = torch.matmul(attention_weights, unit_values)\n",
    "#         # print(out.shape)\n",
    "\n",
    "#         # Concatenate the outputs of the different heads\n",
    "#         out = out.view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "#         # print(out.shape)\n",
    "\n",
    "#         # Finally, apply a linear layer to get the final output\n",
    "#         out = self.values(out)\n",
    "\n",
    "#         return attention_weights, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b1a312-5627-4a04-9e2f-dd5dc3889a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Embedding层\n",
    "\n",
    "# user_history_feature 对于一个user的多个历史行为，将其拼接成一维向量 要先经过一层通道注意力机制得到最后结果\n",
    "# (样本数,history,20,200) ->多头 ->(样本数,20,200)->转置->(样本数,200,20) ->SE->特征权重->(样本数,200,20) ->转置-> 加权->(样本数,1，200)\n",
    "# user_pay_history_feature 加上batch的\n",
    "# 用户历史\n",
    "class UserPayHistoryEmbedding(nn.Module):\n",
    "    def __init__(self, continue_embedding_dim, discrete_embedding_dim, feature_category_num_dict, feature_column_dict):\n",
    "        super(UserPayHistoryEmbedding, self).__init__()\n",
    "        # 连续特征\n",
    "        # 离散特征\n",
    "        self.feature_category_num_dict = feature_category_num_dict\n",
    "        # 离散embedding\n",
    "        # self.user_pay_history_QOE_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,\n",
    "        #                                                         feature_column_dict['history_QOE_discrete'],\n",
    "        #                                                         discrete_embedding_dim)\n",
    "        self.user_pay_history_CHONGHE_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,\n",
    "                                                                           feature_column_dict['history_CHONGHE_discrete_add_D'],\n",
    "                                                                           discrete_embedding_dim)\n",
    "        # self.user_pay_history_FUFEI_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,\n",
    "        #                                                                    feature_column_dict['history_FUFEI_discrete'],\n",
    "        #                                                                    discrete_embedding_dim)\n",
    "        # MLP  连续embedding\n",
    "        # category_feature_num_list = category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_continue'])\n",
    "        # self.user_pay_history_QOE_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_continue']), continue_embedding_dim)\n",
    "        self.user_pay_history_CHONGHE_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_continue']), continue_embedding_dim)\n",
    "        # self.user_pay_history_FUFEI_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_continue']), continue_embedding_dim)\n",
    "\n",
    "    def forward(self, batch_feature_tensor_pay_CHONGHE_discrete,batch_feature_tensor_pay_CHONGHE_continue):\n",
    "        # user_history Embedding\n",
    "        # user_history_continue_features_embedding 得到(batch, 1, continue_feature_num, continue_embedding_dim)\n",
    "        # user_history_discrete_features_embedding 得到(batch, 1, discrete_feature_num, discrete_embedding_dim)\n",
    "        # history中有三种：QOE/CHONGHE/FUFEI,将其分别转化为embedding然后合并\n",
    "        # embedding的数据要求输入是整数类型 因此转为int，输入数据得是从0开始的索引后的数据，因此mask后得到-1以及在输入时得到了从0开始的索引后值，\n",
    "        # 现在所有discrete数据输入时+1，即 batch_feature_tensor_pay_QOE_discrete[:, :, i]+1 \n",
    "        # for i in range(batch_feature_tensor_pay_QOE_discrete.shape[2]):\n",
    "        #     print(i,batch_feature_tensor_pay_QOE_discrete.shape[2],batch_feature_tensor_pay_QOE_discrete[:, :, i]+1,self.user_pay_history_QOE_discrete_embeddings[i].num_embeddings )\n",
    "        # batch_feature_tensor_pay_QOE_discrete = batch_feature_tensor_pay_QOE_discrete.int()\n",
    "        batch_feature_tensor_pay_CHONGHE_discrete = batch_feature_tensor_pay_CHONGHE_discrete.int()\n",
    "        # batch_feature_tensor_pay_FUFEI_discrete = batch_feature_tensor_pay_FUFEI_discrete.int()\n",
    "        \n",
    "        # user_history_pay_QOE_discrete_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_pay_QOE_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "        #      enumerate(self.user_pay_history_QOE_discrete_embeddings)], dim=-2)\n",
    "        # user_history_pay_QOE_continue_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_pay_QOE_continue[:,:, i].unsqueeze(-1).float()) for i, embedding_layer in\n",
    "        #      enumerate(self.user_pay_history_QOE_continue_embedding)], dim=-2)\n",
    "        # user_history_pay_QOE_vec = torch.cat(\n",
    "        #     [user_history_pay_QOE_discrete_column_discrete_features_embedding, user_history_pay_QOE_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "        \n",
    "        user_history_pay_CHONGHE_discrete_column_discrete_features_embedding = torch.stack(\n",
    "            [embedding_layer(batch_feature_tensor_pay_CHONGHE_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "             enumerate(self.user_pay_history_CHONGHE_discrete_embeddings)], dim=-2)\n",
    "        user_history_pay_CHONGHE_continue_column_discrete_features_embedding = torch.stack(\n",
    "             [embedding_layer(batch_feature_tensor_pay_CHONGHE_continue[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.user_pay_history_CHONGHE_continue_embedding)], dim=-2)\n",
    "        user_history_pay_CHONGHE_vec = torch.cat(\n",
    "            [user_history_pay_CHONGHE_discrete_column_discrete_features_embedding,\n",
    "             user_history_pay_CHONGHE_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "        \n",
    "        # user_history_pay_FUFEI_discrete_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_pay_FUFEI_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "        #      enumerate(self.user_pay_history_FUFEI_discrete_embeddings)], dim=-2)\n",
    "        # user_history_pay_FUFEI_continue_column_discrete_features_embedding = torch.stack(\n",
    "        #      [embedding_layer(batch_feature_tensor_pay_FUFEI_continue[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "        #      enumerate(self.user_pay_history_FUFEI_continue_embedding)], dim=-2)\n",
    "        # user_history_pay_FUFEI_vec = torch.cat(\n",
    "        #     [user_history_pay_FUFEI_discrete_column_discrete_features_embedding,\n",
    "        #      user_history_pay_FUFEI_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "        # print(user_history_pay_FUFEI_discrete_column_discrete_features_embedding.shape,user_history_pay_FUFEI_continue_column_discrete_features_embedding.shape)\n",
    "\n",
    "        return user_history_pay_CHONGHE_vec\n",
    "        \n",
    "# target_feature\n",
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, continue_embedding_dim, discrete_embedding_dim, feature_category_num_dict, feature_column_dict):\n",
    "        super(TargetEmbedding, self).__init__()\n",
    "        # 连续特征  与付费、非付费共享一套特征\n",
    "        # 离散特征  与付费、非付费共享一套特征\n",
    "        self.feature_category_num_dict = feature_category_num_dict\n",
    "        # 离散embedding\n",
    "        # self.target_QOE_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,feature_column_dict['history_QOE_discrete'],discrete_embedding_dim)\n",
    "        self.target_CHONGHE_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,feature_column_dict['history_CHONGHE_discrete'],discrete_embedding_dim)\n",
    "        # self.target_FUFEI_discrete_embeddings = discrete_embedding(self.feature_category_num_dict,feature_column_dict['history_FUFEI_discrete'],discrete_embedding_dim)\n",
    "        # MLP  连续embedding\n",
    "        # self.target_QOE_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_continue']), continue_embedding_dim)\n",
    "        self.target_CHONGHE_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_continue']), continue_embedding_dim)\n",
    "        # self.target_FUFEI_continue_embedding = continuous_embedding(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_continue']), continue_embedding_dim)\n",
    "\n",
    "    def forward(self, batch_feature_tensor_target_CHONGHE_discrete,batch_feature_tensor_target_CHONGHE_continue):\n",
    "        # target Embedding\n",
    "        # target_continue_features_embedding 得到(batch, 1, continue_feature_num, continue_embedding_dim)\n",
    "        # target_discrete_features_embedding 得到(batch, 1, discrete_feature_num, discrete_embedding_dim)\n",
    "        # 有三种：QOE/CHONGHE/FUFEI,将其分别转化为embedding然后合并\n",
    "        # batch_feature_tensor_target_QOE_discrete = batch_feature_tensor_target_QOE_discrete.int()\n",
    "        batch_feature_tensor_target_CHONGHE_discrete = batch_feature_tensor_target_CHONGHE_discrete.int()\n",
    "        # batch_feature_tensor_target_FUFEI_discrete = batch_feature_tensor_target_FUFEI_discrete.int()\n",
    "        # target_QOE_discrete_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_target_QOE_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "        #      enumerate(self.target_QOE_discrete_embeddings)], dim=-2)\n",
    "        # target_QOE_continue_column_discrete_features_embedding = torch.stack(\n",
    "        #      [embedding_layer(batch_feature_tensor_target_QOE_continue[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "        #      enumerate(self.target_QOE_continue_embedding)], dim=-2)\n",
    "        # target_QOE_vec = torch.cat(\n",
    "        #     [target_QOE_discrete_column_discrete_features_embedding, target_QOE_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "        \n",
    "        target_CHONGHE_discrete_column_discrete_features_embedding = torch.stack(\n",
    "            [embedding_layer(batch_feature_tensor_target_CHONGHE_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "             enumerate(self.target_CHONGHE_discrete_embeddings)], dim=-2)\n",
    "        target_CHONGHE_continue_column_discrete_features_embedding = torch.stack(\n",
    "            [embedding_layer(batch_feature_tensor_target_CHONGHE_continue[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.target_CHONGHE_continue_embedding)], dim=-2)\n",
    "        target_CHONGHE_vec = torch.cat(\n",
    "            [target_CHONGHE_discrete_column_discrete_features_embedding,\n",
    "             target_CHONGHE_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "        \n",
    "        # target_FUFEI_discrete_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_target_FUFEI_discrete[:, :, i]+1) for i, embedding_layer in\n",
    "        #      enumerate(self.target_FUFEI_discrete_embeddings)], dim=-2)\n",
    "        # target_FUFEI_continue_column_discrete_features_embedding = torch.stack(\n",
    "        #     [embedding_layer(batch_feature_tensor_target_FUFEI_continue[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "        #      enumerate(self.target_FUFEI_continue_embedding)], dim=-2)\n",
    "        # target_FUFEI_vec = torch.cat(\n",
    "        #     [target_FUFEI_discrete_column_discrete_features_embedding,\n",
    "        #      target_FUFEI_continue_column_discrete_features_embedding], dim=2)  # 特征级合并\n",
    "\n",
    "        return target_CHONGHE_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53d8814f-4dd7-4f2f-a6c0-e9af13389f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Attention层\n",
    "\n",
    "\n",
    "# 用户历史embedding 多头+SE  (batch, history, feature_num, feature_dim)->(batch, 1，feature_dim)\n",
    "class HistoryDimScalingLayer(nn.Module):\n",
    "    def __init__(self, num_heads, feature_dim, feature_category_num_dict, max_history_len):\n",
    "        super(HistoryDimScalingLayer, self).__init__()\n",
    "        # 多头注意力\n",
    "        self.multi_head_attention = MultiHeadSelfAttention(num_heads, feature_dim,max_history_len)\n",
    "        # SE注意力\n",
    "        # self.se_attention_QOE = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_discrete'])))\n",
    "        self.se_attention_CHONGHE = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_discrete_add_D'])))\n",
    "        # self.se_attention_FUFEI = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_discrete'])))\n",
    "\n",
    "    def forward(self,  user_history_CHONGHE_vec,pay_CHONGHE_mask=None):\n",
    "        # (batch, history, feature_num, feature_dim) ->多头 ->(batch, feature_num, feature_dim)->转置->(batch, feature_dim, feature_num) ->SE->特征权重->(batch, feature_dim, feature_num) ->转置-> 加权->(batch, 1，feature_dim)\n",
    "        # 多头注意力  例(batch, history, 20, 200) ->多头 ->(batch, 20, 200)\n",
    "        # print('user_history_QOE_vec',user_history_QOE_vec.shape,pay_QOE_mask.shape)\n",
    "        # ********多头注意力前转化************\n",
    "        # 二、三维度互换  变为(batch, feature_num, history, 200)\n",
    "        # user_history_QOE_vec = user_history_QOE_vec.permute(0, 2, 1, 3)  \n",
    "        user_history_CHONGHE_vec = user_history_CHONGHE_vec.permute(0, 2, 1, 3) \n",
    "        # user_history_FUFEI_vec = user_history_FUFEI_vec.permute(0, 2, 1, 3) \n",
    "        # 记录特征数\n",
    "        # user_history_QOE_temp_dim=user_history_QOE_vec.shape[1]  \n",
    "        user_history_CHONGHE_temp_dim=user_history_CHONGHE_vec.shape[1]\n",
    "        # user_history_FUFEI_temp_dim=user_history_FUFEI_vec.shape[1]\n",
    "        #（样本数*特征数,历史数，200）\n",
    "        # user_history_QOE_vec=user_history_QOE_vec.reshape(-1,max_history_len,feature_dim)\n",
    "        user_history_CHONGHE_vec=user_history_CHONGHE_vec.reshape(-1,max_history_len,feature_dim)\n",
    "        # user_history_FUFEI_vec=user_history_FUFEI_vec.reshape(-1,max_history_len,feature_dim)\n",
    "        #(样本数*特征数，200）\n",
    "        # mutli_QOE_weight, multi_user_history_QOE_vec,_ = self.multi_head_attention(user_history_QOE_vec, mask=pay_QOE_mask)\n",
    "        mutli_CHONGHE_weight, multi_user_history_CHONGHE_vec,_ = self.multi_head_attention(user_history_CHONGHE_vec, mask=pay_CHONGHE_mask)\n",
    "        # mutli_FUFEI_weight, multi_user_history_FUFEI_vec,_ = self.multi_head_attention(user_history_FUFEI_vec, mask=pay_FUFEI_mask)\n",
    "        #(样本数,特征数，200）\n",
    "        # multi_user_history_QOE_vec=multi_user_history_QOE_vec.view(-1,user_history_QOE_temp_dim,feature_dim)\n",
    "        multi_user_history_CHONGHE_vec=multi_user_history_CHONGHE_vec.view(-1,user_history_CHONGHE_temp_dim,feature_dim)\n",
    "        # multi_user_history_FUFEI_vec=multi_user_history_FUFEI_vec.view(-1,user_history_FUFEI_temp_dim,feature_dim)\n",
    "        # print('multi_user_history_QOE_vec',multi_user_history_QOE_vec.shape) #,multi_user_history_QOE_vec[0,0,:])\n",
    "        \n",
    "        # 去掉第二维  (batch, 1, 20, 200)->(batch, 20, 200)\n",
    "        # multi_user_history_QOE_vec = multi_user_history_QOE_vec.squeeze(dim=1)\n",
    "        # multi_user_history_CHONGHE_vec = multi_user_history_CHONGHE_vec.squeeze(dim=1)\n",
    "        # multi_user_history_FUFEI_vec= multi_user_history_FUFEI_vec.squeeze(dim=1)\n",
    "        # 调整维度 (batch, 20, 200)->(batch,20,200,1)  (batch,feature_num.embedding_dim,1)\n",
    "        # multi_user_history_QOE_vec = multi_user_history_QOE_vec.unsqueeze(-1)\n",
    "        multi_user_history_CHONGHE_vec = multi_user_history_CHONGHE_vec.unsqueeze(-1)\n",
    "        # multi_user_history_FUFEI_vec= multi_user_history_FUFEI_vec.unsqueeze(-1)\n",
    "        # 转置 交换最后两个维度 (feature_num 和 embedding_dim)\n",
    "        # multi_user_history_QOE_vec = torch.transpose(multi_user_history_QOE_vec, 1, 2)\n",
    "        # multi_user_history_CHONGHE_vec = torch.transpose(multi_user_history_CHONGHE_vec, 1, 2)\n",
    "        # multi_user_history_FUFEI_vec = torch.transpose(multi_user_history_FUFEI_vec, 1, 2)\n",
    "\n",
    "        # SE注意力  (batch,feature_num,feature_dim,1) ->SE->特征权重->(batch,feature_num,feature_dim,1)->去除最后一列-> 加权->(batch, 1，feature_dim)\n",
    "        # se_QOE_weight, se_user_history_QOE_vec,_= self.se_attention_QOE(multi_user_history_QOE_vec)\n",
    "        se_CHONGHE_weight, se_user_history_CHONGHE_vec,_ = self.se_attention_CHONGHE(multi_user_history_CHONGHE_vec)\n",
    "        # se_FUFEI_weight, se_user_history_FUFEI_vec,_ = self.se_attention_FUFEI(multi_user_history_FUFEI_vec)\n",
    "\n",
    "\n",
    "        HistoryDimScaling_Weight_Result = {\n",
    "            # 'mutli_QOE_weight': mutli_QOE_weight,\n",
    "            'mutli_CHONGHE_weight': mutli_CHONGHE_weight,\n",
    "            # 'mutli_FUFEI_weight': mutli_FUFEI_weight,\n",
    "            # 'se_QOE_weight': se_QOE_weight,\n",
    "            'se_CHONGHE_weight': se_CHONGHE_weight,\n",
    "            # 'se_FUFEI_weight': se_FUFEI_weight\n",
    "        }\n",
    "        return HistoryDimScaling_Weight_Result, se_user_history_CHONGHE_vec\n",
    "\n",
    "# 目标产品embedding SE  (batch, 1, feature_num, feature_dim)->(batch, 1，feature_dim)\n",
    "class TargetDimScalingLayer(nn.Module):\n",
    "    def __init__(self, feature_dim, feature_category_num_dict):\n",
    "        super(TargetDimScalingLayer, self).__init__()\n",
    "        # SE注意力\n",
    "        # self.se_attention_QOE = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_QOE_discrete'])))\n",
    "        self.se_attention_CHONGHE = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_CHONGHE_discrete'])))\n",
    "        # self.se_attention_FUFEI = SELayer(len(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_continue']))+len(category_feature_num(feature_category_num_dict, feature_column_dict['history_FUFEI_discrete'])))\n",
    "\n",
    "    def forward(self,  target_CHONGHE_vec, mask=None):\n",
    "        # (batch, 1, feature_num, feature_dim) (batch, feature_num, feature_dim)->转置->(batch, feature_dim, feature_num) ->SE->特征权重->(batch, feature_dim, feature_num) ->转置-> 加权->(batch, 1，feature_dim)\n",
    "        # target_QOE_vec = target_QOE_vec.squeeze(1)  # 使用 squeeze 函数移除大小为 1 的维度\n",
    "        # target_CHONGHE_vec = target_CHONGHE_vec.squeeze(1)  # 使用 squeeze 函数移除大小为 1 的维度\n",
    "        # target_FUFEI_vec = target_FUFEI_vec.squeeze(1)  # 使用 squeeze 函数移除大小为 1 的维度\n",
    "        # 转置 交换最后两个维度 (20 和 200)\n",
    "        # target_QOE_vec = torch.transpose(target_QOE_vec, -2, -1)\n",
    "        # target_CHONGHE_vec = torch.transpose(target_CHONGHE_vec, -2, -1)\n",
    "        # target_FUFEI_vec = torch.transpose(target_FUFEI_vec, -2, -1)\n",
    "        # 去掉第二维  (batch, 1, 20, 200)->(batch, 20, 200)\n",
    "        # target_QOE_vec = target_QOE_vec.squeeze(dim=1)\n",
    "        target_CHONGHE_vec = target_CHONGHE_vec.squeeze(dim=1)\n",
    "        # target_FUFEI_vec= target_FUFEI_vec.squeeze(dim=1)\n",
    "        # 调整维度 (batch, 20, 200)->(batch,20,200,1)  (batch,feature_num.embedding_dim,1)\n",
    "        # target_QOE_vec = target_QOE_vec.unsqueeze(-1)\n",
    "        target_CHONGHE_vec = target_CHONGHE_vec.unsqueeze(-1)\n",
    "        # target_FUFEI_vec= target_FUFEI_vec.unsqueeze(-1)\n",
    "\n",
    "        # SE注意力  (batch, feature_dim, feature_num) ->SE->特征权重->(batch, feature_dim, feature_num)->转置-> 加权->(batch, 1，feature_dim)\n",
    "        # 结果为权重，合并后向量，合并前向量\n",
    "        # se_QOE_weight, se_target_QOE_vec,_ = self.se_attention_QOE(target_QOE_vec)\n",
    "        se_CHONGHE_weight, se_target_CHONGHE_vec,_ = self.se_attention_CHONGHE(target_CHONGHE_vec)\n",
    "        # se_FUFEI_weight, se_target_FUFEI_vec,_ = self.se_attention_FUFEI(target_FUFEI_vec)\n",
    "\n",
    "        TargetDimScaling_Weight_Result = {\n",
    "            # 'se_QOE_weight': se_QOE_weight,\n",
    "            'se_CHONGHE_weight': se_CHONGHE_weight,\n",
    "            # 'se_FUFEI_weight': se_FUFEI_weight\n",
    "        }\n",
    "        return TargetDimScaling_Weight_Result, se_target_CHONGHE_vec\n",
    "\n",
    "# 用户历史与目标记录的attention层\n",
    "class History_Target_AttentionLayer(nn.Module):\n",
    "    def __init__(self, num_heads, feature_dim):\n",
    "        super(History_Target_AttentionLayer, self).__init__()\n",
    "        # self.target_history_pay_feature_pianhao_QOE_layer = MultiHeadHistory_TargetAttention(num_heads, feature_dim)\n",
    "        self.target_history_pay_feature_pianhao_CHONGHE_layer = MultiHeadHistory_TargetAttention(num_heads, feature_dim)\n",
    "        # self.target_history_pay_feature_pianhao_FUFEI_layer = MultiHeadHistory_TargetAttention(num_heads, feature_dim)\n",
    "\n",
    "    def forward(self, se_user_history_pay_CHONGHE_vec, se_target_CHONGHE_vec, pay_CHONGHE_mask=None):\n",
    "        # 将QOE、CHONGHE、FUFEI分别做attention\n",
    "        # 对目标特征求对历史特征的偏好   (batch, 1，feature_dim)输出\n",
    "        # target_history_pay_attention_QOE_weight, target_history_pay_attention_QOE_vec = self.target_history_pay_feature_pianhao_QOE_layer(se_target_QOE_vec, se_user_history_pay_QOE_vec, se_user_history_pay_QOE_vec)\n",
    "        target_history_pay_attention_CHONGHE_weight, target_history_pay_attention_CHONGHE_vec = self.target_history_pay_feature_pianhao_CHONGHE_layer(se_target_CHONGHE_vec, se_user_history_pay_CHONGHE_vec, se_user_history_pay_CHONGHE_vec)\n",
    "        # target_history_pay_attention_FUFEI_weight, target_history_pay_attention_FUFEI_vec = self.target_history_pay_feature_pianhao_FUFEI_layer(se_target_FUFEI_vec, se_user_history_pay_FUFEI_vec, se_user_history_pay_FUFEI_vec)\n",
    "        # CONCAT  (batch, 3，feature_dim)输出\n",
    "        # target_history_pay_attention_vec = torch.cat((target_history_pay_attention_QOE_vec, target_history_pay_attention_CHONGHE_vec), dim=1)\n",
    "        target_history_pay_attention_vec = target_history_pay_attention_CHONGHE_vec\n",
    "        return target_history_pay_attention_vec,target_history_pay_attention_CHONGHE_weight\n",
    "\n",
    "# class History_Target_AttentionLayer(nn.Module):\n",
    "#     def __init__(self, num_heads, feature_dim):\n",
    "#         super(History_Target_AttentionLayer, self).__init__()\n",
    "#         self.target_history_pay_feature_pianhao_layer = MultiHeadHistory_TargetAttention(num_heads, feature_dim)\n",
    "#         self.target_history_not_pay_feature_pianhao_layer = MultiHeadHistory_TargetAttention(num_heads, feature_dim)\n",
    "\n",
    "#     def forward(self, se_user_history_pay_QOE_vec, se_user_history_pay_CHONGHE_vec, se_user_history_pay_FUFEI_vec, se_target_QOE_vec, se_target_CHONGHE_vec, se_target_FUFEI_vec, pay_QOE_mask=None, pay_CHONGHE_mask=None, pay_FUFEI_mask=None):\n",
    "#         # 将QOE、CHONGHE、FUFEI叠加，形成三个特征的向量  (batch, 1，feature_dim)->(batch, 3，feature_dim)\n",
    "#         user_history_pay_vec = torch.cat((se_user_history_pay_QOE_vec, se_user_history_pay_CHONGHE_vec, se_user_history_pay_FUFEI_vec), dim=1)\n",
    "#         target_vec = torch.cat((se_target_QOE_vec, se_target_CHONGHE_vec, se_target_FUFEI_vec), dim=1)\n",
    "        \n",
    "#         # 对目标特征求对历史特征的偏好   (batch, 3，feature_dim)输出\n",
    "#         target_history_pay_attention_weight, target_history_pay_attention_vec = self.target_history_pay_feature_pianhao_layer(target_vec, user_history_pay_vec)\n",
    "\n",
    "#         return target_history_pay_attention_weight, target_history_pay_attention_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55d9fff9-b6f3-457a-a203-48e5819897d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.整合模型\n",
    "\n",
    "\n",
    "# (batch,600)经过网络变成200 +(batch,featuer_user*200)经过网络变成200 -> (batch,200)\n",
    "# (batch,200) ->MLP ->(batch，1) ->sigmoid -> (batch,1)\n",
    "\n",
    "# 整合层\n",
    "class MatchingModel(nn.Module):\n",
    "    def __init__(self, feature_category_num_dict, feature_column_dict, continue_embedding_dim,\n",
    "                 discrete_embedding_dim,num_heads, feature_dim, max_history_len):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        # Embedding层\n",
    "        # self.user_info_embedding_layer = UserInfoEmbedding(continue_embedding_dim, discrete_embedding_dim, feature_category_num_dict, feature_column_dict)\n",
    "        self.user_history_pay_embedding_layer = UserPayHistoryEmbedding(continue_embedding_dim, discrete_embedding_dim, feature_category_num_dict, feature_column_dict)\n",
    "        #print('embedding user_history结果')\n",
    "        self.target_embedding_layer = TargetEmbedding(continue_embedding_dim, discrete_embedding_dim, feature_category_num_dict, feature_column_dict)\n",
    "        \n",
    "        # User History & Target Attention层\n",
    "        self.history_pay_attention_layer = HistoryDimScalingLayer(num_heads, feature_dim, feature_category_num_dict, max_history_len)\n",
    "        self.target_attention_layer = TargetDimScalingLayer(feature_dim, feature_category_num_dict)\n",
    "\n",
    "        # Target History Attention层\n",
    "        self.target_history_attention_layer = History_Target_AttentionLayer(num_heads, feature_dim)\n",
    "\n",
    "        # 维度转换层\n",
    "        final_dim = 20\n",
    "        self.target_dim_change = dense_layer_noReLu(1*feature_dim, final_dim)  # (batch,3,200)->(batch,600)->(batch,200)\n",
    "        # user_info_feature_num = feature_category_num_dict['user_info_continue'].shape[2] + feature_category_num_dict['user_info_discrete'].shape[2]\n",
    "        # self.user_info_dim_change = dense_layer(user_info_feature_num, 200)  # (batch,user_info_feature,200)->(batch,user_info_feature*200)->(batch,200)\n",
    "        # MLP\n",
    "        self.pay_vec_MLP_layer = dense_layer_noReLu(final_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                batch_feature_tensor_target_CHONGHE_continue,\n",
    "                batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                label_tensor):\n",
    "        # Embedding层\n",
    "        user_history_pay_CHONGHE_vec = self.user_history_pay_embedding_layer(batch_feature_tensor_pay_CHONGHE_discrete,batch_feature_tensor_pay_CHONGHE_continue)        \n",
    "        target_CHONGHE_vec = self.target_embedding_layer(batch_feature_tensor_target_CHONGHE_discrete,batch_feature_tensor_target_CHONGHE_continue)\n",
    "        # print('user_history_pay_FUFEI_vec size=',user_history_pay_FUFEI_vec.size())\n",
    "        # print('target_QOE_vec size=',target_QOE_vec.size())\n",
    "        # User History & Target Attention层\n",
    "        # 合并mask输入  \n",
    "        # print(\"Shape of mask tensor:\", batch_feature_tensor_pay_QOE_discrete_mask.shape,batch_feature_tensor_pay_QOE_continue_mask.shape)\n",
    "        # pay_QOE_mask = torch.cat((batch_feature_tensor_pay_QOE_discrete_mask, batch_feature_tensor_pay_QOE_continue_mask), dim=2)\n",
    "        pay_CHONGHE_mask = torch.cat((batch_feature_tensor_pay_CHONGHE_discrete_mask, batch_feature_tensor_pay_CHONGHE_continue_mask), dim=2)\n",
    "        # pay_FUFEI_mask = torch.cat((batch_feature_tensor_pay_FUFEI_discrete_mask, batch_feature_tensor_pay_FUFEI_continue_mask), dim=2)\n",
    "        HistoryDimScaling_Weight_Result,  se_user_history_pay_CHONGHE_vec= self.history_pay_attention_layer( user_history_pay_CHONGHE_vec, pay_CHONGHE_mask=pay_CHONGHE_mask)\n",
    "        TargetDimScaling_Weight_Result, se_target_CHONGHE_vec = self.target_attention_layer(target_CHONGHE_vec)\n",
    "        # print('se_user_history_pay_QOE_vec size=', se_user_history_pay_QOE_vec.shape)\n",
    "        # print('se_target_QOE_vec size=', se_target_QOE_vec.shape)\n",
    "        # Target with History Attention层\n",
    "        target_history_pay_attention_vec,\\\n",
    "        target_history_pay_attention_CHONGHE_weight= self.target_history_attention_layer(\n",
    "            se_user_history_pay_CHONGHE_vec, \n",
    "            se_target_CHONGHE_vec)\n",
    "        # print('target_history_pay_attention_vec size=', target_history_pay_attention_vec.shape)\n",
    "\n",
    "        # # 拼接user_info_vec与target_history_pay_attention_vec等\n",
    "        # user_info_vec = user_info_vec.squeeze(1)  # 使用 squeeze 函数移除大小为 1 的维度\n",
    "        # FUFEI:(batch,3,200)->(batch,3*200)经过网络->(batch,200) + uer_info:(batch,featuer_user*200)经过网络->(batch,200) 叠加后-> (batch,400)\n",
    "        # 维度转换 (batch,3,200)->(batch,feature*200)经过网络->(batch,200)\n",
    "        target_history_pay_attention_vec = target_history_pay_attention_vec.view(batch_size, -1)   # 将张量 x 重塑为 (batch, 3*200)  使用 -1 作为自动计算的维度       \n",
    "        target_history_pay_attention_vec = self.target_dim_change(target_history_pay_attention_vec)\n",
    "        # print('target_history_pay_attention_vec',target_history_pay_attention_vec)\n",
    "\n",
    "        # MLP\n",
    "        # (batch,200) ->MLP ->(batch，1) ->sigmoid -> (batch,1)\n",
    "        out_vec = self.pay_vec_MLP_layer(target_history_pay_attention_vec)\n",
    "        # print('out_vec size=',out_vec.shape,'out_vec:',out_vec)\n",
    "        # 使用softmax函数将logits转换为概率分布\n",
    "        # softmax_score = F.softmax(out_vec, dim=1)  # 在类别维度（dim=1）上应用softmax\n",
    "        sigmoid_score = torch.sigmoid(out_vec)  # 在类别维度（dim=1）上应用softmax\n",
    "        # sigmoid_score = out_vec  # 在类别维度（dim=1）上应用softmax\n",
    "        softmax_score = torch.softmax(out_vec, dim=1)\n",
    "        # print('softmax_score size=',softmax_score.shape,'score:',softmax_score)\n",
    "        # print('sigmoid_score size=',sigmoid_score.shape,'score:',sigmoid_score)\n",
    "\n",
    "        return softmax_score, sigmoid_score, HistoryDimScaling_Weight_Result, TargetDimScaling_Weight_Result, target_history_pay_attention_CHONGHE_weight\n",
    "\n",
    "# 损失函数\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFunction, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target_label):\n",
    "        # pred是未经处理过的原值，target_label是0、1标签\n",
    "        # 计算第一个任务的二元交叉熵损失\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, target_label, reduction='none')\n",
    "        return loss\n",
    "\n",
    "# 输出权重结果到文件夹 首先要压缩维度到特征上，然后根据特征名列表输出\n",
    "# tensor_dict_idx = ['pay_QOE_continue','pay_QOE_discrete','pay_CHONGHE_continue','pay_CHONGHE_discrete','pay_FUFEI_continue','pay_FUFEI_discrete','target_QOE_continue','target_QOE_discrete','target_CHONGHE_continue','target_CHONGHE_discrete','target_FUFEI_continue','target_FUFEI_discrete']\n",
    "# def outPut_weight_file(output_weight_result_path,tensor_dict_idx,\n",
    "#                        HistoryDimScaling_Weight_Result, TargetDimScaling_Weight_Result, target_history_pay_attention_weight,\n",
    "#                        user_history_pay_QOE_discrete_column,user_history_pay_QOE_continue_column,\n",
    "#                        user_history_pay_CHONGHE_discrete_column,user_history_pay_CHONGHE_continue_column,\n",
    "#                        user_history_pay_FUFEI_discrete_column,user_history_pay_FUFEI_continue_column):\n",
    "#     if\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a90faa8-237f-4578-9890-a187f8f11efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建大模型的实例 'drama_upuser_subscriptions_num,drama_sound_max_traffic_position_in_sound_avg,label1'\n",
    "model = MatchingModel(feature_category_num_dict, feature_column_dict, continue_embedding_dim,\n",
    "                 discrete_embedding_dim, num_heads, feature_dim, max_history_len)\n",
    "# print('模型创建完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf028181-1968-4486-b6ef-3a1d338888f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.模型训练 Trainging\n",
    "\n",
    "def model_training(model,train_loader,val_loader, lossfunction,optimizer,EPOCH,device):\n",
    "    # 定义早停策略的参数\n",
    "    best_val_loss = float('inf')  # 初始化最佳验证损失为正无穷\n",
    "    patience = 3  # 容忍多少个epoch没有验证性能提升\n",
    "    early_stopping_counter = 0  # 初始化计数器\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        total_classfier_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        train_time = 0\n",
    "        val_time = 0\n",
    "        for batch in train_loader:\n",
    "            batch = [data.to(device) for data in batch]\n",
    "            batch_feature_tensor_pay_CHONGHE_discrete,\\\n",
    "            batch_feature_tensor_pay_CHONGHE_continue,\\\n",
    "            batch_feature_tensor_target_CHONGHE_discrete,\\\n",
    "            batch_feature_tensor_target_CHONGHE_continue,\\\n",
    "            batch_feature_tensor_pay_CHONGHE_discrete_mask,\\\n",
    "            batch_feature_tensor_pay_CHONGHE_continue_mask,\\\n",
    "            train_label_tensor = batch  \n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            softmax_score, sigmoid_score, HistoryDimScaling_Weight_Result, TargetDimScaling_Weight_Result, \\\n",
    "            target_history_pay_attention_CHONGHE_weight = model(batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                                            batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                                            batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                                            batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                                            batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                                            batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                                            train_label_tensor)  \n",
    "\n",
    "            # print('HistoryDimScaling_Weight_Result, TargetDimScaling_Weight_Result, target_history_pay_attention_weight',\n",
    "            #      HistoryDimScaling_Weight_Result['mutli_QOE_weight'].shape, TargetDimScaling_Weight_Result['se_QOE_weight'].shape, target_history_pay_attention_weight.shape)\n",
    "            # sigmoid\n",
    "            sigmoid_score = sigmoid_score[:, 0]  # (样本数，1)\n",
    "            train_label_tensor = train_label_tensor[:, 0].to(device)  # (样本数，1)\n",
    "            train_label_tensor[train_label_tensor == 1] = 0\n",
    "            train_label_tensor[train_label_tensor == 2] = 1\n",
    "            # train_label_tensor = torch.where(train_label_tensor == 1, torch.tensor(0).to(device), torch.tensor(1).to(device))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "            loss = lossfunction(sigmoid_score, train_label_tensor.float())\n",
    "            # softmax\n",
    "            # softmax_score = softmax_score[:, 0]  # (样本数，1)\n",
    "            # train_label_tensor = train_label_tensor[:, 0].to(device)  # (样本数，1)\n",
    "            # train_label_tensor = torch.where(train_label_tensor == 1, torch.tensor(0).to(device), torch.tensor(1).to(device))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "            # loss = lossfunction(softmax_score, train_label_tensor.float())\n",
    "            loss.to(device)\n",
    "            \n",
    "             # loss回传检查\n",
    "            # for name, parms in model.named_parameters():\t\n",
    "            #     if parms.grad is not None:  # 检查梯度是否为None\n",
    "            #         grad_mean = torch.mean(parms.grad)  # 计算梯度的均值\n",
    "            #         print('-->name:', name, '-->grad_requirs:', parms.requires_grad, '-->grad_mean: {:.4f}'.format(grad_mean))\n",
    "            #     else:\n",
    "            #         print('-->name:', name, '-->grad_requirs:', parms.requires_grad, '-->grad_mean: None')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(\"=============更新之后===========\")\n",
    "            # for name, parms in model.named_parameters():\t\n",
    "            #     if parms.grad is not None:  # 检查梯度是否为None\n",
    "            #         grad_mean = torch.mean(parms.grad)  # 计算梯度的均值\n",
    "            #         print('-->name:', name, '-->grad_requirs:', parms.requires_grad, '-->grad_mean: {:.4f}'.format(grad_mean))\n",
    "            #     else:\n",
    "            #         print('-->name:', name, '-->grad_requirs:', parms.requires_grad, '-->grad_mean: None')\n",
    "            # print(optimizer)\n",
    "            # input(\"=====迭代结束=====\")\n",
    "\n",
    "            # 损失\n",
    "            total_loss += loss.item()\n",
    "            train_time += 1\n",
    "            print('||--训练：----------',train_time,'个batch运行时间：',datetime.datetime.now(),'-------------')\n",
    "        # 平均损失\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1},loss:{average_loss}\")\n",
    "\n",
    "            # 验证集评估\n",
    "            model.eval()  # 将模型切换为评估模式\n",
    "            with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "                total_loss_val = 0.0\n",
    "                total_auc_val = 0.0\n",
    "                total_acc_val = 0\n",
    "                total_f1_val = 0\n",
    "                total_precision_val = 0\n",
    "                total_recall_val = 0\n",
    "                val_time = 0\n",
    "                for batch_val in val_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "                    batch_val = [data.to(device) for data in batch_val]\n",
    "                    val_batch_feature_tensor_pay_CHONGHE_discrete,\\\n",
    "                    val_batch_feature_tensor_pay_CHONGHE_continue,\\\n",
    "                    val_batch_feature_tensor_target_CHONGHE_discrete,\\\n",
    "                    val_batch_feature_tensor_target_CHONGHE_continue,\\\n",
    "                    val_batch_feature_tensor_pay_CHONGHE_discrete_mask,\\\n",
    "                    val_batch_feature_tensor_pay_CHONGHE_continue_mask,\\\n",
    "                    val_label_tensor = batch_val \n",
    "                    softmax_score_val, sigmoid_score_val, HistoryDimScaling_Weight_Result_val, TargetDimScaling_Weight_Result_val, \\\n",
    "                    target_history_pay_attention_CHONGHE_weight_val = model(val_batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                                                        val_batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                                                        val_batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                                                        val_batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                                                        val_batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                                                        val_batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                                                        val_label_tensor) \n",
    "\n",
    "                    # sigmoid\n",
    "                    sigmoid_score_val = sigmoid_score_val[:, 0]  # (样本数，1)\n",
    "                    sigmoid_score_val = sigmoid_score_val.cpu()# .detach()  # 转为CPU\n",
    "                    val_label_tensor = val_label_tensor[:, 0]  # (样本数，1)\n",
    "                    val_label_tensor = val_label_tensor.cpu()\n",
    "                    val_label_tensor[val_label_tensor == 1] = 0\n",
    "                    val_label_tensor[val_label_tensor == 2] = 1\n",
    "                    # val_label_tensor = torch.where(val_label_tensor == 1, torch.tensor(0), torch.tensor(1))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "                    loss_val = lossfunction(sigmoid_score_val, val_label_tensor.float())\n",
    "                    # softmax\n",
    "                    # softmax_score_val = softmax_score_val[:, 0]  # (样本数，1)\n",
    "                    # softmax_score_val = softmax_score_val.cpu()# .detach()  # 转为CPU\n",
    "                    # val_label_tensor = val_label_tensor[:, 0]  # (样本数，1)\n",
    "                    # val_label_tensor = val_label_tensor.cpu()\n",
    "                    # val_label_tensor = torch.where(val_label_tensor == 1, torch.tensor(0), torch.tensor(1))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "                    # loss_val = lossfunction(softmax_score_val, val_label_tensor.float())\n",
    "    \n",
    "                    # 损失\n",
    "                    total_loss_val += loss_val.item()\n",
    "                    # 计算验证集上的精度\n",
    "                    # predicted_classes_val = (sigmoid_score_val > 0.5).long()\n",
    "                    # total_acc_val += (predicted_classes_val == val_label_tensor).sum().item() / len(val_label_tensor)\n",
    "                    # total_f1_val += f1_score(val_label_tensor, predicted_classes_val)\n",
    "                    # total_recall_val += recall_score(val_label_tensor, predicted_classes_val)\n",
    "                    # precision_val = ((predicted_classes_val == 1) & (val_label_tensor == 1)).sum().item() / (predicted_classes_val == 1).sum().item()\n",
    "                    # total_precision_val += precision_val\n",
    "                    total_auc_val += roc_auc_score(val_label_tensor, sigmoid_score_val)\n",
    "                    # total_auc_val += roc_auc_score(val_label_tensor, softmax_score_val)\n",
    "                    val_time += 1\n",
    "                    print('||--验证：----------',val_time,'个batch运行时间：',datetime.datetime.now(),'-------------')\n",
    "                # 平均损失\n",
    "                average_loss_val = total_loss_val / len(val_loader)\n",
    "                average_auc_val = total_auc_val / len(val_loader)\n",
    "                average_acc_val = total_acc_val / len(val_loader)\n",
    "                average_f1_val = total_f1_val / len(val_loader)\n",
    "                average_precision_val = total_precision_val / len(val_loader)\n",
    "                average_recall_val = total_recall_val / len(val_loader)\n",
    "                print(f\"Validation Loss: {average_loss_val},AUC: {average_auc_val},ACC:{average_acc_val},F1:{average_f1_val},Precision:{average_precision_val},Recall:{average_recall_val}\") \n",
    "\n",
    "                if average_loss_val < best_val_loss:\n",
    "                    best_val_loss = average_loss_val\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"早停策略触发，停止训练在第 {epoch} 个epoch.\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5cf0a32d-b61b-484f-a801-8c16e3d792be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试 Test\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        total_loss_test = 0.0\n",
    "        total_auc_test = 0.0\n",
    "        total_acc_test  = 0\n",
    "        total_f1_test = 0\n",
    "        total_precision_test = 0\n",
    "        total_recall_test = 0\n",
    "        test_time = 0\n",
    "        results = []  # 用于保存结果的列表\n",
    "        for batch_test in test_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "            batch_test = [data.to(device) for data in batch_test]\n",
    "            test_batch_feature_tensor_pay_CHONGHE_discrete,\\\n",
    "            test_batch_feature_tensor_pay_CHONGHE_continue,\\\n",
    "            test_batch_feature_tensor_target_CHONGHE_discrete,\\\n",
    "            test_batch_feature_tensor_target_CHONGHE_continue,\\\n",
    "            ttest_batch_feature_tensor_pay_CHONGHE_discrete_mask,\\\n",
    "            test_batch_feature_tensor_pay_CHONGHE_continue_mask,\\\n",
    "            test_label_tensor= batch_test \n",
    "            softmax_score_test, sigmoid_score_test, HistoryDimScaling_Weight_Result_test, TargetDimScaling_Weight_Result_test, \\\n",
    "            target_history_pay_attention_CHONGHE_weight_test = model(test_batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                                                test_batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                                                test_batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                                                test_batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                                                test_batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                                                test_batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                                                test_label_tensor)  \n",
    "\n",
    "            # sigmoid\n",
    "            sigmoid_score_test = sigmoid_score_test[:, 0]  # (样本数，1)\n",
    "            sigmoid_score_test = sigmoid_score_test.cpu()#.detach()  # 转为CPU\n",
    "            # print('sigmoid_score_test',sigmoid_score_test)\n",
    "            test_label_tensor = test_label_tensor[:, 0]  # (样本数，1)\n",
    "            test_label_tensor = test_label_tensor.cpu()\n",
    "            test_label_tensor[test_label_tensor == 1] = 0\n",
    "            test_label_tensor[test_label_tensor == 2] = 1\n",
    "            # test_label_tensor = torch.where(test_label_tensor == 1, torch.tensor(0), torch.tensor(1))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "            loss_test = lossfunction(sigmoid_score_test, test_label_tensor.float())\n",
    "            # softmax\n",
    "            # softmax_score_test = softmax_score_test[:, 0]  # (样本数，1)\n",
    "            # softmax_score_test = softmax_score_test.cpu()#.detach()  # 转为CPU\n",
    "            # test_label_tensor = test_label_tensor[:, 0]  # (样本数，1)\n",
    "            # test_label_tensor = test_label_tensor.cpu()\n",
    "            # test_label_tensor = torch.where(test_label_tensor == 1, torch.tensor(0), torch.tensor(1))  # 使用 torch.where 将 1 映射为 0，将 2 映射为 1\n",
    "            # loss_test = lossfunction(softmax_score_test, test_label_tensor.float())\n",
    "            # 损失\n",
    "            total_loss_test += loss_test.item()\n",
    "            # 计算验证集上的精度\n",
    "            # predicted_classes_test = (sigmoid_score_test > 0.5).long()\n",
    "            # total_acc_test += (predicted_classes_test == test_label_tensor).sum().item() / len(test_label_tensor)\n",
    "            # total_f1_test += f1_score(test_label_tensor, predicted_classes_test)\n",
    "            # total_recall_test += recall_score(test_label_tensor, predicted_classes_test)\n",
    "            # precision_test = ((predicted_classes_test == 1) & (test_label_tensor == 1)).sum().item() / (predicted_classes_test == 1).sum().item()\n",
    "            # total_precision_test += precision_test\n",
    "            total_auc_test += roc_auc_score(test_label_tensor, sigmoid_score_test)\n",
    "            # total_auc_test += roc_auc_score(test_label_tensor, softmax_score_test)\n",
    "            test_time += 1\n",
    "            print('||--测试：----------',test_time,'个batch运行时间：',datetime.datetime.now(),'-------------')\n",
    "        # 平均损失\n",
    "        average_loss_test = total_loss_test / len(test_loader)\n",
    "        average_auc_test = total_auc_test / len(test_loader)\n",
    "        average_acc_test = total_acc_test / len(test_loader)\n",
    "        average_f1_test = total_f1_test / len(test_loader)\n",
    "        average_precision_test = total_precision_test / len(test_loader)\n",
    "        average_recall_test = total_recall_test / len(test_loader)\n",
    "        print(\n",
    "            f\"Test Loss: {average_loss_test},AUC: {average_auc_test},ACC:{average_acc_test},F1:{average_f1_test},Precision:{average_precision_test},Recall:{average_recall_test}\")\n",
    "        return average_loss_test, average_auc_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ccf0921-8a05-4ee5-beda-d085cdd1e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=:1\n",
      "划分文件已存在，不再进行数据划分\n",
      "数据预处理结束\n",
      "数据划分完成\n",
      "张量生成完成\n",
      "模型搭建完成\n",
      "模型转移到GPU完成\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:12:37.713187 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:12:39.147863 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:12:40.392819 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:12:41.610373 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:12:42.775504 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:12:43.918925 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:12:45.109908 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:12:46.258976 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:12:47.485702 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:12:48.641995 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:12:49.812673 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:12:50.959948 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:12:52.146830 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:12:53.336774 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:12:55.256359 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:12:56.690095 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:12:58.055405 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:12:59.315596 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:13:00.755672 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:13:02.332110 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:13:03.528095 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:13:04.696412 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:13:05.868029 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:13:07.050828 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:13:08.451673 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:13:09.588231 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:13:11.104079 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:13:12.188451 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:13:13.430052 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:13:14.581676 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:13:15.785328 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:13:16.965073 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:13:18.144078 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:13:19.294510 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:13:20.604733 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:13:21.843003 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:13:23.146849 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:13:24.299004 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:13:25.444253 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:13:27.064710 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:13:28.223444 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:13:29.658223 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:13:30.677565 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:13:32.273465 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:13:33.814878 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:13:35.039463 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:13:36.228357 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:13:37.492610 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:13:38.918942 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:13:40.094314 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:13:41.247559 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:13:42.674982 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:13:43.997206 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:13:45.168283 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:13:46.383101 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:13:47.697036 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:13:48.730045 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:13:49.973429 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:13:51.198652 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:13:52.334331 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:13:53.539898 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:13:54.860468 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:13:56.041872 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:13:57.194163 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:13:58.469314 -------------\n",
      "Epoch 5,loss:0.6902045836815467\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:13:58.876562 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:13:59.291654 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:13:59.538997 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:13:59.838344 -------------\n",
      "Validation Loss: 0.693622350692749,AUC: 0.5902355650293202,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:14:01.348758 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:14:02.723814 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:14:04.043095 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:14:05.275996 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:14:06.504691 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:14:07.706157 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:14:09.038260 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:14:10.196963 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:14:11.300206 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:14:12.376893 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:14:13.681764 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:14:15.112469 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:14:16.265949 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:14:17.438100 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:14:18.614893 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:14:19.823584 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:14:21.252362 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:14:22.336184 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:14:23.525021 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:14:24.693864 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:14:25.890518 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:14:27.060666 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:14:28.303145 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:14:29.865415 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:14:31.445587 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:14:32.894525 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:14:34.404647 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:14:35.617312 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:14:36.733884 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:14:38.062993 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:14:39.277103 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:14:40.428545 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:14:41.548652 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:14:42.729759 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:14:43.985896 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:14:45.171966 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:14:46.496110 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:14:47.670445 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:14:49.127671 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:14:50.223899 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:14:51.401708 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:14:52.598756 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:14:53.888397 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:14:55.041964 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:14:56.264111 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:14:57.420193 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:14:58.602409 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:14:59.783783 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:15:00.929145 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:15:02.474721 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:15:03.762446 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:15:04.921311 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:15:06.130762 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:15:07.323334 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:15:08.707606 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:15:09.818421 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:15:11.009047 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:15:12.167101 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:15:13.581736 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:15:14.665840 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:15:15.861796 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:15:17.172949 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:15:18.228560 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:15:19.571439 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:15:21.003682 -------------\n",
      "Epoch 10,loss:0.6883383118189298\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:15:21.266299 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:15:21.547783 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:15:21.877463 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:15:22.154594 -------------\n",
      "Validation Loss: 0.6919376999139786,AUC: 0.6061389171884772,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:15:23.604955 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:15:24.723367 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:15:25.894601 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:15:27.137798 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:15:28.337644 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:15:29.754254 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:15:30.872030 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:15:32.329236 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:15:33.606359 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:15:35.054947 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:15:36.222868 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:15:37.520719 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:15:38.687465 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:15:39.916484 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:15:41.276637 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:15:42.326910 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:15:43.529797 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:15:44.671442 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:15:46.049438 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:15:47.237160 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:15:48.413060 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:15:49.607940 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:15:51.164567 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:15:52.385183 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:15:53.587255 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:15:54.819036 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:15:56.150354 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:15:57.266783 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:15:58.587520 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:15:59.755839 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:16:01.056567 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:16:02.451911 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:16:03.693402 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:16:04.829579 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:16:06.018110 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:16:07.624580 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:16:08.862849 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:16:10.238686 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:16:11.373132 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:16:12.547271 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:16:13.720252 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:16:14.875045 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:16:16.210075 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:16:17.259001 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:16:18.434982 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:16:19.885298 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:16:21.221823 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:16:22.769631 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:16:23.902172 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:16:25.218027 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:16:26.574943 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:16:27.689314 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:16:28.853662 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:16:30.083018 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:16:31.319340 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:16:32.813481 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:16:34.141647 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:16:35.358526 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:16:36.551962 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:16:37.937830 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:16:39.398705 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:16:40.526059 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:16:41.749358 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:16:43.203581 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:16:44.351445 -------------\n",
      "Epoch 15,loss:0.6746110916137695\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:16:44.612586 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:16:44.890979 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:16:45.134542 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:16:45.477895 -------------\n",
      "Validation Loss: 0.6848389506340027,AUC: 0.6716264426449846,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:16:46.901095 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:16:48.078148 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:16:49.275881 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:16:50.609368 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:16:52.010441 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:16:53.117900 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:16:54.417121 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:16:55.575040 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:16:56.839981 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:16:58.003641 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:16:59.098217 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:17:00.296866 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:17:01.908937 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:17:03.181226 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:17:04.414659 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:17:05.480228 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:17:06.819248 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:17:08.312117 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:17:09.607506 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:17:11.078107 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:17:12.253098 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:17:13.398095 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:17:14.606187 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:17:15.765725 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:17:16.946101 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:17:18.104348 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:17:19.266297 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:17:20.455847 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:17:21.860933 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:17:23.171965 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:17:24.280715 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:17:25.433080 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:17:26.987997 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:17:28.264306 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:17:29.360246 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:17:30.565983 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:17:32.412629 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:17:33.670107 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:17:34.748005 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:17:35.942136 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:17:37.300913 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:17:38.436892 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:17:39.627577 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:17:40.777378 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:17:42.142163 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:17:43.546993 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:17:44.958741 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:17:46.087428 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:17:47.323963 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:17:48.512664 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:17:49.651489 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:17:50.832764 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:17:52.074470 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:17:53.625238 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:17:54.854631 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:17:56.018394 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:17:57.169444 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:17:58.490161 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:17:59.669660 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:18:00.905787 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:18:02.565394 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:18:03.675407 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:18:05.036345 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:18:06.088231 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:18:07.293861 -------------\n",
      "Epoch 20,loss:0.6312348154874948\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:18:07.618422 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:18:08.069900 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:18:08.393275 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:18:08.688317 -------------\n",
      "Validation Loss: 0.6308144927024841,AUC: 0.7233298557561467,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:18:09.960791 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:18:11.385962 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:18:12.550834 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:18:13.751026 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:18:15.147826 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:18:16.296900 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:18:17.537799 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:18:18.687375 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:18:19.878033 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:18:21.210592 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:18:22.299720 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:18:23.490308 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:18:24.664185 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:18:26.001066 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:18:27.320743 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:18:28.467221 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:18:29.774138 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:18:31.276271 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:18:32.909577 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:18:34.285783 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:18:35.485161 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:18:36.803954 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:18:38.285074 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:18:39.486723 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:18:40.835628 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:18:42.185789 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:18:43.454534 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:18:44.753370 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:18:46.184002 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:18:47.428995 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:18:48.649523 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:18:49.953820 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:18:51.198711 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:18:52.470314 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:18:53.662178 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:18:54.797357 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:18:56.166539 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:18:57.458959 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:18:59.062057 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:19:00.427427 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:19:01.871691 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:19:03.544872 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:19:04.757891 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:19:05.949149 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:19:07.366516 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:19:08.697017 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:19:09.897241 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:19:11.140432 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:19:12.498894 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:19:13.591842 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:19:14.956849 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:19:16.148353 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:19:17.364729 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:19:19.057428 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:19:20.279885 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:19:21.521138 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:19:22.831139 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:19:24.351420 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:19:25.582995 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:19:26.717631 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:19:28.127642 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:19:29.506617 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:19:30.924234 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:19:32.540228 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:19:33.929836 -------------\n",
      "Epoch 25,loss:0.6383838286766639\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:19:34.308255 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:19:34.895932 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:19:35.174294 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:19:35.499368 -------------\n",
      "Validation Loss: 0.6374308913946152,AUC: 0.7303199047238791,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:19:36.890255 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:19:38.267443 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:19:39.608550 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:19:40.888211 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:19:42.244230 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:19:43.439039 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:19:44.629398 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:19:45.862904 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:19:47.040465 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:19:48.440584 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:19:49.651036 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:19:51.283667 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:19:52.630068 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:19:53.980185 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:19:55.321769 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:19:56.597558 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:19:57.779748 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:19:59.195276 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:20:00.589122 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:20:02.319503 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:20:03.790959 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:20:05.022905 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:20:06.504347 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:20:07.973779 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:20:09.287041 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:20:10.713013 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:20:12.073326 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:20:13.377276 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:20:14.704814 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:20:15.978332 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:20:17.442466 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:20:18.674122 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:20:19.914134 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:20:21.127748 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:20:22.824673 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:20:24.118764 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:20:25.540460 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:20:26.760887 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:20:28.222672 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:20:29.555066 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:20:30.879167 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:20:32.551064 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:20:33.733872 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:20:35.015528 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:20:36.343138 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:20:37.493801 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:20:39.060978 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:20:40.254968 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:20:41.662947 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:20:42.803974 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:20:44.042523 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:20:45.446368 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:20:46.521128 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:20:47.810199 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:20:49.303449 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:20:50.409036 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:20:51.598356 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:20:52.998040 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:20:54.288788 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:20:55.606633 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:20:57.091900 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:20:58.248895 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:20:59.498911 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:21:00.779119 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:21:02.552697 -------------\n",
      "Epoch 30,loss:0.5822714452560132\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:21:02.813643 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:21:03.206130 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:21:03.573846 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:21:03.970578 -------------\n",
      "Validation Loss: 0.6276651322841644,AUC: 0.743447348761861,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:21:05.302520 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:21:06.642412 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:21:08.027592 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:21:09.313406 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:21:10.783679 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:21:12.117322 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:21:13.391828 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:21:14.569151 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:21:15.972875 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:21:17.315175 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:21:18.488093 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:21:19.670383 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:21:20.909837 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:21:22.047818 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:21:23.291576 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:21:24.639091 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:21:25.892139 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:21:27.644889 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:21:28.839568 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:21:30.214678 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:21:31.762871 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:21:33.028530 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:21:34.434437 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:21:35.773519 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:21:36.923964 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:21:38.521640 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:21:39.737617 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:21:41.128188 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:21:42.454778 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:21:43.909243 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:21:45.296528 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:21:46.522185 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:21:47.872435 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:21:49.243682 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:21:50.885400 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:21:52.231572 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:21:53.493674 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:21:54.845502 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:21:55.971671 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:21:57.124275 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:21:58.501572 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:21:59.735478 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:22:01.029527 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:22:02.452073 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:22:03.693303 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:22:04.934319 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:22:06.111182 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:22:07.478955 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:22:08.691615 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:22:09.867359 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:22:11.093750 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:22:12.465416 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:22:13.583897 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:22:15.062699 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:22:16.169048 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:22:17.424874 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:22:18.671841 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:22:19.769628 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:22:20.989779 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:22:22.091204 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:22:23.407816 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:22:24.590876 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:22:25.747131 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:22:26.921573 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:22:28.237772 -------------\n",
      "Epoch 35,loss:0.5747288327950698\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:22:28.555623 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:22:28.813260 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:22:29.069505 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:22:29.366663 -------------\n",
      "Validation Loss: 0.6301040500402451,AUC: 0.7450435933711979,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:22:31.071430 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:22:32.554059 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:22:33.831349 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:22:34.957319 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:22:36.124355 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:22:37.345836 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:22:38.615559 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:22:39.808970 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:22:41.092426 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:22:42.269678 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:22:43.757406 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:22:44.919335 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:22:46.207974 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:22:47.571390 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:22:49.139062 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:22:50.237081 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:22:51.477375 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:22:52.710925 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:22:53.938488 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:22:55.157316 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:22:56.310995 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:22:57.615463 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:22:58.744105 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:23:00.313972 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:23:02.069533 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:23:03.853425 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:23:04.989974 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:23:06.187209 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:23:07.440129 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:23:08.771527 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:23:09.871933 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:23:11.067386 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:23:12.395298 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:23:13.624301 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:23:14.857087 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:23:16.013544 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:23:17.289256 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:23:18.804485 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:23:19.967323 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:23:21.328917 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:23:22.840115 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:23:24.256450 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:23:25.494292 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:23:26.682102 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:23:28.175981 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:23:29.470459 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:23:30.800557 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:23:32.475560 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:23:33.771359 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:23:35.334657 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:23:36.626830 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:23:37.861666 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:23:39.289520 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:23:40.630561 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:23:42.069014 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:23:43.320086 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:23:44.685087 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:23:45.793069 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:23:47.092574 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:23:48.411924 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:23:49.818479 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:23:51.288494 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:23:52.586852 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:23:53.968089 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:23:55.184969 -------------\n",
      "Epoch 40,loss:0.5705490524952228\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:23:55.441728 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:23:55.856649 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:23:56.125561 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:23:56.408216 -------------\n",
      "Validation Loss: 0.6375581324100494,AUC: 0.7459041846518328,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:23:57.731627 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:23:58.932216 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:24:00.351849 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:24:02.087829 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:24:03.456905 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:24:04.761059 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:24:06.030589 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:24:07.483257 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:24:08.815230 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:24:09.998605 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:24:11.208544 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:24:12.669179 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:24:13.926755 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:24:15.048790 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:24:16.461069 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:24:17.637196 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:24:19.073639 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:24:20.421657 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:24:21.535098 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:24:23.189478 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:24:24.536887 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:24:25.885640 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:24:27.399017 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:24:28.517858 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:24:29.670008 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:24:31.045682 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:24:32.456235 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:24:33.927742 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:24:35.066711 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:24:36.299764 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:24:37.455720 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:24:39.213843 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:24:40.300966 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:24:41.537389 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:24:42.875164 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:24:44.150924 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:24:45.279820 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:24:46.485103 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:24:47.650163 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:24:48.874407 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:24:50.089700 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:24:51.432110 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:24:52.734404 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:24:53.808348 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:24:55.166555 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:24:56.615802 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:24:57.964332 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:24:59.060497 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:25:00.425147 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:25:02.105428 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:25:03.684857 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:25:04.903156 -------------\n",
      "||--训练：---------- 1 个batch运行时间： 2024-03-13 15:25:06.263017 -------------\n",
      "||--训练：---------- 2 个batch运行时间： 2024-03-13 15:25:07.610289 -------------\n",
      "||--训练：---------- 3 个batch运行时间： 2024-03-13 15:25:08.883974 -------------\n",
      "||--训练：---------- 4 个batch运行时间： 2024-03-13 15:25:10.257664 -------------\n",
      "||--训练：---------- 5 个batch运行时间： 2024-03-13 15:25:12.002975 -------------\n",
      "||--训练：---------- 6 个batch运行时间： 2024-03-13 15:25:13.231975 -------------\n",
      "||--训练：---------- 7 个batch运行时间： 2024-03-13 15:25:14.709056 -------------\n",
      "||--训练：---------- 8 个batch运行时间： 2024-03-13 15:25:16.018260 -------------\n",
      "||--训练：---------- 9 个batch运行时间： 2024-03-13 15:25:17.183898 -------------\n",
      "||--训练：---------- 10 个batch运行时间： 2024-03-13 15:25:18.429190 -------------\n",
      "||--训练：---------- 11 个batch运行时间： 2024-03-13 15:25:19.791675 -------------\n",
      "||--训练：---------- 12 个batch运行时间： 2024-03-13 15:25:21.264614 -------------\n",
      "||--训练：---------- 13 个batch运行时间： 2024-03-13 15:25:22.581397 -------------\n",
      "Epoch 45,loss:0.5776680845480698\n",
      "||--验证：---------- 1 个batch运行时间： 2024-03-13 15:25:22.877248 -------------\n",
      "||--验证：---------- 2 个batch运行时间： 2024-03-13 15:25:23.243590 -------------\n",
      "||--验证：---------- 3 个batch运行时间： 2024-03-13 15:25:23.775483 -------------\n",
      "||--验证：---------- 4 个batch运行时间： 2024-03-13 15:25:24.127445 -------------\n",
      "Validation Loss: 0.6331689804792404,AUC: 0.7465152075862928,ACC:0.0,F1:0.0,Precision:0.0,Recall:0.0\n",
      "早停策略触发，停止训练在第 44 个epoch.\n",
      "模型训练完成\n",
      "||--------训练结束时间： 2024-03-13 15:25:24.132835 -------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 575 but got size 128 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 193\u001b[0m\n\u001b[1;32m    185\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(test_batch_feature_tensor_pay_CHONGHE_discrete,\n\u001b[1;32m    186\u001b[0m                                 test_batch_feature_tensor_pay_CHONGHE_continue,\n\u001b[1;32m    187\u001b[0m                                 test_batch_feature_tensor_target_CHONGHE_discrete,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m                                 test_batch_feature_tensor_pay_CHONGHE_continue_mask,\n\u001b[1;32m    191\u001b[0m                                 test_label_tensor)\n\u001b[1;32m    192\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m     average_loss_test, average_auc_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 测试的每个样本结果保存到csv\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# 将本次训练的结果添加到DataFrame中\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m#     test_auc_df = test_auc_df.append({'时间':datetime.datetime.now(),'model':'model3.1','运行位置':'GPU','Type':'Abb_QOE&FUFEI','dataset':data_time_windows,'train_ratio':train_ratio,'feature_embedding':feature_dim,'batchSize':batch_size,'lr':lr,'max_history_len':max_history_len,'实验数': i + 1, '测试集总损失': average_loss_test, 'AUC': average_auc_test}, ignore_index=True)\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# # 将结果保存到CSV文件中\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# test_auc_df.to_csv('./Dataset/maoerDL_result_maoer_pay_pred_model3_1.csv', index=False)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# print('结果已输出')\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m||--------结束时间：\u001b[39m\u001b[38;5;124m'\u001b[39m,datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 24\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_test \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch_test]\n\u001b[1;32m     16\u001b[0m test_batch_feature_tensor_pay_CHONGHE_discrete,\\\n\u001b[1;32m     17\u001b[0m test_batch_feature_tensor_pay_CHONGHE_continue,\\\n\u001b[1;32m     18\u001b[0m test_batch_feature_tensor_target_CHONGHE_discrete,\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m test_batch_feature_tensor_pay_CHONGHE_continue_mask,\\\n\u001b[1;32m     22\u001b[0m test_label_tensor\u001b[38;5;241m=\u001b[39m batch_test \n\u001b[1;32m     23\u001b[0m softmax_score_test, sigmoid_score_test, HistoryDimScaling_Weight_Result_test, TargetDimScaling_Weight_Result_test, \\\n\u001b[0;32m---> 24\u001b[0m target_history_pay_attention_CHONGHE_weight_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch_feature_tensor_pay_CHONGHE_discrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_batch_feature_tensor_pay_CHONGHE_continue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_batch_feature_tensor_target_CHONGHE_discrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_batch_feature_tensor_target_CHONGHE_continue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_batch_feature_tensor_pay_CHONGHE_discrete_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_batch_feature_tensor_pay_CHONGHE_continue_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_label_tensor\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# sigmoid\u001b[39;00m\n\u001b[1;32m     33\u001b[0m sigmoid_score_test \u001b[38;5;241m=\u001b[39m sigmoid_score_test[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (样本数，1)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/maoer_DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/maoer_DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[90], line 50\u001b[0m, in \u001b[0;36mMatchingModel.forward\u001b[0;34m(self, batch_feature_tensor_pay_CHONGHE_discrete, batch_feature_tensor_pay_CHONGHE_continue, batch_feature_tensor_target_CHONGHE_discrete, batch_feature_tensor_target_CHONGHE_continue, batch_feature_tensor_pay_CHONGHE_discrete_mask, batch_feature_tensor_pay_CHONGHE_continue_mask, label_tensor)\u001b[0m\n\u001b[1;32m     43\u001b[0m target_CHONGHE_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_embedding_layer(batch_feature_tensor_target_CHONGHE_discrete,batch_feature_tensor_target_CHONGHE_continue)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print('user_history_pay_FUFEI_vec size=',user_history_pay_FUFEI_vec.size())\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print('target_QOE_vec size=',target_QOE_vec.size())\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# User History & Target Attention层\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 合并mask输入  \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(\"Shape of mask tensor:\", batch_feature_tensor_pay_QOE_discrete_mask.shape,batch_feature_tensor_pay_QOE_continue_mask.shape)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# pay_QOE_mask = torch.cat((batch_feature_tensor_pay_QOE_discrete_mask, batch_feature_tensor_pay_QOE_continue_mask), dim=2)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m pay_CHONGHE_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_feature_tensor_pay_CHONGHE_discrete_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_feature_tensor_pay_CHONGHE_continue_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# pay_FUFEI_mask = torch.cat((batch_feature_tensor_pay_FUFEI_discrete_mask, batch_feature_tensor_pay_FUFEI_continue_mask), dim=2)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m HistoryDimScaling_Weight_Result,  se_user_history_pay_CHONGHE_vec\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_pay_attention_layer( user_history_pay_CHONGHE_vec, pay_CHONGHE_mask\u001b[38;5;241m=\u001b[39mpay_CHONGHE_mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 575 but got size 128 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# 模型运行\n",
    "\n",
    "# 创建一个空的DataFrame来存储结果\n",
    "test_auc_df = pd.DataFrame(columns=['时间','model','运行位置','Type','dataset','train_ratio','feature_embedding','batchSize','lr','max_history_len','实验数', '测试集总损失', 'AUC'])\n",
    "for i in range(5):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    print(f\"i=:{i+1}\")\n",
    "    n = i\n",
    "    # 数据集 train、val、test划分及总数据hash表(以user_id为key的存储对应对应行的hash表)及不同类特征数存储的字典\n",
    "    train_list, val_list, test_list, data_hash, feature_category_num_dict = data_input(data_time_windows, path, dataset_spilt_path, train_ratio, val_ratio, test_ratio, total_continue_feature)\n",
    "    # 获取训练、验证、测试集对应的数据形成的向量hash存储及label\n",
    "    train_data_tensor_hash, train_label, train_data_tensor_hash_history_mask = get_feature_to_matrix(train_list, data_hash, feature_column_dict)\n",
    "    val_data_tensor_hash, val_label, val_data_tensor_hash_history_mask = get_feature_to_matrix(val_list, data_hash, feature_column_dict)\n",
    "    test_data_tensor_hash, test_label, test_data_tensor_hash_history_mask = get_feature_to_matrix(test_list, data_hash, feature_column_dict)\n",
    "    # 输出查看结果\n",
    "    # for key1 in train_data_tensor_hash.keys():\n",
    "    #     dimensions1 = train_data_tensor_hash[key1]['pay_QOE_continue'].size()\n",
    "    #     dimensions2 = train_data_tensor_hash[key1]['pay_QOE_discrete'].size()\n",
    "    #     dimensions3 = train_data_tensor_hash[key1]['pay_CHONGHE_continue'].size()\n",
    "    #     dimensions4 = train_data_tensor_hash[key1]['target_QOE_continue'].size()\n",
    "    #     dimensions5 = train_data_tensor_hash[key1]['target_QOE_discrete'].size()\n",
    "    #     dimensions6 = train_data_tensor_hash[key1]['target_CHONGHE_continue'].size()\n",
    "    #     print(\"val_data_tensor_hash size=\", dimensions1,dimensions2,dimensions3,dimensions4,dimensions5,dimensions6)\n",
    "\n",
    "    # 生成batch再添加维度对齐张量（三个维度）这里张量输出的全是三维 (batch_size, 1 or max_history_len, feature_num)\n",
    "    train_batch_feature_tensor_dict = generate_user_feature_alignment_tensor(train_list,train_data_tensor_hash)\n",
    "    val_batch_feature_tensor_dict = generate_user_feature_alignment_tensor(val_list,val_data_tensor_hash)\n",
    "    test_batch_feature_tensor_dict = generate_user_feature_alignment_tensor(test_list,test_data_tensor_hash)\n",
    "    train_label_tensor = torch.tensor(train_label)\n",
    "    val_label_tensor = torch.tensor(val_label)\n",
    "    test_label_tensor = torch.tensor(test_label)\n",
    "    train_label_tensor = train_label_tensor.unsqueeze(-1)\n",
    "    val_label_tensor = val_label_tensor.unsqueeze(-1)\n",
    "    test_label_tensor = test_label_tensor.unsqueeze(-1)  # 在最后新增一个维度，因为TensorDataset要第一维大小相同 label变为(batch,1)\n",
    "    # mask矩阵的字典\n",
    "    train_batch_feature_tensor_history_mask_dict = generate_user_feature_alignment_tensor(train_list,train_data_tensor_hash_history_mask, is_mask=True)\n",
    "    val_batch_feature_tensor_history_mask_dict = generate_user_feature_alignment_tensor(val_list,val_data_tensor_hash_history_mask, is_mask=True)\n",
    "    test_batch_feature_tensor_history_mask_dict = generate_user_feature_alignment_tensor(test_list,test_data_tensor_hash_history_mask, is_mask=True)\n",
    "    print('张量生成完成')\n",
    "    \n",
    "    # # TensorDataset输入得是张量，因此由字典转为张量\n",
    "    # train_batch_feature_tensor_pay_QOE_discrete = train_batch_feature_tensor_dict['pay_QOE_discrete']\n",
    "    train_batch_feature_tensor_pay_CHONGHE_discrete = train_batch_feature_tensor_dict['pay_CHONGHE_discrete']\n",
    "    # train_batch_feature_tensor_pay_FUFEI_discrete = train_batch_feature_tensor_dict['pay_FUFEI_discrete']\n",
    "    # train_batch_feature_tensor_pay_QOE_continue = train_batch_feature_tensor_dict['pay_QOE_continue']\n",
    "    train_batch_feature_tensor_pay_CHONGHE_continue = train_batch_feature_tensor_dict['pay_CHONGHE_continue']\n",
    "    # train_batch_feature_tensor_pay_FUFEI_continue = train_batch_feature_tensor_dict['pay_FUFEI_continue']\n",
    "    # train_batch_feature_tensor_target_QOE_discrete = train_batch_feature_tensor_dict['target_QOE_discrete']\n",
    "    train_batch_feature_tensor_target_CHONGHE_discrete = train_batch_feature_tensor_dict['target_CHONGHE_discrete']\n",
    "    # train_batch_feature_tensor_target_FUFEI_discrete = train_batch_feature_tensor_dict['target_FUFEI_discrete']\n",
    "    # train_batch_feature_tensor_target_QOE_continue = train_batch_feature_tensor_dict['target_QOE_continue']\n",
    "    train_batch_feature_tensor_target_CHONGHE_continue = train_batch_feature_tensor_dict['target_CHONGHE_continue']\n",
    "    # train_batch_feature_tensor_target_FUFEI_continue = train_batch_feature_tensor_dict['target_FUFEI_continue']\n",
    "    # train_batch_feature_tensor_pay_QOE_discrete_mask = train_batch_feature_tensor_history_mask_dict['pay_QOE_discrete']\n",
    "    train_batch_feature_tensor_pay_CHONGHE_discrete_mask = train_batch_feature_tensor_history_mask_dict['pay_CHONGHE_discrete']\n",
    "    # train_batch_feature_tensor_pay_FUFEI_discrete_mask = train_batch_feature_tensor_history_mask_dict['pay_FUFEI_discrete']\n",
    "    # train_batch_feature_tensor_pay_QOE_continue_mask = train_batch_feature_tensor_history_mask_dict['pay_QOE_continue']\n",
    "    train_batch_feature_tensor_pay_CHONGHE_continue_mask = train_batch_feature_tensor_history_mask_dict['pay_CHONGHE_continue']\n",
    "    # train_batch_feature_tensor_pay_FUFEI_continue_mask = train_batch_feature_tensor_history_mask_dict['pay_FUFEI_continue']\n",
    "\n",
    "    # val_batch_feature_tensor_pay_QOE_discrete = val_batch_feature_tensor_dict['pay_QOE_discrete']\n",
    "    val_batch_feature_tensor_pay_CHONGHE_discrete = val_batch_feature_tensor_dict['pay_CHONGHE_discrete']\n",
    "    # val_batch_feature_tensor_pay_FUFEI_discrete = val_batch_feature_tensor_dict['pay_FUFEI_discrete']\n",
    "    # val_batch_feature_tensor_pay_QOE_continue = val_batch_feature_tensor_dict['pay_QOE_continue']\n",
    "    val_batch_feature_tensor_pay_CHONGHE_continue = val_batch_feature_tensor_dict['pay_CHONGHE_continue']\n",
    "    # val_batch_feature_tensor_pay_FUFEI_continue = val_batch_feature_tensor_dict['pay_FUFEI_continue']\n",
    "    # val_batch_feature_tensor_target_QOE_discrete = val_batch_feature_tensor_dict['target_QOE_discrete']\n",
    "    val_batch_feature_tensor_target_CHONGHE_discrete = val_batch_feature_tensor_dict['target_CHONGHE_discrete']\n",
    "    # val_batch_feature_tensor_target_FUFEI_discrete = val_batch_feature_tensor_dict['target_FUFEI_discrete']\n",
    "    # val_batch_feature_tensor_target_QOE_continue = val_batch_feature_tensor_dict['target_QOE_continue']\n",
    "    val_batch_feature_tensor_target_CHONGHE_continue = val_batch_feature_tensor_dict['target_CHONGHE_continue']\n",
    "    # val_batch_feature_tensor_target_FUFEI_continue = val_batch_feature_tensor_dict['target_FUFEI_continue']\n",
    "    # val_batch_feature_tensor_pay_QOE_discrete_mask = val_batch_feature_tensor_history_mask_dict['pay_QOE_discrete']\n",
    "    val_batch_feature_tensor_pay_CHONGHE_discrete_mask = val_batch_feature_tensor_history_mask_dict['pay_CHONGHE_discrete']\n",
    "    # val_batch_feature_tensor_pay_FUFEI_discrete_mask = val_batch_feature_tensor_history_mask_dict['pay_FUFEI_discrete']\n",
    "    # val_batch_feature_tensor_pay_QOE_continue_mask = val_batch_feature_tensor_history_mask_dict['pay_QOE_continue']\n",
    "    val_batch_feature_tensor_pay_CHONGHE_continue_mask = val_batch_feature_tensor_history_mask_dict['pay_CHONGHE_continue']\n",
    "    # val_batch_feature_tensor_pay_FUFEI_continue_mask = val_batch_feature_tensor_history_mask_dict['pay_FUFEI_continue']\n",
    "    \n",
    "    # test_batch_feature_tensor_pay_QOE_discrete = test_batch_feature_tensor_dict['pay_QOE_discrete']\n",
    "    test_batch_feature_tensor_pay_CHONGHE_discrete = test_batch_feature_tensor_dict['pay_CHONGHE_discrete']\n",
    "    # test_batch_feature_tensor_pay_FUFEI_discrete = test_batch_feature_tensor_dict['pay_FUFEI_discrete']\n",
    "    # test_batch_feature_tensor_pay_QOE_continue = test_batch_feature_tensor_dict['pay_QOE_continue']\n",
    "    test_batch_feature_tensor_pay_CHONGHE_continue = test_batch_feature_tensor_dict['pay_CHONGHE_continue']\n",
    "    # test_batch_feature_tensor_pay_FUFEI_continue = test_batch_feature_tensor_dict['pay_FUFEI_continue']\n",
    "    # test_batch_feature_tensor_target_QOE_discrete = test_batch_feature_tensor_dict['target_QOE_discrete']\n",
    "    test_batch_feature_tensor_target_CHONGHE_discrete = test_batch_feature_tensor_dict['target_CHONGHE_discrete']\n",
    "    # test_batch_feature_tensor_target_FUFEI_discrete = test_batch_feature_tensor_dict['target_FUFEI_discrete']\n",
    "    # test_batch_feature_tensor_target_QOE_continue = test_batch_feature_tensor_dict['target_QOE_continue']\n",
    "    test_batch_feature_tensor_target_CHONGHE_continue = test_batch_feature_tensor_dict['target_CHONGHE_continue']\n",
    "    # test_batch_feature_tensor_target_FUFEI_continue = test_batch_feature_tensor_dict['target_FUFEI_continue']\n",
    "    # test_batch_feature_tensor_pay_QOE_discrete_mask = test_batch_feature_tensor_history_mask_dict['pay_QOE_discrete']\n",
    "    test_batch_feature_tensor_pay_CHONGHE_discrete_mask = test_batch_feature_tensor_history_mask_dict['pay_CHONGHE_discrete']\n",
    "    # test_batch_feature_tensor_pay_FUFEI_discrete_mask = test_batch_feature_tensor_history_mask_dict['pay_FUFEI_discrete']\n",
    "    # test_batch_feature_tensor_pay_QOE_continue_mask = test_batch_feature_tensor_history_mask_dict['pay_QOE_continue']\n",
    "    test_batch_feature_tensor_pay_CHONGHE_continue_mask = test_batch_feature_tensor_history_mask_dict['pay_CHONGHE_continue']\n",
    "    # test_batch_feature_tensor_pay_FUFEI_continue_mask = test_batch_feature_tensor_history_mask_dict['pay_FUFEI_continue']\n",
    "\n",
    "    # 训练集\n",
    "    train_dataset = TensorDataset(train_batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                train_batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                train_batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                train_batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                train_batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                train_batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                train_label_tensor)\n",
    "    val_dataset = TensorDataset(val_batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                val_batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                val_batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                val_batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                val_batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                val_batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                val_label_tensor)\n",
    "\n",
    "    # 旧\n",
    "    # train_batch_feature_tensor = list(train_batch_feature_tensor_dict.values())\n",
    "    # train_batch_feature_tensor_history_mask = list(train_batch_feature_tensor_history_mask_dict.values())\n",
    "    # val_batch_feature_tensor = list(val_batch_feature_tensor_dict.values())\n",
    "    # val_batch_feature_tensor_history_mask = list(val_batch_feature_tensor_history_mask_dict.values())\n",
    "    # test_batch_feature_tensor = list(test_batch_feature_tensor_dict.values())\n",
    "    # test_batch_feature_tensor_history_mask = list(test_batch_feature_tensor_history_mask_dict.values())  \n",
    "    # # 训练集\n",
    "    # train_dataset = TensorDataset(*train_batch_feature_tensor, *train_batch_feature_tensor_history_mask, train_label_tensor)\n",
    "    # val_dataset = TensorDataset(*val_batch_feature_tensor, *val_batch_feature_tensor_history_mask, val_label_tensor)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)  # 记得改回随机\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # 确保您的计算机上有CUDA支持的GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 创建大模型的实例\n",
    "    model = MatchingModel(feature_category_num_dict, feature_column_dict, continue_embedding_dim,\n",
    "                 discrete_embedding_dim, num_heads, feature_dim, max_history_len)\n",
    "    print('模型搭建完成')\n",
    "    model.to(device)\n",
    "    # 进一步处理 列表转移到GPU\n",
    "    # for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_QOE_discrete_embeddings)):\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_QOE_discrete_embeddings[i] = \\\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_QOE_discrete_embeddings[i].to(device)\n",
    "    for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_discrete_embeddings)):\n",
    "        model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_discrete_embeddings[i] = \\\n",
    "        model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_discrete_embeddings[i].to(device)\n",
    "    # for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_FUFEI_discrete_embeddings)):\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_FUFEI_discrete_embeddings[i] = \\\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_FUFEI_discrete_embeddings[i].to(device)\n",
    "    # for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_QOE_continue_embedding)):\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_QOE_continue_embedding[i] = \\\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_QOE_continue_embedding[i].to(device)\n",
    "    for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_continue_embedding)):\n",
    "        model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_continue_embedding[i] = \\\n",
    "        model.user_history_pay_embedding_layer.user_pay_history_CHONGHE_continue_embedding[i].to(device)\n",
    "    # for i in range(len(model.user_history_pay_embedding_layer.user_pay_history_FUFEI_continue_embedding)):\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_FUFEI_continue_embedding[i] = \\\n",
    "    #     model.user_history_pay_embedding_layer.user_pay_history_FUFEI_continue_embedding[i].to(device)\n",
    "    \n",
    "    # for i in range(len(model.target_embedding_layer.target_QOE_discrete_embeddings)):\n",
    "    #     model.target_embedding_layer.target_QOE_discrete_embeddings[i] = \\\n",
    "    #     model.target_embedding_layer.target_QOE_discrete_embeddings[i].to(device)\n",
    "    for i in range(len(model.target_embedding_layer.target_CHONGHE_discrete_embeddings)):\n",
    "        model.target_embedding_layer.target_CHONGHE_discrete_embeddings[i] = \\\n",
    "        model.target_embedding_layer.target_CHONGHE_discrete_embeddings[i].to(device)\n",
    "    # for i in range(len(model.target_embedding_layer.target_FUFEI_discrete_embeddings)):\n",
    "    #     model.target_embedding_layer.target_FUFEI_discrete_embeddings[i] = \\\n",
    "    #     model.target_embedding_layer.target_FUFEI_discrete_embeddings[i].to(device)\n",
    "    # for i in range(len(model.target_embedding_layer.target_QOE_continue_embedding)):\n",
    "    #     model.target_embedding_layer.target_QOE_continue_embedding[i] = \\\n",
    "    #     model.target_embedding_layer.target_QOE_continue_embedding[i].to(device)\n",
    "    for i in range(len(model.target_embedding_layer.target_CHONGHE_continue_embedding)):\n",
    "        model.target_embedding_layer.target_CHONGHE_continue_embedding[i] = \\\n",
    "        model.target_embedding_layer.target_CHONGHE_continue_embedding[i].to(device)\n",
    "    # for i in range(len(model.target_embedding_layer.target_FUFEI_continue_embedding)):\n",
    "    #     model.target_embedding_layer.target_FUFEI_continue_embedding[i] = \\\n",
    "    #     model.target_embedding_layer.target_FUFEI_continue_embedding[i].to(device)\n",
    "    print('模型转移到GPU完成')\n",
    "    lossfunction = nn.BCELoss()\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9)\n",
    "\n",
    "    # 训练\n",
    "    model_training(model, train_loader, val_loader, lossfunction, optimizer, 500, device)\n",
    "    print('模型训练完成')\n",
    "    print('||--------训练结束时间：',datetime.datetime.now(),'-------------')\n",
    "    # 测试\n",
    "    test_dataset = TensorDataset(test_batch_feature_tensor_pay_CHONGHE_discrete,\n",
    "                                test_batch_feature_tensor_pay_CHONGHE_continue,\n",
    "                                test_batch_feature_tensor_target_CHONGHE_discrete,\n",
    "                                test_batch_feature_tensor_target_CHONGHE_continue,\n",
    "                                test_batch_feature_tensor_pay_CHONGHE_discrete_mask,\n",
    "                                test_batch_feature_tensor_pay_CHONGHE_continue_mask,\n",
    "                                test_label_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    average_loss_test, average_auc_test = test_model(model, test_loader)\n",
    "    # 测试的每个样本结果保存到csv\n",
    "    # 将本次训练的结果添加到DataFrame中\n",
    "#     test_auc_df = test_auc_df.append({'时间':datetime.datetime.now(),'model':'model3.1','运行位置':'GPU','Type':'Abb_QOE&FUFEI','dataset':data_time_windows,'train_ratio':train_ratio,'feature_embedding':feature_dim,'batchSize':batch_size,'lr':lr,'max_history_len':max_history_len,'实验数': i + 1, '测试集总损失': average_loss_test, 'AUC': average_auc_test}, ignore_index=True)\n",
    "# # 将结果保存到CSV文件中\n",
    "# test_auc_df.to_csv('./Dataset/maoerDL_result_maoer_pay_pred_model3_1.csv', index=False)\n",
    "# print('结果已输出')\n",
    "print('||--------结束时间：',datetime.datetime.now(),'-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4de78-e6d2-4b0c-ab58-5a7cc163e30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec32e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
