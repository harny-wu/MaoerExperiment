{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c1e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "house_path='dataset//selected_feature_num_can_house_distinct__201901_202103.csv'\n",
    "host_path='dataset//selected_feature_num_can_host_distinct_201901_202103.csv'\n",
    "host_pic_path='dataset//feature_img_can_host_distinct_201901_202103.csv'\n",
    "comment_path='new_dataset//selected_feature_can_comment_basic_201901_202103.csv'\n",
    "\n",
    "\n",
    "house_id_column='HOUSE_ID'\n",
    "host_id_column='host_id'\n",
    "host_pic_id_column='host_id'\n",
    "comment_id_column='comment_id'\n",
    "neg_comment_id_column='pos_comment_id'\n",
    "house_continuous_column=['HOUSE_PRICE_DAY','HOUSE_BATHROOM_NUM', 'HOUSE_BEDROOM_NUM', 'HOUSE_BED_NUM', 'HOUSE_DESCRIPTION_LEN', 'HOUSE_ACCOMMODATES',\n",
    "         'HOUSE_NAME_LEN', 'HOUSE_NEIGHBOUR_OVERVIEW_LEN','HOUSE_REVIEW_SCORE_CLEAN',\n",
    "         'HOUSE_REVIEW_SCORE_ACCURACY', 'HOUSE_REVIEW_SCORE_CHECKIN', 'HOUSE_REVIEW_SCORE_COMMUNICATION',\n",
    "         'HOUSE_REVIEW_SCORE_VALUE', 'HOUSE_REVIEW_SCORE_LOCATION','HOUSE_REVIEW_SCORE_RATE']\n",
    "\n",
    "host_continuous_column=['HOST_ABOUT_LEN', 'HOST_ACCEPTANCE_RATE', 'HOST_HOUSE_NUM', 'HOST_NAME_LEN', 'HOST_RESPONSE_RATE',\n",
    "         'HOST_VERIFICATION_NUM', 'info_amount', 'readability', 'user_about_t1', 'user_about_t3', 'user_about_t4',\n",
    "         'user_about_t2','HOST_HOUSE_REVIEW_SCORE_COMMUNICAT_AVG']\n",
    "\n",
    "\n",
    "\n",
    "house_comment_continuous_column=['house_tran_review_num','house_tran_reviewer_num', 'house_tran_review_1y_num',\n",
    "                                 'house_tran_review_30d_num',  'house_tran_comment_subjectivity_score_avg',\n",
    "                                 'house_tran_comment_plarity_score_avg', 'house_tran_review_theme_RoomService_avg',\n",
    "                                 'house_tran_review_theme_IndoorEnvironment_avg', 'house_tran_review_theme_NeighborFacilities_avg',\n",
    "                                 'house_tran_review_Transportation_avg', 'house_tran_review_BookingRelus_avg',\n",
    "                                 'house_tran_review_TouristScenery_avg','house_tran_review_HostServices_avg',\n",
    "                                 'house_tran_review_theme_HouseFacilities_avg', 'house_tran_review_theme_AccommodExperience_avg',\n",
    "                                 'house_tran_review_pos_num', 'house_tran_review_neg_num',\n",
    "                                 'interval_house_first_review', 'interval_house_previous_review']\n",
    "\n",
    "host_comment_continuous_column=['host_tran_review_num']\n",
    "\n",
    "house_categorical_column1=['HOUSE_IS_INSTANT_BOOKABLE']\n",
    "host_categorical_column1=['HOST_HAS_PROFILE_PIC', 'HOST_IS_IDENTITY_VERIFIED', 'HOST_IS_SUPERHOST', 'HOST_NAME_HAS_DIG',\n",
    "         'HOST_NAME_HAS_ENGLISH']\n",
    "house_categorical_column2=['HOUSE_PROPERTY_TYPE', 'HOUSE_ROOM_TYPE']\n",
    "host_categorical_column2=['HOST_RESPONSE_TIME']\n",
    "\n",
    "house_multi_column=['HOUSE_HAS_AMENITIES_BALCONY',\n",
    " 'HOUSE_HAS_AMENITIES_BATHTUB',\n",
    " 'HOUSE_HAS_AMENITIES_BED_LINENS',\n",
    " 'HOUSE_HAS_AMENITIES_CO_DETECTOR',\n",
    " 'HOUSE_HAS_AMENITIES_CRIB',\n",
    " 'HOUSE_HAS_AMENITIES_DRYER',\n",
    " 'HOUSE_HAS_AMENITIES_ETHERNET',\n",
    " 'HOUSE_HAS_AMENITIES_FREE_PARKING',\n",
    " 'HOUSE_HAS_AMENITIES_HANGERS',\n",
    " 'HOUSE_HAS_AMENITIES_KID_FRIENDLY',\n",
    " 'HOUSE_HAS_AMENITIES_LOCK_BEDROOM_DOOR',\n",
    " 'HOUSE_HAS_AMENITIES_PAID_PARKING',\n",
    " 'HOUSE_HAS_AMENITIES_WASHER']\n",
    "\n",
    "host_pic_continuous_column=['host_pic_age_min',\n",
    " 'host_pic_beauty_female_score_max',\n",
    " 'host_pic_beauty_female_score_min',\n",
    " 'host_pic_beauty_female_score_sum',\n",
    " 'host_pic_beauty_male_score_avg',\n",
    " 'host_pic_beauty_male_score_max',\n",
    " 'host_pic_beauty_male_score_min',\n",
    " 'host_pic_beauty_male_score_sum',\n",
    " 'host_pic_blur_blurness_avg',\n",
    " 'host_pic_blur_blurness_sum',\n",
    " 'host_pic_blur_gaussianblur_avg',\n",
    " 'host_pic_emotion_disgust_std',\n",
    " 'host_pic_emotion_neutral_avg',\n",
    " 'host_pic_head_pose_pitch_sum',\n",
    " 'host_pic_head_pose_yal_avg',\n",
    " 'host_pic_male_age_min',\n",
    " 'host_pic_mouth_close_max',\n",
    " 'host_pic_mouth_occlusion_sum']\n",
    "\n",
    "\n",
    "npratio=50 #负样本数\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "num_heads=8\n",
    "\n",
    "feature_dim=128\n",
    "max_history_len=15\n",
    "num_experts=3\n",
    "num_tasks=2\n",
    "multi_embedding_dim = 18  # 设置嵌入维度\n",
    "expert_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "gate_hidden_units=[[feature_dim,feature_dim//2],[feature_dim//2,num_experts]]\n",
    "classifier_dnn_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "weight_dnn_hidden_units=[[feature_dim*3,feature_dim],[feature_dim,2]]\n",
    "user_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "house_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "attetion_dnn_hidden_units=[[4*feature_dim,2*feature_dim],[2*feature_dim,feature_dim]]\n",
    "lr=0.0001\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c0c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_history_path1 = 'new_dataset2//can_history_201801.csv'\n",
    "val_user_history_path1 = 'new_dataset2//can_history_201805.csv'\n",
    "test_user_history_path1 = 'new_dataset2//can_history_201806.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path1='new_dataset2//new_can_comment_neg_201801_04_50neg.csv'\n",
    "val_neg_comment_path1='new_dataset2//new_can_comment_neg_201805_50neg.csv'\n",
    "test_neg_comment_path1='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "\n",
    "sclar_path1='new_dataset/all_feature+label_201801_04_50neg.csv'\n",
    "\n",
    "\n",
    "train_user_history_path2 = 'new_dataset2//can_history_201802_05.csv'\n",
    "val_user_history_path2 = 'new_dataset2//can_history_201806.csv'\n",
    "test_user_history_path2 = 'new_dataset2//can_history_201807.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path2='new_dataset2//new_can_comment_neg_201802_05_50neg.csv'\n",
    "val_neg_comment_path2='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "test_neg_comment_path2='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "\n",
    "sclar_path2='new_dataset2/all_feature+label_201802_05_50neg.csv'\n",
    "\n",
    "train_user_history_path3 = 'new_dataset2//can_history_201803_06.csv'\n",
    "val_user_history_path3 = 'new_dataset2//can_history_201807.csv'\n",
    "test_user_history_path3 = 'new_dataset2//can_history_201808.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path3='new_dataset2//new_can_comment_neg_201803_06_50neg.csv'\n",
    "val_neg_comment_path3='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "test_neg_comment_path3='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "\n",
    "sclar_path3='new_dataset2/all_feature+label_201803_06_50neg.csv'\n",
    "\n",
    "train_user_history_path4 = 'new_dataset2//can_history_201804_07.csv'\n",
    "val_user_history_path4 = 'new_dataset2//can_history_201808.csv'\n",
    "test_user_history_path4 = 'new_dataset2//can_history_201809.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path4='new_dataset2//new_can_comment_neg_201804_07_50neg.csv'\n",
    "val_neg_comment_path4='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "test_neg_comment_path4='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "\n",
    "sclar_path4='new_dataset2/all_feature+label_201804_07_50neg.csv'\n",
    "\n",
    "train_user_history_path5 = 'new_dataset2//can_history_201805_08.csv'\n",
    "val_user_history_path5 = 'new_dataset2//can_history_201809.csv'\n",
    "test_user_history_path5 = 'new_dataset2//can_history_201810.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path5='new_dataset2//new_can_comment_neg_201805_08_50neg.csv'\n",
    "val_neg_comment_path5='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "test_neg_comment_path5='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "\n",
    "sclar_path5='new_dataset2/all_feature+label_201805_08_50neg.csv'\n",
    "\n",
    "train_user_history_path6 = 'new_dataset2//can_history_201806_09.csv'\n",
    "val_user_history_path6 = 'new_dataset2//can_history_201810.csv'\n",
    "test_user_history_path6 = 'new_dataset2//can_history_201811.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path6='new_dataset2//new_can_comment_neg_201806_09_50neg.csv'\n",
    "val_neg_comment_path6='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "test_neg_comment_path6='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "\n",
    "sclar_path6='new_dataset2/all_feature+label_201806_09_50neg.csv'\n",
    "\n",
    "train_user_history_path7 = 'new_dataset2//can_history_201807_10.csv'\n",
    "val_user_history_path7 = 'new_dataset2//can_history_201811.csv'\n",
    "test_user_history_path7 = 'new_dataset2//can_history_201812.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path7='new_dataset2//new_can_comment_neg_201807_10_50neg.csv'\n",
    "val_neg_comment_path7='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "test_neg_comment_path7='new_dataset2//new_can_comment_neg_201812_50neg.csv'\n",
    "\n",
    "sclar_path7='new_dataset2/all_feature+label_201807_10_50neg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_history_paths=[train_user_history_path1,train_user_history_path2,train_user_history_path3,train_user_history_path4,train_user_history_path5,train_user_history_path6,train_user_history_path7]\n",
    "val_user_history_paths=[val_user_history_path1,val_user_history_path2,val_user_history_path3,val_user_history_path4,val_user_history_path5,val_user_history_path6,val_user_history_path7]\n",
    "test_user_history_paths=[test_user_history_path1,test_user_history_path2,test_user_history_path3,test_user_history_path4,test_user_history_path5,test_user_history_path6,test_user_history_path7]\n",
    "\n",
    "train_neg_comment_paths=[train_neg_comment_path1,train_neg_comment_path2,train_neg_comment_path3,train_neg_comment_path4,train_neg_comment_path5,train_neg_comment_path6,train_neg_comment_path7]\n",
    "val_neg_comment_paths=[val_neg_comment_path1,val_neg_comment_path2,val_neg_comment_path3,val_neg_comment_path4,val_neg_comment_path5,val_neg_comment_path6,val_neg_comment_path7]\n",
    "test_neg_comment_paths=[test_neg_comment_path1,test_neg_comment_path2,test_neg_comment_path3,test_neg_comment_path4,test_neg_comment_path5,test_neg_comment_path6,test_neg_comment_path7]\n",
    "\n",
    "sclar_paths=[sclar_path1,sclar_path2,sclar_path3,sclar_path4,sclar_path5,sclar_path6,sclar_path7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f938d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取house或host的静态特征\n",
    "def read_house_host(path, id_column, object, continuous_column, categorical_column1, categorical_column2):\n",
    "    # 从CSV文件中读取房屋信息\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "\n",
    "    # 提取连续特征和类别特征,multi-hot特征\n",
    "    continuous_features = data[continuous_column]\n",
    "    categorical_features1 = data[categorical_column1]  # 类别特征列\n",
    "    categorical_features2 = data[categorical_column2]\n",
    "\n",
    "    # 使用LabelEncoder将类别特征转换为编号\n",
    "    label_encoders = {}  # 存储特征列对应的编码器对象\n",
    "    categorical_encodings = {}  # 存储特征值对应的编号\n",
    "    for col in categorical_features2.columns:\n",
    "        le = LabelEncoder()\n",
    "        categorical_features2.loc[:, col] = le.fit_transform(categorical_features2[col])\n",
    "        label_encoders[col] = le\n",
    "        categorical_encodings[col] = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    # 得到每个分类特征的类别数\n",
    "    num_categories_list2 = [len(le.classes_) for le in label_encoders.values()]\n",
    "    \n",
    "    \n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        multi_features =  data.iloc[:, 10:52]\n",
    "        #补缺失值，2\n",
    "        multi_features=multi_features.fillna(-2)\n",
    "        house_multi_matrix = np.array(multi_features)\n",
    "        # 创建一个与最后一行维度相同的 NaN 数组\n",
    "        nan_row = np.full((1, house_multi_matrix.shape[1]),-2)\n",
    "        # 将 NaN 数组插入到 house_multi_matrix 的最后一行\n",
    "        house_multi_matrix = np.concatenate((house_multi_matrix, nan_row), axis=0)\n",
    "    else:\n",
    "        # 将连续特征中的百分比特征转换成数值类型\n",
    "        # 找到包含百分数的列\n",
    "        percent_columns = ['HOST_ACCEPTANCE_RATE','HOST_RESPONSE_RATE']  \n",
    "        # 将百分数转换为浮点数\n",
    "        for column in percent_columns:\n",
    "            continuous_features[column] = continuous_features[column].str.rstrip('%').astype(float) / 100.0\n",
    "\n",
    "                        \n",
    "    #补缺失值，连续值补-2，分类1特征补2，分类2特征不用补\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "    categorical_features1=categorical_features1.fillna(2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "    categorical_matrix1 = np.array(categorical_features1).astype(int)\n",
    "    categorical_matrix2 = np.array(categorical_features2)\n",
    "\n",
    "    # 添加一行-2\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    continuous_matrix = np.vstack((continuous_matrix, nan_row))\n",
    "    # 计算每一列的最大值加1\n",
    "    new_row1 = np.full(categorical_matrix1.shape[1], 2)\n",
    "    max_values2 = np.max(categorical_matrix2, axis=0) + 1\n",
    "    # 将最大值加1的行添加到categorical_matrix1\n",
    "    categorical_matrix1 = np.vstack((categorical_matrix1,new_row1))\n",
    "    categorical_matrix2 = np.vstack((categorical_matrix2, max_values2))\n",
    "\n",
    "    \n",
    "\n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, house_multi_matrix, num_categories_list2\n",
    "    else:\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, num_categories_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b21d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取host图片特征\n",
    "def read_host_pic_features(path):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data['host_id'].unique())}\n",
    "    # 提取连续特征\n",
    "    continuous_features = data.iloc[:, 5:]\n",
    "    host_pic_continuous_column = data.iloc[:, 5:].columns.tolist()\n",
    "    \n",
    "    #补缺失值-2\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "        \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "\n",
    "    #当host没有pic特征时，取最后一行，特征为-2\n",
    "    # 创建全为nan的一行\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    # 在continuous_matrix最后添加zero_row\n",
    "    continuous_matrix = np.vstack((continuous_matrix,nan_row))\n",
    "\n",
    "    return index,continuous_matrix,host_pic_continuous_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f81314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取house和host以及负样本动态特征\n",
    "#读取house和host以及负样本动态特征\n",
    "def read_comment_features(path,id_column,house_continuous_column,host_continuous_column):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "    # 提取连续特征\n",
    "    house_continuous_features = data[house_continuous_column]\n",
    "    host_continuous_features = data[host_continuous_column]\n",
    "    #补缺失值-2\n",
    "    house_continuous_features=house_continuous_features.fillna(-2)\n",
    "    host_continuous_features=host_continuous_features.fillna(-2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    house_continuous_matrix = np.array(house_continuous_features)\n",
    "    host_continuous_matrix = np.array(host_continuous_features)\n",
    "    \n",
    "    # 创建一个全为 -2 的行\n",
    "    nan_row1 = np.full((1, house_continuous_matrix.shape[1]), -2)\n",
    "    nan_row2 = np.full((1, host_continuous_matrix.shape[1]), -2)\n",
    "    # 将 nan_row 插入到 host_continuous_matrix 的最后一行\n",
    "    house_continuous_matrix = np.vstack((house_continuous_matrix, nan_row1))\n",
    "    host_continuous_matrix = np.vstack((host_continuous_matrix, nan_row2))\n",
    "\n",
    "    #提取对应的house_id和host_id\n",
    "    comment_house=np.array(data['listing_id'])\n",
    "    comment_host = np.array(data['host_id'])\n",
    "    #最后一行为-1，comment_id不存在时，house_id,host_id为-1\n",
    "    comment_house = np.append(comment_house, -1)\n",
    "    comment_host = np.append(comment_host, -1)\n",
    "\n",
    "    return index, house_continuous_matrix, host_continuous_matrix, comment_house,comment_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37749b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换 history 列为长度为15的数组\n",
    "def process_history(history,max_history_len):\n",
    "    if len(history) >= max_history_len:\n",
    "        processed_history = history[-max_history_len:]\n",
    "    else:\n",
    "        processed_history = [-1] * (max_history_len - len(history)) + history\n",
    "    return processed_history\n",
    "\n",
    "\n",
    "# 将填充-1的位置标记为True\n",
    "def create_mask(history):\n",
    "    mask = [True if item == -1 else False for item in history]\n",
    "    return mask\n",
    "\n",
    "#获取所有用户历史记录对应的house、host索引\n",
    "def user_history(path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len):\n",
    "    data = pd.read_csv(path)\n",
    "    # 将history列的数据转换为列表格式\n",
    "    data['history'] = data['history'].apply(lambda x: [int(item) for item in x.strip('[]').split()])\n",
    "    # 获取唯一的 reviewer_id\n",
    "    unique_reviewer_ids = data['reviewer_id'].unique()\n",
    "\n",
    "    # 创建 reviewer_id 到编号的映射字典\n",
    "    user_index = {reviewer_id: idx for idx, reviewer_id in enumerate(unique_reviewer_ids)}\n",
    "    # 应用处理函数到每个 history\n",
    "    data['processed_history'] = data['history'].apply(lambda x: process_history(x, max_history_len))  # shape（user_num,history_num）,comment_id,填充为-1\n",
    "    # 生成整个 history 的矩阵\n",
    "    history_matrix = np.array(data['processed_history'].tolist())\n",
    "\n",
    "    # 将 history_matrix 中的 comment_id 转换为 host_id，不存在的设为 -1\n",
    "    history_matrix_host = []\n",
    "    history_matrix_house = []\n",
    "    history_matrix_host_pic = []\n",
    "    history_matrix_mask = []\n",
    "    history_matrix_comment = []\n",
    "    for user_history in history_matrix:\n",
    "        user_history_host = []\n",
    "        user_history_host_pic = []\n",
    "        user_history_house = []\n",
    "        user_history_mask = []\n",
    "        user_history_comment = []\n",
    "\n",
    "        for comment_id in user_history:\n",
    "            user_history_comment.append(comment_index.get(comment_id, -1))  # comment编号，填充的comment编号为-1\n",
    "            user_history_host_id = comment_host[comment_index.get(comment_id, -1)]  # host_id\n",
    "            user_history_house_id = comment_house[comment_index.get(comment_id, -1)]  # house_id\n",
    "            user_history_host.append(host_index.get(user_history_host_id, -1))  # host表编号\n",
    "            user_history_host_pic.append(host_pic_index.get(user_history_host_id, -1))  # host_pic表编号\n",
    "            user_history_house.append(house_index.get(user_history_house_id, -1))  # house表编号\n",
    "\n",
    "        history_matrix_comment.append(user_history_comment)  # shape（user_num,history_num）,comment编号\n",
    "        history_matrix_mask.append(create_mask(user_history))# 在填充的位置标记true\n",
    "        history_matrix_host.append(user_history_host)  # host表编号,shape（user_num,history_num）\n",
    "        history_matrix_host_pic.append(user_history_host_pic)  # host_pic表编号\n",
    "        history_matrix_house.append(user_history_house)  # house表编号  shape（user_num,history_num）\n",
    "\n",
    "    return user_index, history_matrix_comment, history_matrix_host, history_matrix_host_pic, history_matrix_house, history_matrix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ab7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf96144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得测试集正负样本对应的house、host索引\n",
    "def get_train_input_index(path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor):\n",
    "    neg_data=pd.read_csv(path)   #负样本特征表，train_neg_comment_path\n",
    "    df_comment=pd.read_csv(comment_path)  #评论表\n",
    "    #读取负样本动态特征，房子房东id，以及对应的正样本的评论id\n",
    "    pos_comment_id=[]\n",
    "    neg_house_comment_features=[]\n",
    "    neg_host_comment_features=[]\n",
    "    neg_house_id=[]\n",
    "    neg_host_id=[]\n",
    "    grouped_neg_data = neg_data.groupby('pos_comment_id')\n",
    "    for name, group in grouped_neg_data:\n",
    "        pos_comment_id.append(name)  #正样本comment_id\n",
    "        neg_house_comment_matrix=group[house_comment_continuous_column].values[:]\n",
    "        neg_host_comment_matrix=group[host_comment_continuous_column].values[:]\n",
    "        \n",
    "        #先填缺失值，再标准化\n",
    "        neg_house_comment_matrix = np.nan_to_num(neg_house_comment_matrix, nan=-2)\n",
    "        neg_host_comment_matrix = np.nan_to_num(neg_host_comment_matrix, nan=-2)\n",
    "        \n",
    "        #标准化\n",
    "        neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        \n",
    "        neg_house_comment_features.append(neg_house_comment_matrix)  #负样本房子评论特征(6,特征数)\n",
    "        neg_host_comment_features.append(neg_host_comment_matrix)#负样本房东评论特征\n",
    "        neg_house_id.append(group['listing_id'].values[:])  #负样本房子id\n",
    "        neg_host_id.append(group['host_id'].values[:])\n",
    "        \n",
    "\n",
    "    \n",
    "    #负样本特征转换成tensor\n",
    "    neg_house_comment_features=torch.tensor(neg_house_comment_features).clone().detach()\n",
    "    neg_host_comment_features=torch.tensor(neg_host_comment_features).clone().detach()\n",
    "    \n",
    "    #根据id，获得负样本房子，房东的index\n",
    "    neg_house_id=np.array(neg_house_id)\n",
    "    neg_host_id=np.array(neg_host_id)\n",
    "    # 使用NumPy的vectorized操作获取所有id对应的索引\n",
    "    input_neg_house_index= np.vectorize(house_index.get)(neg_house_id)\n",
    "    input_neg_host_pic_index= np.vectorize(host_pic_index.get,otypes=[int])(neg_host_id,-1)\n",
    "    input_neg_host_index = np.vectorize(host_index.get, otypes=[int])(neg_host_id, -1)\n",
    "    \n",
    "    #根据正样本的评论id，获得房子，房东。comment。reviewer的index\n",
    "    #获取正样本的评论id对应的index\n",
    "    input_pos_comment_index=np.vectorize(comment_index.get)(pos_comment_id)\n",
    "    #获得房子，房东。reviewer的id\n",
    "    input_pos_house_id=comment_house[input_pos_comment_index]\n",
    "    input_pos_host_id=comment_host[input_pos_comment_index]\n",
    "    input_reviewer_id=df_comment['reviewer_id'][input_pos_comment_index].to_numpy()\n",
    "    #获得房子，房东。reviewer的index\n",
    "    input_pos_house_index=np.vectorize(house_index.get)(input_pos_house_id)\n",
    "    input_pos_host_index=np.vectorize(host_index.get)(input_pos_host_id)\n",
    "    input_pos_host_pic_index=np.vectorize(host_pic_index.get,otypes=[int])(input_pos_host_id,-1)\n",
    "    input_reviewer_index=np.vectorize(user_index.get)(input_reviewer_id)\n",
    "\n",
    "    #获得正样本的动态特征\n",
    "    pos_house_comment_features =house_comment_tensor[input_pos_comment_index]\n",
    "    pos_host_comment_features =host_comment_tensor[input_pos_comment_index]\n",
    "    \n",
    "    #合并正负样本\n",
    "    #动态特征\n",
    "    house_comment_features = torch.cat((pos_house_comment_features.unsqueeze(1), neg_house_comment_features), dim=1)\n",
    "    host_comment_features = torch.cat((pos_host_comment_features.unsqueeze(1), neg_host_comment_features), dim=1)\n",
    "    #房子，房东的index\n",
    "    input_house_index=np.concatenate((input_pos_house_index[:, np.newaxis], input_neg_house_index), axis=1)\n",
    "    input_host_index=np.concatenate((input_pos_host_index[:, np.newaxis], input_neg_host_index), axis=1)\n",
    "    input_host_pic_index=np.concatenate((input_pos_host_pic_index[:, np.newaxis], input_neg_host_pic_index), axis=1)\n",
    "    \n",
    "    label = np.zeros((len(input_house_index), 1 + npratio))\n",
    "    label[:, 0] = 1\n",
    "    label= torch.tensor(label)\n",
    "    \n",
    "    return pos_comment_id,input_house_index,input_host_index,input_host_pic_index,input_reviewer_index,label,house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851dbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据house索引，获得house的特征，返回为tensor\n",
    "def get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,input_house_index):\n",
    "    # 样本编号转换成tensor\n",
    "    input_house_index = torch.tensor(input_house_index).clone().detach()\n",
    "    # house\n",
    "    # continuous_features\n",
    "    house_continuous_features = house_continuous_tensor[input_house_index]\n",
    "    # categorical_features1\n",
    "    house_categorical_features1 = house_categorical_tensor1[input_house_index]\n",
    "    # categorical_features2\n",
    "    house_categorical_feature2 = house_categorical_tensor2[input_house_index]\n",
    "    # multi_features\n",
    "    house_multi_features = house_multi_tensor[input_house_index]\n",
    "\n",
    "    return house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c309aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,input_host_index, input_host_pic_index):\n",
    "    # host\n",
    "    # continuous_features\n",
    "    host_continuous_features = host_continuous_tensor[input_host_index]\n",
    "    # categorical_features1\n",
    "    host_categorical_features1 = host_categorical_tensor1[input_host_index]\n",
    "    # categorical_features2\n",
    "    host_categorical_features2 = host_categorical_tensor2[input_host_index]\n",
    "    # host_pic_features\n",
    "    host_pic_features = host_pic_continuous_tensor[input_host_pic_index]\n",
    "            \n",
    "    return host_continuous_features,host_categorical_features1,host_categorical_features2,host_pic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccf91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据样本中的user编号获得对应house、host编号\n",
    "def get_user_input_index(history_matrix_comment,history_matrix_host,history_matrix_host_pic,history_matrix_house,history_matrix_mask,input_reviewer_index):\n",
    "    input_history_comment = history_matrix_comment[input_reviewer_index]  # shape(样本数, max_history)\n",
    "    input_history_host = history_matrix_host[input_reviewer_index]\n",
    "    input_history_host_pic = history_matrix_host_pic[input_reviewer_index]\n",
    "    input_history_house = history_matrix_house[input_reviewer_index]\n",
    "    input_history_mask = history_matrix_mask[input_reviewer_index]\n",
    "\n",
    "    return input_history_comment,input_history_host,input_history_host_pic,input_history_house,input_history_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc53242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取user历史house、host的动态特征（只有正样本）\n",
    "def get_history_comment_input(history_comment_index,house_comment_tensor,host_comment_tensor):\n",
    "    # 样本编号转换成tensor\n",
    "    history_comment_index = torch.tensor(history_comment_index).clone().detach()\n",
    "    house_comment_features=house_comment_tensor[history_comment_index]\n",
    "    host_comment_features=host_comment_tensor[history_comment_index]\n",
    "    return house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c0729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_scaler(path,column):\n",
    "    data=pd.read_csv(path)\n",
    "    continuous_features=data[column]\n",
    "    # 补缺失值-2\n",
    "    continuous_features= continuous_features.fillna(-2)\n",
    "    continuous_matrix=np.array(continuous_features)\n",
    "    # 创建一个StandardScaler对象\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(continuous_matrix)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e6cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#dnn层,hidden_units=[[输入1，输出1],[输入2，输出2]]\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden_units, activation, l2_reg, dropout_rate, use_bn):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # 创建一个空的列表用于存储神经网络的层\n",
    "        layers = []\n",
    "\n",
    "        # 遍历每个隐藏层的设置\n",
    "        for hidden_size in hidden_units:\n",
    "            # 添加一个线性层（全连接层），输入维度为 hidden_size[0]，输出维度为 hidden_size[1]\n",
    "            layers.append(nn.Linear(hidden_size[0], hidden_size[1]))\n",
    "\n",
    "            # 如果 use_bn 为 True，则添加批归一化层\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size[1]))\n",
    "\n",
    "            # 添加激活函数层，激活函数为传入的 activation 函数\n",
    "            layers.append(activation)\n",
    "\n",
    "            # 添加 dropout 层，dropout_rate 为 dropout 的比例\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # 将所有层组合成一个顺序的神经网络\n",
    "        self.dnn_network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在前向传播过程中，输入数据 x 经过 dnn_network 这个神经网络模型\n",
    "        return self.dnn_network(x)\n",
    "\n",
    "#用户建模\n",
    "#用户建模\n",
    "class DinAttention(nn.Module):\n",
    "    def __init__(self,attetion_dnn_hidden_units):\n",
    "        super(DinAttention, self).__init__()\n",
    "        self.dnn1= DNN(attetion_dnn_hidden_units, nn.Sigmoid(), 0.0, 0.0, False)\n",
    "        self.dnn2= nn.Linear(feature_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, history_matrix, house,mask=None):\n",
    "        query= torch.unsqueeze(house, dim=-2)#(batch,npratio+1,1,400)\n",
    "        keys=torch.unsqueeze(history_matrix, dim=1)  #(batch,1,15,400)\n",
    "        query_len=query.size()[1]  #npratio+1\n",
    "#         keys = keys.repeat(1,query_len,1,1)#(batch,npratio+1,15,400)\n",
    "        keys = keys.expand(-1,query_len ,-1, -1)\n",
    "        keys_len = keys.size()[2]   #15\n",
    "#         querys = query.repeat(1,1, keys_len, 1)#(batch,npratio+1,15,400)\n",
    "        querys = query.expand(-1,-1, keys_len, -1)\n",
    "        atten_input = torch.cat([querys, keys, querys - keys, querys * keys], dim=-1)  # (batch,npratio+1,T, 4 * embed_size)\n",
    "        \n",
    "        # 经过三层全连接层\n",
    "        output1 = self.dnn1(atten_input)\n",
    "        output2 = self.dnn2(output1)  # (batch,npratio+1,T, 1)\n",
    "#         outputs = output2.transpose(2, 3)  # (batch,npratio+1, 1,T)\n",
    "#         user_att = outputs.view(-1, keys_len)  # 展平为一维向量（batch，15）\n",
    "        user_att = output2.squeeze(-1) #每个历史记录的权重(batch,npratio+1,T)\n",
    "        \n",
    "        if mask is not None:\n",
    "            #mask(batch,T),扩展成(batch,npratio+1,T)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.repeat(1,query_len,1)\n",
    "            user_att = user_att.masked_fill(mask, float('-inf'))  # 对填充向量进行mask操作(batch,npratio+1,T)\n",
    "        user_att = user_att / (keys.size()[-1] ** 0.5)\n",
    "        user_att = torch.softmax(user_att, dim=-1)  # (batch,npratio+1,T)\n",
    "        weighted_sum = torch.matmul(user_att.unsqueeze(-2), keys).squeeze(-2)  # (batch,npratio+1,400)\n",
    "        return weighted_sum\n",
    "\n",
    " # 全连接层\n",
    "def dense_layer(in_features, out_features):\n",
    "    # in_features=hidden_size,out_features=1\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features,bias=True),\n",
    "        nn.ReLU())\n",
    "\n",
    "\n",
    "def categorical1_embedding(num_categorical1_features): #输入categorical1的特征数\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical1_embeddings = []\n",
    "    for i in range(num_categorical1_features):\n",
    "        embedding_layer1 = nn.Embedding(3, feature_dim)\n",
    "        categorical1_embeddings.append(embedding_layer1)\n",
    "    return categorical1_embeddings\n",
    "\n",
    "def categorical2_embedding(num_categorical2_features,num_categories_list2): #输入categorical2的特征数，各个特征的特征值数量的列表\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical2_embeddings = []\n",
    "    # 循环遍历每个特征，为每个特征创建嵌入层并添加到列表中\n",
    "    for i in range(num_categorical2_features):\n",
    "        categorie2_embedding_dim = int(math.log2(num_categories_list2[i]+1))  # 嵌入维度为类别数的底数为2的对数\n",
    "        embedding_layer2 = nn.Embedding(num_categories_list2[i]+1, feature_dim)\n",
    "        categorical2_embeddings.append(embedding_layer2)\n",
    "    return categorical2_embeddings\n",
    "\n",
    "def continuous_embedding(num_continuous_features):\n",
    "    continuous_embedding_layers = []\n",
    "    for i in range(num_continuous_features):\n",
    "        embedding_layer = dense_layer(1, feature_dim)\n",
    "        continuous_embedding_layers.append(embedding_layer)\n",
    "    return continuous_embedding_layers\n",
    "\n",
    "\n",
    "    \n",
    "class Dice(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dice, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.zeros((1,)))\n",
    "        self.epsilon = 1e-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        norm_x = (x - x.mean(dim=0)) / torch.sqrt(x.var(dim=0) + self.epsilon)\n",
    "        p = torch.sigmoid(norm_x)\n",
    "        x = self.alpha * x.mul(1-p) + x.mul(p)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HouseEmbedding(nn.Module):\n",
    "    def __init__(self,num_house_categorical1_features,num_house_categorical2_features,num_house_categories_list2,\n",
    "                 num_house_multi_features,multi_embedding_dim,num_host_categorical1_features,num_host_categorical2_features,\n",
    "                 num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                 num_host_pic_continuous_features,num_house_comment_features,feature_dim):\n",
    "        super(HouseEmbedding, self).__init__()\n",
    "        self.multi_embedding_dim=multi_embedding_dim\n",
    "        self.num_house_multi_features=num_house_multi_features\n",
    "        self.sum_num_features=num_house_categorical1_features+num_house_categorical2_features+num_house_continuous_features+num_house_multi_features+num_house_comment_features+num_host_categorical1_features+num_host_categorical2_features+num_host_continuous_features+num_host_pic_continuous_features\n",
    "        # house_categorical1\n",
    "        self.house_categorical1_embeddings = categorical1_embedding(num_house_categorical1_features)\n",
    "        #house_categorical2\n",
    "        self.house_categorical2_embeddings = categorical2_embedding(num_house_categorical2_features,num_house_categories_list2)\n",
    "        #house_continuous\n",
    "        self.house_continuous_embedding_layer = continuous_embedding(num_house_continuous_features)\n",
    "        #house_multi\n",
    "        self.multi_embedding_layer = categorical1_embedding(num_house_multi_features)\n",
    "        #house_comment\n",
    "        self.house_comment_embedding_layer = continuous_embedding(num_house_comment_features)\n",
    "\n",
    "        # host_categorical1\n",
    "        self.host_categorical1_embeddings = categorical1_embedding(num_host_categorical1_features)\n",
    "        # host_categorical2\n",
    "        self.host_categorical2_embeddings = categorical2_embedding(num_host_categorical2_features,num_host_categories_list2)\n",
    "        #host_continuous_features\n",
    "        self.host_continuous_embedding_layer = continuous_embedding(num_host_continuous_features)\n",
    "        #host_pic_features\n",
    "        self.host_pic_embedding_layer = continuous_embedding(num_host_pic_continuous_features)\n",
    "        \n",
    "        # MLP256-11\n",
    "        self.house_dense_layer = dense_layer(self.sum_num_features*feature_dim,feature_dim)\n",
    "\n",
    "    def forward(self, house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features):\n",
    "        # house\n",
    "        # house_continuous_features(样本数, 2, 14)\n",
    "        # house_multi_features(样本数, 2, 42->18),(样本数, 15, 42->18)\n",
    "        # 对多值特征进行嵌入\n",
    "        house_multi_embeddings = torch.cat(\n",
    "            [embedding_layer(house_multi_features[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.multi_embedding_layer)], dim=-1)\n",
    "        # house_comment_features（样本数, 2, 18）\n",
    "        # house_categories_embeddings1,shape(样本数，2,2(2*1))\n",
    "        house_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(house_categorical_features1[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical1_embeddings)], dim=-1)\n",
    "        # house_categories_embeddings2，shape(样本数，2,(52+4)->embedding维度之和=7(5+2))\n",
    "        house_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(house_categorical_feature2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical2_embeddings)], dim=-1)\n",
    "        #连续值embedding(样本数，2,特征数,200)\n",
    "        house_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(house_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_continuous_embedding_layer)], dim=-1)\n",
    "        # house_comment_features（样本数, 2, 19,200）\n",
    "        house_comment_embeddings = torch.cat(\n",
    "            [embedding_layer(house_comment_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_comment_embedding_layer)], dim=-1)\n",
    "        \n",
    "        # concate（样本数, 2,61）\n",
    "        house_con = torch.cat(\n",
    "            [house_continuous_embeddings, house_multi_embeddings, house_comment_embeddings, house_categories_embeddings1,\n",
    "             house_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        # host_categories_embeddings1，shape(样本数，2,10(2*5))\n",
    "        host_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features1[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical1_embeddings)], dim=-1)\n",
    "        #host_categories_embeddings2，shape(样本数，2,5->2(2*1))\n",
    "        host_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical2_embeddings)], dim=-1)\n",
    "        #host_continuous_features\n",
    "        host_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(host_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_continuous_embedding_layer)], dim=-1)\n",
    "        #host_pic_features\n",
    "        host_pic_embeddings = torch.cat(\n",
    "            [embedding_layer(host_pic_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_pic_embedding_layer)], dim=-1)        \n",
    "    \n",
    "        # concate(样本数，2,195)-host_comment_features(11)=184\n",
    "        host_con = torch.cat(\n",
    "            [host_continuous_embeddings,host_pic_embeddings,host_categories_embeddings1,host_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        all_con=torch.cat(\n",
    "            [house_con,host_con], dim=-1)\n",
    "        \n",
    "        house_vec = self.house_dense_layer(all_con.float())  # shape(样本数, 2,200)\n",
    "        return house_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a77e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingModel(nn.Module):\n",
    "    def __init__(self, num_house_categorical1_features,num_host_categorical1_features,num_house_categorical2_features,num_host_categorical2_features,num_house_multi_features,multi_embedding_dim,\n",
    "                 num_house_categories_list2,num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                 num_host_pic_continuous_features,num_house_comment_features,feature_dim,num_heads,max_history_len,\n",
    "                 num_experts, num_tasks, expert_hidden_units, gate_hidden_units,user_dnn_hidden_units,house_dnn_hidden_units,attetion_dnn_hidden_units):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        #embedding\n",
    "        self.house_embedding=HouseEmbedding(num_house_categorical1_features,num_house_categorical2_features,num_house_categories_list2,\n",
    "                                            num_house_multi_features,multi_embedding_dim,num_host_categorical1_features,\n",
    "                                            num_host_categorical2_features,num_host_categories_list2,num_house_continuous_features,\n",
    "                                            num_host_continuous_features,num_host_pic_continuous_features,num_house_comment_features,\n",
    "                                            feature_dim)\n",
    "        #attetion\n",
    "        self.user_house_encoder = DinAttention(attetion_dnn_hidden_units)\n",
    "        self.dnn1 = nn.Linear(feature_dim*2, feature_dim)\n",
    "        self.dnn2 = nn.Linear(feature_dim, feature_dim//2)\n",
    "        self.dnn3 = nn.Linear(feature_dim//2, 1)\n",
    "        self.dice=Dice()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,\n",
    "                 host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features,\n",
    "                 user_house_continuous_features,user_house_categorical_features1,user_house_categorical_feature2,user_house_multi_features,user_house_comment_features,\n",
    "                 user_host_continuous_features,user_host_categorical_features1,user_host_categorical_features2,user_host_comment_features,user_host_pic_features\n",
    "                ,mask):\n",
    "        #house\n",
    "        # shape(样本数,npratio+1,200)\n",
    "        house_vec=self.house_embedding(house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features)\n",
    "        # shape(样本数, 15,200)\n",
    "        user_house_emb=self.house_embedding(user_house_continuous_features,user_house_categorical_features1,user_house_categorical_feature2,user_house_multi_features,user_house_comment_features,user_host_continuous_features,user_host_categorical_features1,user_host_categorical_features2,user_host_comment_features,user_host_pic_features)\n",
    "\n",
    "    \n",
    "        #用户建模\n",
    "#         pos_house_vec=house_vec[:,0]  #正样本+负样本item(batch,npratio+1,dim)\n",
    "#         pos_house_vec_modified = pos_house_vec.unsqueeze(1) #(batch,1，dim)\n",
    "#         neg_house_vec=house_vec[:,1:] #负样本item（batch,npratio,dim）\n",
    "        user_house_vec=self.user_house_encoder(user_house_emb,house_vec,mask)  #shape(batch,npratio+1,200)\n",
    "#         neg_user_house_vec=self.user_house_encoder(user_house_emb,neg_house_vec,mask)  #shape(样本数,npratio,400)\n",
    "#         user_house_vec = torch.stack((pos_user_house_vec, neg_user_house_vec), dim=1)   #shape(样本数,npratio+1, 400)\n",
    "        \n",
    "        \n",
    "\n",
    "        #匹配分数\n",
    "        res = torch.cat([user_house_vec,house_vec],-1)  ##shape(样本数,npratio+1, 200*2)\n",
    "        res=res.reshape(-1,feature_dim*2)  #shape(样本数*(npratio+1), 200*2)\n",
    "\n",
    "        output1 = self.dnn1(res)\n",
    "        output1 = self.dice(output1)\n",
    "        output2 = self.dnn2(output1)\n",
    "        output2 = self.dice(output2)\n",
    "        output3 = self.dnn3(output2)  # (样本数*(npratio+1), 1)\n",
    "        output3=torch.sigmoid(output3)\n",
    "        outputs = output3.reshape(-1,npratio+1,1)  # (样本数,npratio+1, 1)\n",
    "        score=outputs.squeeze(-1)# (样本数,npratio+1)\n",
    "#         score=outputs[:,:,0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82a6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e08a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, positive_scores, negative_scores):\n",
    "        # 计算正样本和负样本之间的分数差异\n",
    "        loss = -torch.log(torch.sigmoid(positive_scores - negative_scores))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a01060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e9995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc01b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc38237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model,train_loader,val_loader,\n",
    "                   rec_criterion,optimizer,EPOCH,device):\n",
    "    # 定义早停策略的参数\n",
    "    best_val_loss = float('inf')  # 初始化最佳验证损失为正无穷\n",
    "    patience = 3  # 容忍多少个epoch没有验证性能提升\n",
    "    early_stopping_counter = 0  # 初始化计数器\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        total_classfier_loss = 0.0\n",
    "        total_rec_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        total_regu_loss = 0.0\n",
    "        total_regu_loss1 = 0.0\n",
    "        total_regu_loss2 = 0.0\n",
    "        total_regu_loss3 = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = [data.to(device) for data in batch]\n",
    "            house_continuous_features, house_categorical_features1, house_categorical_feature2, house_multi_features, house_comment_features, \\\n",
    "            host_continuous_features, host_categorical_features1, host_categorical_features2, host_comment_features, host_pic_features, \\\n",
    "            user_house_continuous_features, user_house_categorical_features1, user_house_categorical_feature2, user_house_multi_features, user_house_comment_features, \\\n",
    "            user_host_continuous_features, user_host_categorical_features1, user_host_categorical_features2, user_host_comment_features, user_host_pic_features, \\\n",
    "            label, mask = batch\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            sum_scores = model(house_continuous_features,\n",
    "                                                                   house_categorical_features1,\n",
    "                                                                   house_categorical_feature2, house_multi_features,\n",
    "                                                                   house_comment_features,\n",
    "                                                                   host_continuous_features, host_categorical_features1,\n",
    "                                                                   host_categorical_features2, host_comment_features,\n",
    "                                                                   host_pic_features,\n",
    "                                                                   user_house_continuous_features,\n",
    "                                                                   user_house_categorical_features1,\n",
    "                                                                   user_house_categorical_feature2,\n",
    "                                                                   user_house_multi_features,\n",
    "                                                                   user_house_comment_features,\n",
    "                                                                   user_host_continuous_features,\n",
    "                                                                   user_host_categorical_features1,\n",
    "                                                                   user_host_categorical_features2,\n",
    "                                                                   user_host_comment_features, user_host_pic_features\n",
    "                                                                   , mask)\n",
    "            \n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            positive_scores = sum_scores[:, 0].unsqueeze(-1)\n",
    "            negative_scores = sum_scores[:, 1:]\n",
    "            rec_loss =  rec_criterion(sum_scores, label.float())\n",
    "            loss = rec_loss \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #计算平均loss\n",
    "\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "\n",
    "        average_loss=total_loss/len(train_loader)\n",
    "        \n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"Epoch {epoch + 1}loss:{average_loss}\")\n",
    "            \n",
    "            # 验证集评估\n",
    "            model.eval()  # 将模型切换为评估模式\n",
    "            with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "                total_val_loss = 0.0\n",
    "                total_val_auc = 0.0\n",
    "                total_regu_loss_val=0.0\n",
    "                total_rec_loss_val = 0.0\n",
    "                for batch_val in val_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "                    batch_val = [data.to(device) for data in batch_val]\n",
    "                    house_continuous_features_val, house_categorical_features1_val, house_categorical_feature2_val, house_multi_features_val, house_comment_features_val, \\\n",
    "                    host_continuous_features_val, host_categorical_features1_val, host_categorical_features2_val, host_comment_features_val, host_pic_features_val, \\\n",
    "                    user_house_continuous_features_val, user_house_categorical_features1_val, user_house_categorical_feature2_val, user_house_multi_features_val, user_house_comment_features_val, \\\n",
    "                    user_host_continuous_features_val, user_host_categorical_features1_val, user_host_categorical_features2_val, user_host_comment_features_val, user_host_pic_features_val, \\\n",
    "                    label_val, mask_val = batch_val\n",
    "                    #在验证集上进行前向传播\n",
    "                    sum_scores_val = model(house_continuous_features_val, house_categorical_features1_val,\n",
    "                                                    house_categorical_feature2_val, house_multi_features_val,\n",
    "                                                    house_comment_features_val, host_continuous_features_val,\n",
    "                                                    host_categorical_features1_val, host_categorical_features2_val,\n",
    "                                                    host_comment_features_val, host_pic_features_val,\n",
    "                                                    user_house_continuous_features_val,\n",
    "                                                    user_house_categorical_features1_val,\n",
    "                                                    user_house_categorical_feature2_val, user_house_multi_features_val,\n",
    "                                                    user_house_comment_features_val, user_host_continuous_features_val,\n",
    "                                                    user_host_categorical_features1_val, user_host_categorical_features2_val,\n",
    "                                                    user_host_comment_features_val, user_host_pic_features_val,\n",
    "                                                    mask_val)\n",
    "\n",
    "                    \n",
    "                    #推荐损失\n",
    "                    positive_scores_val = sum_scores_val[:, 0].unsqueeze(-1)\n",
    "                    negative_scores_val = sum_scores_val[:, 1:]\n",
    "                    rec_loss_val =  rec_criterion(sum_scores_val, label_val.float())\n",
    "                    #总损失\n",
    "                    val_loss = rec_loss_val \n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    # 计算验证集上的AUC\n",
    "                    predictions_val = (positive_scores_val > negative_scores_val).float()\n",
    "                    equal_indices_val = positive_scores_val == negative_scores_val\n",
    "                    predictions_val[equal_indices_val] = 0.5\n",
    "                    correct_val = predictions_val.sum().item()\n",
    "                    total_val_auc += correct_val / (len(predictions_val)*npratio)\n",
    "                    \n",
    "                average_val_loss = total_val_loss / len(val_loader)\n",
    "                average_auc_val=total_val_auc/ len(val_loader)\n",
    "                average_regu_loss_val=total_regu_loss_val/ len(val_loader)\n",
    "                average_rec_loss_val=total_rec_loss_val/ len(val_loader)\n",
    "                print(f\"Validation Loss: {average_val_loss},AUC: {average_auc_val}\")\n",
    "                if average_val_loss<best_val_loss:\n",
    "                    best_val_loss = average_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"早停策略触发，停止训练在第 {epoch} 个epoch.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e45d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training(model,train_loader,val_loader,classfier,house_embedding,host_embedding,classfier_optimizer,classfier_criterion,att_criterion,rec_criterion,optimizer,500,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c097e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "496bb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_centered_distance(X):\n",
    "    # 计算每个样本的平方模长\n",
    "    r = torch.sum(X ** 2, dim=1, keepdim=True)\n",
    "\n",
    "    # 计算欧氏距离矩阵\n",
    "    D = torch.sqrt(torch.clamp(r - 2 * torch.mm(X, X.t()) + r.t(), min=0.0) + 1e-8)\n",
    "\n",
    "    # 计算中心化距离矩阵\n",
    "    D -= torch.mean(D, dim=0, keepdim=True)\n",
    "    D -= torch.mean(D, dim=1, keepdim=True)\n",
    "    D += torch.mean(D)\n",
    "    return D\n",
    "def create_distance_covariance(D1, D2):\n",
    "    # 计算 D1 和 D2 之间的距离协方差\n",
    "    n_samples = float(D1.size(0))\n",
    "    dcov = torch.sqrt(torch.clamp(torch.sum(D1 * D2) / (n_samples * n_samples), min=0.0) + 1e-8)\n",
    "    return dcov\n",
    "def create_distance_correlation( X1, X2):\n",
    "    X1=X1.reshape(-1,feature_dim)\n",
    "    X2=X2.reshape(-1,feature_dim)\n",
    "    D1 = create_centered_distance(X1)\n",
    "    D2 = create_centered_distance(X2)\n",
    "\n",
    "    dcov_12 = create_distance_covariance(D1, D2)\n",
    "    dcov_11 = create_distance_covariance(D1, D1)\n",
    "    dcov_22 = create_distance_covariance(D2, D2)\n",
    "\n",
    "    dcor = dcov_12 / (torch.sqrt(torch.clamp(dcov_11 * dcov_22, min=0.0)) + 1e-10)\n",
    "    return dcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        total_test_loss = 0.0\n",
    "        total_rec_loss = 0.0\n",
    "        total_regu_loss_test=0.0\n",
    "        total_test_auc=0.0\n",
    "        total_distance_correlation=0.0\n",
    "        results = []  # 用于保存结果的列表\n",
    "        for batch_test in test_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "            batch_test = [data.to(device) for data in batch_test]\n",
    "            pos_comment_id,house,host,reviewer,house_continuous_features_test, house_categorical_features1_test, house_categorical_feature2_test, house_multi_features_test, house_comment_features_test, \\\n",
    "            host_continuous_features_test, host_categorical_features1_test, host_categorical_features2_test, host_comment_features_test, host_pic_features_test, \\\n",
    "            user_house_continuous_features_test, user_house_categorical_features1_test, user_house_categorical_feature2_test, user_house_multi_features_test, user_house_comment_features_test, \\\n",
    "            user_host_continuous_features_test, user_host_categorical_features1_test, user_host_categorical_features2_test, user_host_comment_features_test, user_host_pic_features_test, \\\n",
    "            label_test, mask_test = batch_test\n",
    "                    \n",
    "            sum_scores_test= model(house_continuous_features_test, house_categorical_features1_test,\n",
    "                                                                                                                            house_categorical_feature2_test, house_multi_features_test, \n",
    "                                                                                                                            house_comment_features_test, host_continuous_features_test, \n",
    "                                                                                                                            host_categorical_features1_test, host_categorical_features2_test, \n",
    "                                                                                                                            host_comment_features_test, host_pic_features_test,                                                                                           \n",
    "                                                                                                                            user_house_continuous_features_test, user_house_categorical_features1_test, \n",
    "                                                                                                                            user_house_categorical_feature2_test, user_house_multi_features_test,\n",
    "                                                                                                                            user_house_comment_features_test, user_host_continuous_features_test, \n",
    "                                                                                                                            user_host_categorical_features1_test, user_host_categorical_features2_test, \n",
    "                                                                                                                            user_host_comment_features_test, user_host_pic_features_test,mask_test)\n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            positive_scores_test = sum_scores_test[:, 0].unsqueeze(-1)\n",
    "            negative_scores_test = sum_scores_test[:, 1:]\n",
    "            rec_loss_test = rec_criterion(sum_scores_test,label_test.float())\n",
    "            #总损失\n",
    "            test_loss = rec_loss_test \n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            \n",
    "            \n",
    "            # 计算验证集上的AUC\n",
    "            predictions_test = (positive_scores_test > negative_scores_test).float()\n",
    "            equal_indices = positive_scores_test == negative_scores_test\n",
    "            predictions_test[equal_indices] = 0.5\n",
    "            correct_test = predictions_test.sum().item()\n",
    "            total_test_auc += correct_test /  (len(predictions_test)*npratio)\n",
    "\n",
    "        \n",
    "            for i in range(house.shape[0]):\n",
    "                #正样本\n",
    "                results.append({\n",
    "                    'pos_comment_id':pos_comment_id[i],\n",
    "                    'house': house[i][0],\n",
    "                    'host': host[i][0],\n",
    "                    'reviewer': reviewer[i],\n",
    "                    'sum_score': sum_scores_test[i][0],  # 正样本分数\n",
    "\n",
    "                    'label': label_test[i][0]\n",
    "                    \n",
    "                })\n",
    "                for j in range(npratio):\n",
    "                    #负样本\n",
    "                    results.append({\n",
    "                        'pos_comment_id':pos_comment_id[i],\n",
    "                        'house': house[i][1+j],\n",
    "                        'host': host[i][1+j],\n",
    "                        'reviewer': reviewer[i],             \n",
    "                        'sum_score': sum_scores_test[i][1+j],  # 负样本分数\n",
    "\n",
    "                        'label': label_test[i][1+j]\n",
    "                    })\n",
    "                \n",
    "        average_test_loss = total_test_loss / len(test_loader)\n",
    "        average_auc_test=total_test_auc/ len(test_loader)\n",
    "#         average_distance_correlation=total_distance_correlation/ len(test_loader)\n",
    "        \n",
    "\n",
    "        print(f\"Test Loss: {average_test_loss},AUC: {average_auc_test}\")\n",
    "        \n",
    "#         print(f\"average_distance_correlation: {average_distance_correlation}\")\n",
    "        \n",
    "        \n",
    "        # 将结果列表转换为DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results['pos_comment_id'] = df_results['pos_comment_id'].apply(lambda x: x.item())\n",
    "        df_results['house'] = df_results['house'].apply(lambda x: x.item())\n",
    "        df_results['host'] = df_results['host'].apply(lambda x: x.item())\n",
    "        df_results['reviewer'] = df_results['reviewer'].apply(lambda x: x.item())\n",
    "        df_results['sum_score'] = df_results['sum_score'].apply(lambda x: x.item())\n",
    "\n",
    "        df_results['label'] = df_results['label'].apply(lambda x: x.item())\n",
    "        df_results['item_count'] = df_results.groupby('reviewer')['reviewer'].transform('count')\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "        return df_results,average_test_loss,average_auc_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0401b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results,average_test_loss,average_auc_test,average_att_loss_test=test_model(model, test_loader)\n",
    "# print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e458c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-n\n",
    "def top_evaluation(df_results,n):\n",
    "    df_results_shuffled = df_results.sample(frac=1).reset_index(drop=True)\n",
    "    # 初始化结果列表\n",
    "    ndcg_list = []\n",
    "    hr_list=[]\n",
    "    \n",
    "    for user_id, user_group in df_results_shuffled.groupby('pos_comment_id'):\n",
    "        user_group = user_group.sort_values(by='sum_score', ascending=False)  # 按分数降序排序\n",
    "        top_items = user_group.head(n)['label'].tolist()  # 选取前n个的标签\n",
    "        true_labels = user_group['label'].tolist()  # 用户所有候选物品的真实标签\n",
    "                \n",
    "        # 计算NDCG\n",
    "        idcg = sorted(true_labels, reverse=True)[:n]  # 理想情况下的DCG\n",
    "        dcg = sum(rel / np.log(i + 2) for i, rel in enumerate(top_items))  # 计算DCG\n",
    "        ndcg = dcg / sum(rel / np.log(i + 2) for i, rel in enumerate(idcg))  # 计算NDCG\n",
    "        ndcg_list.append(ndcg)\n",
    "        # 计算HR@N\n",
    "        hit_rate = 1 if any(label == 1 for label in top_items) else 0  # 若Top N中有真实正样本，HR@N为1，否则为0\n",
    "        hr_list.append(hit_rate)\n",
    "\n",
    "        \n",
    "    # 计算平均值\n",
    "    average_ndcg = np.mean(ndcg_list)\n",
    "    average_hr = np.mean(hr_list)\n",
    "    \n",
    "    return average_ndcg, average_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264fb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a03120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=+1\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3789946d6fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m#训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrec_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_modals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-44898355c9ab>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(model, train_loader, val_loader, rec_criterion, optimizer, EPOCH, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mrec_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrec_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import cloudpickle\n",
    "# 创建一个空的DataFrame来存储结果\n",
    "test_auc_df = pd.DataFrame(columns=['时间窗','实验数', '测试集总损失', 'AUC','NDCG@5','HR@5','NDCG@10','HR@10'])\n",
    "for train_user_history_path,val_user_history_path,test_user_history_path,train_neg_comment_path,val_neg_comment_path,test_neg_comment_path,sclar_path in zip(train_user_history_paths,val_user_history_paths,test_user_history_paths,train_neg_comment_paths,val_neg_comment_paths,test_neg_comment_paths,sclar_paths):\n",
    "    \n",
    "    for n in range(5):\n",
    "        print(f'i=+{n+1}')\n",
    "\n",
    "        # 假设文件名总是有相同的结构，可以根据固定的位置进行切片\n",
    "        start_index = train_user_history_path.find('2018') + 4\n",
    "        end_index = start_index + 5\n",
    "        substring = train_user_history_path[start_index:end_index]\n",
    "        print(substring)\n",
    "\n",
    "        # 设置 CSV 文件路径\n",
    "        csv_path = 'new_result//new_DIN-allfeature.csv'\n",
    "\n",
    "        # 检查文件是否存在，如果存在则不写入表头\n",
    "        try:\n",
    "            with open(csv_path, 'r') as f:\n",
    "                header_exists = True\n",
    "        except FileNotFoundError:\n",
    "            header_exists = False\n",
    "\n",
    "        house_index,house_continuous_matrix,house_categorical_matrix1,house_categorical_matrix2,house_categorical_encodings,house_multi_matrix,num_house_categories_list2=read_house_host(house_path,house_id_column,'house',house_continuous_column,house_categorical_column1,house_categorical_column2)\n",
    "        host_index,host_continuous_matrix,host_categorical_matrix1,host_categorical_matrix2,host_categorical_encodings,num_host_categories_list2=read_house_host(host_path,host_id_column,'host',host_continuous_column,host_categorical_column1,host_categorical_column2)\n",
    "\n",
    "        #读取host图片特征\n",
    "        host_pic_index, host_pic_continuous_matrix, host_pic_continuous_column = read_host_pic_features(host_pic_path)\n",
    "\n",
    "        #读取正样本house和host动态特征\n",
    "        comment_index,house_comment_matrix,host_comment_matrix,comment_house,comment_host=read_comment_features(comment_path,comment_id_column,house_comment_continuous_column,host_comment_continuous_column)\n",
    "        #获取所有用户历史记录对应的house、host索引\n",
    "        train_user_index, train_history_matrix_comment, train_history_matrix_host, train_history_matrix_host_pic, train_history_matrix_house, train_history_matrix_mask = user_history(train_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        val_user_index, val_history_matrix_comment, val_history_matrix_host, val_history_matrix_host_pic, val_history_matrix_house, val_history_matrix_mask = user_history(val_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        test_user_index, test_history_matrix_comment, test_history_matrix_host, test_history_matrix_host_pic, test_history_matrix_house, test_history_matrix_mask = user_history(test_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "\n",
    "\n",
    "        house_scaler=creat_scaler(sclar_path,house_continuous_column)\n",
    "        host_scaler=creat_scaler(sclar_path,host_continuous_column)\n",
    "        house_comment_scaler=creat_scaler(sclar_path,house_comment_continuous_column)\n",
    "        host_comment_scaler=creat_scaler(sclar_path,host_comment_continuous_column)\n",
    "        host_pic_scaler=creat_scaler(sclar_path,host_pic_continuous_column)\n",
    "\n",
    "        #连续特征标准化\n",
    "        house_continuous_matrix = house_scaler.transform(house_continuous_matrix)\n",
    "        host_continuous_matrix = host_scaler.transform(host_continuous_matrix)\n",
    "        house_comment_matrix = house_comment_scaler.transform(house_comment_matrix)\n",
    "        #     neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        host_comment_matrix = host_comment_scaler.transform(host_comment_matrix)\n",
    "        #     neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        host_pic_continuous_matrix = host_pic_scaler.transform(host_pic_continuous_matrix)\n",
    "\n",
    "        #特征矩阵转换成tensor\n",
    "        house_continuous_tensor=torch.tensor(house_continuous_matrix).detach()\n",
    "        house_categorical_tensor1=torch.tensor(house_categorical_matrix1).detach()\n",
    "        house_categorical_tensor2=torch.tensor(house_categorical_matrix2).detach()\n",
    "        house_multi_tensor=torch.tensor(house_multi_matrix).detach()\n",
    "        house_comment_tensor=torch.tensor(house_comment_matrix).detach()\n",
    "        #     neg_house_comment_tensor=torch.tensor(neg_house_comment_matrix).clone().detach()\n",
    "\n",
    "        host_continuous_tensor=torch.tensor(host_continuous_matrix).detach()\n",
    "        host_categorical_tensor1=torch.tensor(host_categorical_matrix1).detach()\n",
    "        host_categorical_tensor2=torch.tensor(host_categorical_matrix2).detach()\n",
    "        host_comment_tensor=torch.tensor(host_comment_matrix).detach()\n",
    "        #     neg_host_comment_tensor=torch.tensor(neg_host_comment_matrix).clone().detach()\n",
    "        host_pic_continuous_tensor=torch.tensor(host_pic_continuous_matrix).detach()\n",
    "\n",
    "        #训练集输入特征编号、label.负样本动态特征（标准化后的tensor）\n",
    "        train_pos_comment_id,train_house,train_host,train_host_pic,train_reviewer,train_label,train_house_comment_features,train_host_comment_features=get_train_input_index(train_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,train_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        val_pos_comment_id,val_house,val_host,val_host_pic,val_reviewer,val_label,val_house_comment_features,val_host_comment_features=get_train_input_index(val_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,val_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        test_pos_comment_id,test_house,test_host,test_host_pic,test_reviewer,test_label,test_house_comment_features,test_host_comment_features=get_train_input_index(test_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,test_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "\n",
    "        #获得训练集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_house)\n",
    "        #获得验证集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        val_house_continuous_features,val_house_categorical_features1,val_house_categorical_feature2,val_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_house)\n",
    "        #获得测试集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        test_house_continuous_features,test_house_categorical_features1,test_house_categorical_feature2,test_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_house)\n",
    "\n",
    "\n",
    "        #获得训练集输入的host特征\n",
    "        train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_host,train_host_pic)\n",
    "        #获得验证集输入的host特征\n",
    "        val_host_continuous_features,val_host_categorical_features1,val_host_categorical_features2,val_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_host,val_host_pic)\n",
    "        #获得测试集输入的host特征\n",
    "        test_host_continuous_features,test_host_categorical_features1,test_host_categorical_features2,test_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_host,test_host_pic)\n",
    "\n",
    "        #user历史记录矩阵转换成tensor\n",
    "        train_history_matrix_comment=torch.tensor(train_history_matrix_comment).detach()\n",
    "        train_history_matrix_mask=torch.tensor(train_history_matrix_mask).detach()\n",
    "        train_history_matrix_host=torch.tensor(train_history_matrix_host).detach()\n",
    "        train_history_matrix_host_pic=torch.tensor(train_history_matrix_host_pic).detach()\n",
    "        train_history_matrix_house=torch.tensor(train_history_matrix_house).detach()\n",
    "\n",
    "        val_history_matrix_comment=torch.tensor(val_history_matrix_comment).detach()\n",
    "        val_history_matrix_mask=torch.tensor(val_history_matrix_mask).detach()\n",
    "        val_history_matrix_host=torch.tensor(val_history_matrix_host).detach()\n",
    "        val_history_matrix_host_pic=torch.tensor(val_history_matrix_host_pic).detach()\n",
    "        val_history_matrix_house=torch.tensor(val_history_matrix_house).detach()\n",
    "\n",
    "        test_history_matrix_comment=torch.tensor(test_history_matrix_comment).detach()\n",
    "        test_history_matrix_mask=torch.tensor(test_history_matrix_mask).detach()\n",
    "        test_history_matrix_host=torch.tensor(test_history_matrix_host).detach()\n",
    "        test_history_matrix_host_pic=torch.tensor(test_history_matrix_host_pic).detach()\n",
    "        test_history_matrix_house=torch.tensor(test_history_matrix_house).detach()\n",
    "\n",
    "        #得到训练集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        train_history_comment,train_history_host,train_history_host_pic,train_history_house,train_history_mask=get_user_input_index(train_history_matrix_comment,train_history_matrix_host,train_history_matrix_host_pic,train_history_matrix_house,train_history_matrix_mask,train_reviewer)\n",
    "        #得到验证集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        val_history_comment,val_history_host,val_history_host_pic,val_history_house,val_history_mask=get_user_input_index(val_history_matrix_comment,val_history_matrix_host,val_history_matrix_host_pic,val_history_matrix_house,val_history_matrix_mask,val_reviewer)\n",
    "        #得到测试集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        test_history_comment,test_history_host,test_history_host_pic,test_history_house,test_history_mask=get_user_input_index(test_history_matrix_comment,test_history_matrix_host,test_history_matrix_host_pic,test_history_matrix_house,test_history_matrix_mask,test_reviewer)\n",
    "\n",
    "        #获得训练集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_history_house)\n",
    "        #获得验证集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_house_continuous_features,val_user_house_categorical_features1,val_user_house_categorical_feature2,val_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_history_house)\n",
    "        #获得测试集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_house_continuous_features,test_user_house_categorical_features1,test_user_house_categorical_feature2,test_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_history_house)\n",
    "\n",
    "        #获得训练集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_history_host,train_history_host_pic)\n",
    "        #获得验证集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_host_continuous_features,val_user_host_categorical_features1,val_user_host_categorical_features2,val_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_history_host,val_history_host_pic)\n",
    "        #获得测试集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_host_continuous_features,test_user_host_categorical_features1,test_user_host_categorical_features2,test_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_history_host,test_history_host_pic)\n",
    "\n",
    "        #获得训练集输入user历史记录的comment特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_comment_features,train_user_host_comment_features=get_history_comment_input(train_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        val_user_house_comment_features,val_user_host_comment_features=get_history_comment_input(val_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        test_user_house_comment_features,test_user_host_comment_features=get_history_comment_input(test_history_comment,house_comment_tensor,host_comment_tensor)    \n",
    "\n",
    "\n",
    "        num_house_categorical2_features=house_categorical_matrix2.shape[1]\n",
    "        num_host_categorical2_features=host_categorical_matrix2.shape[1]\n",
    "        num_house_categorical1_features=house_categorical_matrix1.shape[1]\n",
    "        num_host_categorical1_features=host_categorical_matrix1.shape[1]\n",
    "        num_house_multi_features=house_multi_matrix.shape[1]\n",
    "        num_house_continuous_features=house_continuous_matrix.shape[1]\n",
    "        num_host_continuous_features=host_continuous_matrix.shape[1]   \n",
    "        num_house_comment_features=house_comment_matrix.shape[1]\n",
    "        num_host_comment_features=host_comment_matrix.shape[1]\n",
    "        num_host_pic_continuous_features=host_pic_continuous_matrix.shape[1]\n",
    "        \n",
    "        train_modals= f'model///new_DIN-allfeature+{substring}+{n}.pth'\n",
    "\n",
    "\n",
    "        #训练集\n",
    "        train_dataset = TensorDataset(train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features,train_house_comment_features,\n",
    "            train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_comment_features,train_host_pic_features,\n",
    "            train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features,train_user_house_comment_features,\n",
    "            train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_comment_features,train_user_host_pic_features,\n",
    "            train_label,train_history_mask)\n",
    "        val_dataset = TensorDataset(val_house_continuous_features, val_house_categorical_features1, val_house_categorical_feature2, val_house_multi_features, val_house_comment_features,\n",
    "            val_host_continuous_features, val_host_categorical_features1, val_host_categorical_features2, val_host_comment_features, val_host_pic_features,\n",
    "            val_user_house_continuous_features, val_user_house_categorical_features1, val_user_house_categorical_feature2, val_user_house_multi_features, val_user_house_comment_features,\n",
    "            val_user_host_continuous_features, val_user_host_categorical_features1, val_user_host_categorical_features2, val_user_host_comment_features, val_user_host_pic_features,\n",
    "            val_label, val_history_mask)\n",
    "\n",
    "\n",
    "        # 创建数据加载器\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        # 确保您的计算机上有CUDA支持的GPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # 创建大模型的实例\n",
    "        model = MatchingModel(num_house_categorical1_features,num_host_categorical1_features,num_house_categorical2_features,num_host_categorical2_features,num_house_multi_features,multi_embedding_dim,\n",
    "                     num_house_categories_list2,num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                     num_host_pic_continuous_features,num_house_comment_features,feature_dim,num_heads,max_history_len,\n",
    "                     num_experts, num_tasks, expert_hidden_units, gate_hidden_units,user_dnn_hidden_units,house_dnn_hidden_units,attetion_dnn_hidden_units)\n",
    "\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        #进一步处理 列表转移到GPU\n",
    "        for i in range(len(model.house_embedding.house_categorical1_embeddings)):\n",
    "            model.house_embedding.house_categorical1_embeddings[i] = model.house_embedding.house_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_categorical2_embeddings)):\n",
    "            model.house_embedding.house_categorical2_embeddings[i] = model.house_embedding.house_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical1_embeddings)):\n",
    "            model.house_embedding.host_categorical1_embeddings[i] = model.house_embedding.host_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical2_embeddings)):\n",
    "            model.house_embedding.host_categorical2_embeddings[i] = model.house_embedding.host_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.multi_embedding_layer)):\n",
    "            model.house_embedding.multi_embedding_layer[i] = model.house_embedding.multi_embedding_layer[i].to(device)\n",
    "            \n",
    "            \n",
    "        for i in range(len(model.house_embedding.house_continuous_embedding_layer)):\n",
    "            model.house_embedding.house_continuous_embedding_layer[i] = model.house_embedding.house_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_comment_embedding_layer)):\n",
    "            model.house_embedding.house_comment_embedding_layer[i] = model.house_embedding.house_comment_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_continuous_embedding_layer)):\n",
    "            model.house_embedding.host_continuous_embedding_layer[i] = model.house_embedding.host_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_pic_embedding_layer)):\n",
    "            model.house_embedding.host_pic_embedding_layer[i] = model.house_embedding.host_pic_embedding_layer[i].to(device)\n",
    "\n",
    "\n",
    "        rec_criterion = nn.BCELoss(size_average=True, reduce=True)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        #训练\n",
    "        model_training(model,train_loader,val_loader,rec_criterion,optimizer,500,device)\n",
    "        with open(train_modals, 'wb') as f:\n",
    "            cloudpickle.dump(model, f)\n",
    "            \n",
    "        #测试\n",
    "        test_reviewer=torch.tensor(test_reviewer).detach()\n",
    "        test_host=torch.tensor(test_host).detach()\n",
    "        test_house=torch.tensor(test_house).detach()\n",
    "        test_pos_comment_id=torch.tensor(test_pos_comment_id, dtype=torch.int64).detach()\n",
    "        test_dataset = TensorDataset(test_pos_comment_id,test_house,test_host,test_reviewer,test_house_continuous_features, test_house_categorical_features1, test_house_categorical_feature2, test_house_multi_features, test_house_comment_features,\n",
    "            test_host_continuous_features, test_host_categorical_features1, test_host_categorical_features2, test_host_comment_features, test_host_pic_features,\n",
    "            test_user_house_continuous_features, test_user_house_categorical_features1, test_user_house_categorical_feature2, test_user_house_multi_features, test_user_house_comment_features,\n",
    "            test_user_host_continuous_features, test_user_host_categorical_features1, test_user_host_categorical_features2, test_user_host_comment_features, test_user_host_pic_features,\n",
    "            test_label, test_history_mask)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        df_results,average_test_loss,average_auc_test=test_model(model, test_loader)\n",
    "        #测试的每个样本结果保存到csv\n",
    "        file_name = f\"result/DIN_cat_{n}.csv\"\n",
    "        df_results.to_csv(file_name, index=False)\n",
    "\n",
    "        top_5_ndcg,top_5_hr=top_evaluation(df_results,5)\n",
    "        top_10_ndcg,top_10_hr=top_evaluation(df_results,10)   \n",
    "\n",
    "        new_row = {\n",
    "            '时间窗':substring,\n",
    "            '实验数': n, \n",
    "            '测试集总损失': average_test_loss, \n",
    "            'AUC': average_auc_test, \n",
    "            'NDCG@5': top_5_ndcg, \n",
    "            'HR@5': top_5_hr, \n",
    "            'NDCG@10': top_10_ndcg, \n",
    "            'HR@10': top_10_hr\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        # 将新的行追加到 CSV 文件中\n",
    "        pd.DataFrame([new_row]).to_csv(csv_path, mode='a', index=False, header=not header_exists)\n",
    "        header_exists = True  # 确保后续追加时不再写入表头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca036b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aea1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1618a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f534554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47047c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db239c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0adbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
