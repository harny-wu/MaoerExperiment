{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c1e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "house_path='dataset//selected_feature_num_can_house_distinct__201901_202103.csv'\n",
    "host_path='dataset//selected_feature_num_can_host_distinct_201901_202103.csv'\n",
    "host_pic_path='dataset//feature_img_can_host_distinct_201901_202103.csv'\n",
    "comment_path='new_dataset//selected_feature_can_comment_basic_201901_202103.csv'\n",
    "\n",
    "\n",
    "house_id_column='HOUSE_ID'\n",
    "host_id_column='host_id'\n",
    "host_pic_id_column='host_id'\n",
    "comment_id_column='comment_id'\n",
    "neg_comment_id_column='pos_comment_id'\n",
    "house_continuous_column=['HOUSE_PRICE_DAY','HOUSE_BATHROOM_NUM', 'HOUSE_BEDROOM_NUM', 'HOUSE_BED_NUM', 'HOUSE_DESCRIPTION_LEN', 'HOUSE_ACCOMMODATES',\n",
    "         'HOUSE_NAME_LEN', 'HOUSE_NEIGHBOUR_OVERVIEW_LEN','HOUSE_REVIEW_SCORE_CLEAN',\n",
    "         'HOUSE_REVIEW_SCORE_ACCURACY', 'HOUSE_REVIEW_SCORE_CHECKIN', 'HOUSE_REVIEW_SCORE_COMMUNICATION',\n",
    "         'HOUSE_REVIEW_SCORE_VALUE', 'HOUSE_REVIEW_SCORE_LOCATION','HOUSE_REVIEW_SCORE_RATE']\n",
    "\n",
    "host_continuous_column=['HOST_ABOUT_LEN', 'HOST_ACCEPTANCE_RATE', 'HOST_HOUSE_NUM', 'HOST_NAME_LEN', 'HOST_RESPONSE_RATE',\n",
    "         'HOST_VERIFICATION_NUM', 'info_amount', 'readability', 'user_about_t1', 'user_about_t3', 'user_about_t4',\n",
    "         'user_about_t2','HOST_HOUSE_REVIEW_SCORE_COMMUNICAT_AVG']\n",
    "\n",
    "\n",
    "\n",
    "house_comment_continuous_column=['house_tran_review_num','house_tran_reviewer_num', 'house_tran_review_1y_num',\n",
    "                                 'house_tran_review_30d_num',  'house_tran_comment_subjectivity_score_avg',\n",
    "                                 'house_tran_comment_plarity_score_avg', 'house_tran_review_theme_RoomService_avg',\n",
    "                                 'house_tran_review_theme_IndoorEnvironment_avg', 'house_tran_review_theme_NeighborFacilities_avg',\n",
    "                                 'house_tran_review_Transportation_avg', 'house_tran_review_BookingRelus_avg',\n",
    "                                 'house_tran_review_TouristScenery_avg','house_tran_review_HostServices_avg',\n",
    "                                 'house_tran_review_theme_HouseFacilities_avg', 'house_tran_review_theme_AccommodExperience_avg',\n",
    "                                 'house_tran_review_pos_num', 'house_tran_review_neg_num',\n",
    "                                 'interval_house_first_review', 'interval_house_previous_review']\n",
    "\n",
    "host_comment_continuous_column=['host_tran_review_num']\n",
    "\n",
    "house_categorical_column1=['HOUSE_IS_INSTANT_BOOKABLE']\n",
    "host_categorical_column1=['HOST_HAS_PROFILE_PIC', 'HOST_IS_IDENTITY_VERIFIED', 'HOST_IS_SUPERHOST', 'HOST_NAME_HAS_DIG',\n",
    "         'HOST_NAME_HAS_ENGLISH']\n",
    "house_categorical_column2=['HOUSE_PROPERTY_TYPE', 'HOUSE_ROOM_TYPE']\n",
    "host_categorical_column2=['HOST_RESPONSE_TIME']\n",
    "\n",
    "\n",
    "npratio=50 #负样本数\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "num_heads=8\n",
    "\n",
    "feature_dim=128\n",
    "max_history_len=15\n",
    "num_experts=3\n",
    "num_tasks=2\n",
    "multi_embedding_dim = 18  # 设置嵌入维度\n",
    "expert_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "gate_hidden_units=[[feature_dim,feature_dim//2],[feature_dim//2,num_experts]]\n",
    "classifier_dnn_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "weight_dnn_hidden_units=[[feature_dim*3,feature_dim],[feature_dim,2]]\n",
    "user_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "house_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "attetion_dnn_hidden_units=[[4*feature_dim,2*feature_dim],[2*feature_dim,feature_dim]]\n",
    "\n",
    "lr=0.0001\n",
    "batch_size = 128\n",
    "t_cont=0.02\n",
    "num_interests=4\n",
    "λ=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c0c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_history_path1 = 'new_dataset2//can_history_201801.csv'\n",
    "val_user_history_path1 = 'new_dataset2//can_history_201805.csv'\n",
    "test_user_history_path1 = 'new_dataset2//can_history_201806.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path1='new_dataset2//new_can_comment_neg_201801_04_50neg.csv'\n",
    "val_neg_comment_path1='new_dataset2//new_can_comment_neg_201805_50neg.csv'\n",
    "test_neg_comment_path1='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "\n",
    "sclar_path1='new_dataset/all_feature+label_201801_04_50neg.csv'\n",
    "\n",
    "\n",
    "train_user_history_path2 = 'new_dataset2//can_history_201802_05.csv'\n",
    "val_user_history_path2 = 'new_dataset2//can_history_201806.csv'\n",
    "test_user_history_path2 = 'new_dataset2//can_history_201807.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path2='new_dataset2//new_can_comment_neg_201802_05_50neg.csv'\n",
    "val_neg_comment_path2='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "test_neg_comment_path2='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "\n",
    "sclar_path2='new_dataset2/all_feature+label_201802_05_50neg.csv'\n",
    "\n",
    "train_user_history_path3 = 'new_dataset2//can_history_201803_06.csv'\n",
    "val_user_history_path3 = 'new_dataset2//can_history_201807.csv'\n",
    "test_user_history_path3 = 'new_dataset2//can_history_201808.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path3='new_dataset2//new_can_comment_neg_201803_06_50neg.csv'\n",
    "val_neg_comment_path3='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "test_neg_comment_path3='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "\n",
    "sclar_path3='new_dataset2/all_feature+label_201803_06_50neg.csv'\n",
    "\n",
    "train_user_history_path4 = 'new_dataset2//can_history_201804_07.csv'\n",
    "val_user_history_path4 = 'new_dataset2//can_history_201808.csv'\n",
    "test_user_history_path4 = 'new_dataset2//can_history_201809.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path4='new_dataset2//new_can_comment_neg_201804_07_50neg.csv'\n",
    "val_neg_comment_path4='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "test_neg_comment_path4='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "\n",
    "sclar_path4='new_dataset2/all_feature+label_201804_07_50neg.csv'\n",
    "\n",
    "train_user_history_path5 = 'new_dataset2//can_history_201805_08.csv'\n",
    "val_user_history_path5 = 'new_dataset2//can_history_201809.csv'\n",
    "test_user_history_path5 = 'new_dataset2//can_history_201810.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path5='new_dataset2//new_can_comment_neg_201805_08_50neg.csv'\n",
    "val_neg_comment_path5='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "test_neg_comment_path5='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "\n",
    "sclar_path5='new_dataset2/all_feature+label_201805_08_50neg.csv'\n",
    "\n",
    "train_user_history_path6 = 'new_dataset2//can_history_201806_09.csv'\n",
    "val_user_history_path6 = 'new_dataset2//can_history_201810.csv'\n",
    "test_user_history_path6 = 'new_dataset2//can_history_201811.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path6='new_dataset2//new_can_comment_neg_201806_09_50neg.csv'\n",
    "val_neg_comment_path6='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "test_neg_comment_path6='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "\n",
    "sclar_path6='new_dataset2/all_feature+label_201806_09_50neg.csv'\n",
    "\n",
    "train_user_history_path7 = 'new_dataset2//can_history_201807_10.csv'\n",
    "val_user_history_path7 = 'new_dataset2//can_history_201811.csv'\n",
    "test_user_history_path7 = 'new_dataset2//can_history_201812.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path7='new_dataset2//new_can_comment_neg_201807_10_50neg.csv'\n",
    "val_neg_comment_path7='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "test_neg_comment_path7='new_dataset2//new_can_comment_neg_201812_50neg.csv'\n",
    "\n",
    "sclar_path7='new_dataset2/all_feature+label_201807_10_50neg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_history_paths=[train_user_history_path1,train_user_history_path2,train_user_history_path3,train_user_history_path4,train_user_history_path5,train_user_history_path6,train_user_history_path7]\n",
    "val_user_history_paths=[val_user_history_path1,val_user_history_path2,val_user_history_path3,val_user_history_path4,val_user_history_path5,val_user_history_path6,val_user_history_path7]\n",
    "test_user_history_paths=[test_user_history_path1,test_user_history_path2,test_user_history_path3,test_user_history_path4,test_user_history_path5,test_user_history_path6,test_user_history_path7]\n",
    "\n",
    "train_neg_comment_paths=[train_neg_comment_path1,train_neg_comment_path2,train_neg_comment_path3,train_neg_comment_path4,train_neg_comment_path5,train_neg_comment_path6,train_neg_comment_path7]\n",
    "val_neg_comment_paths=[val_neg_comment_path1,val_neg_comment_path2,val_neg_comment_path3,val_neg_comment_path4,val_neg_comment_path5,val_neg_comment_path6,val_neg_comment_path7]\n",
    "test_neg_comment_paths=[test_neg_comment_path1,test_neg_comment_path2,test_neg_comment_path3,test_neg_comment_path4,test_neg_comment_path5,test_neg_comment_path6,test_neg_comment_path7]\n",
    "\n",
    "sclar_paths=[sclar_path1,sclar_path2,sclar_path3,sclar_path4,sclar_path5,sclar_path6,sclar_path7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f938d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取house或host的静态特征\n",
    "def read_house_host(path, id_column, object, continuous_column, categorical_column1, categorical_column2):\n",
    "    # 从CSV文件中读取房屋信息\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "\n",
    "    # 提取连续特征和类别特征,multi-hot特征\n",
    "    continuous_features = data[continuous_column]\n",
    "    categorical_features1 = data[categorical_column1]  # 类别特征列\n",
    "    categorical_features2 = data[categorical_column2]\n",
    "\n",
    "    # 使用LabelEncoder将类别特征转换为编号\n",
    "    label_encoders = {}  # 存储特征列对应的编码器对象\n",
    "    categorical_encodings = {}  # 存储特征值对应的编号\n",
    "    for col in categorical_features2.columns:\n",
    "        le = LabelEncoder()\n",
    "        categorical_features2.loc[:, col] = le.fit_transform(categorical_features2[col])\n",
    "        label_encoders[col] = le\n",
    "        categorical_encodings[col] = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    # 得到每个分类特征的类别数\n",
    "    num_categories_list2 = [len(le.classes_) for le in label_encoders.values()]\n",
    "    \n",
    "    \n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        multi_features = data.iloc[:, 10:52]\n",
    "        #补缺失值，2\n",
    "        multi_features=multi_features.fillna(2)\n",
    "        house_multi_matrix = np.array(multi_features)\n",
    "        # 创建一个与最后一行维度相同的 NaN 数组\n",
    "        nan_row = np.full((1, house_multi_matrix.shape[1]),2)\n",
    "        # 将 NaN 数组插入到 house_multi_matrix 的最后一行\n",
    "        house_multi_matrix = np.concatenate((house_multi_matrix, nan_row), axis=0)\n",
    "    else:\n",
    "        # 将连续特征中的百分比特征转换成数值类型\n",
    "        # 找到包含百分数的列\n",
    "        percent_columns = ['HOST_ACCEPTANCE_RATE','HOST_RESPONSE_RATE']  \n",
    "        # 将百分数转换为浮点数\n",
    "        for column in percent_columns:\n",
    "            continuous_features[column] = continuous_features[column].str.rstrip('%').astype(float) / 100.0\n",
    "\n",
    "                        \n",
    "    #补缺失值，连续值补-2，分类1特征补2，分类2特征不用补\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "    categorical_features1=categorical_features1.fillna(2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "    categorical_matrix1 = np.array(categorical_features1).astype(int)\n",
    "    categorical_matrix2 = np.array(categorical_features2)\n",
    "\n",
    "    # 添加一行-2\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    continuous_matrix = np.vstack((continuous_matrix, nan_row))\n",
    "    # 计算每一列的最大值加1\n",
    "    new_row1 = np.full(categorical_matrix1.shape[1], 2)\n",
    "    max_values2 = np.max(categorical_matrix2, axis=0) + 1\n",
    "    # 将最大值加1的行添加到categorical_matrix1\n",
    "    categorical_matrix1 = np.vstack((categorical_matrix1,new_row1))\n",
    "    categorical_matrix2 = np.vstack((categorical_matrix2, max_values2))\n",
    "\n",
    "    \n",
    "\n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, house_multi_matrix, num_categories_list2\n",
    "    else:\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, num_categories_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b21d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取host图片特征\n",
    "def read_host_pic_features(path):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data['host_id'].unique())}\n",
    "    # 提取连续特征\n",
    "    continuous_features =  data.iloc[:, 5:]\n",
    "    host_pic_continuous_column = data.iloc[:, 5:].columns.tolist()\n",
    "    \n",
    "    #补缺失值-2\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "        \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "\n",
    "    #当host没有pic特征时，取最后一行，特征为-2\n",
    "    # 创建全为nan的一行\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    # 在continuous_matrix最后添加zero_row\n",
    "    continuous_matrix = np.vstack((continuous_matrix,nan_row))\n",
    "\n",
    "    return index,continuous_matrix,host_pic_continuous_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f81314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取house和host以及负样本动态特征\n",
    "#读取house和host以及负样本动态特征\n",
    "def read_comment_features(path,id_column,house_continuous_column,host_continuous_column):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "    # 提取连续特征\n",
    "    house_continuous_features = data[house_continuous_column]\n",
    "    host_continuous_features = data[host_continuous_column]\n",
    "    #补缺失值-2\n",
    "    house_continuous_features=house_continuous_features.fillna(-2)\n",
    "    host_continuous_features=host_continuous_features.fillna(-2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    house_continuous_matrix = np.array(house_continuous_features)\n",
    "    host_continuous_matrix = np.array(host_continuous_features)\n",
    "    \n",
    "    # 创建一个全为 -2 的行\n",
    "    nan_row1 = np.full((1, house_continuous_matrix.shape[1]), -2)\n",
    "    nan_row2 = np.full((1, host_continuous_matrix.shape[1]), -2)\n",
    "    # 将 nan_row 插入到 host_continuous_matrix 的最后一行\n",
    "    house_continuous_matrix = np.vstack((house_continuous_matrix, nan_row1))\n",
    "    host_continuous_matrix = np.vstack((host_continuous_matrix, nan_row2))\n",
    "\n",
    "    #提取对应的house_id和host_id\n",
    "    comment_house=np.array(data['listing_id'])\n",
    "    comment_host = np.array(data['host_id'])\n",
    "    #最后一行为-1，comment_id不存在时，house_id,host_id为-1\n",
    "    comment_house = np.append(comment_house, -1)\n",
    "    comment_host = np.append(comment_host, -1)\n",
    "\n",
    "    return index, house_continuous_matrix, host_continuous_matrix, comment_house,comment_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37749b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换 history 列为长度为15的数组\n",
    "def process_history(history,max_history_len):\n",
    "    if len(history) >= max_history_len:\n",
    "        processed_history = history[-max_history_len:]\n",
    "    else:\n",
    "        processed_history = [-1] * (max_history_len - len(history)) + history\n",
    "    return processed_history\n",
    "\n",
    "\n",
    "# 将填充-1的位置标记为True\n",
    "def create_mask(history):\n",
    "    mask = [True if item == -1 else False for item in history]\n",
    "    return mask\n",
    "\n",
    "#获取所有用户历史记录对应的house、host索引\n",
    "def user_history(path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len):\n",
    "    data = pd.read_csv(path)\n",
    "    # 将history列的数据转换为列表格式\n",
    "    data['history'] = data['history'].apply(lambda x: [int(item) for item in x.strip('[]').split()])\n",
    "    # 获取唯一的 reviewer_id\n",
    "    unique_reviewer_ids = data['reviewer_id'].unique()\n",
    "\n",
    "    # 创建 reviewer_id 到编号的映射字典\n",
    "    user_index = {reviewer_id: idx for idx, reviewer_id in enumerate(unique_reviewer_ids)}\n",
    "    # 应用处理函数到每个 history\n",
    "    data['processed_history'] = data['history'].apply(lambda x: process_history(x, max_history_len))  # shape（user_num,history_num）,comment_id,填充为-1\n",
    "    # 生成整个 history 的矩阵\n",
    "    history_matrix = np.array(data['processed_history'].tolist())\n",
    "\n",
    "    # 将 history_matrix 中的 comment_id 转换为 host_id，不存在的设为 -1\n",
    "    history_matrix_host = []\n",
    "    history_matrix_house = []\n",
    "    history_matrix_host_pic = []\n",
    "    history_matrix_mask = []\n",
    "    history_matrix_comment = []\n",
    "    for user_history in history_matrix:\n",
    "        user_history_host = []\n",
    "        user_history_host_pic = []\n",
    "        user_history_house = []\n",
    "        user_history_mask = []\n",
    "        user_history_comment = []\n",
    "\n",
    "        for comment_id in user_history:\n",
    "            user_history_comment.append(comment_index.get(comment_id, -1))  # comment编号，填充的comment编号为-1\n",
    "            user_history_host_id = comment_host[comment_index.get(comment_id, -1)]  # host_id\n",
    "            user_history_house_id = comment_house[comment_index.get(comment_id, -1)]  # house_id\n",
    "            user_history_host.append(host_index.get(user_history_host_id, -1))  # host表编号\n",
    "            user_history_host_pic.append(host_pic_index.get(user_history_host_id, -1))  # host_pic表编号\n",
    "            user_history_house.append(house_index.get(user_history_house_id, -1))  # house表编号\n",
    "\n",
    "        history_matrix_comment.append(user_history_comment)  # shape（user_num,history_num）,comment编号\n",
    "        history_matrix_mask.append(create_mask(user_history))# 在填充的位置标记true\n",
    "        history_matrix_host.append(user_history_host)  # host表编号,shape（user_num,history_num）\n",
    "        history_matrix_host_pic.append(user_history_host_pic)  # host_pic表编号\n",
    "        history_matrix_house.append(user_history_house)  # house表编号  shape（user_num,history_num）\n",
    "\n",
    "    return user_index, history_matrix_comment, history_matrix_host, history_matrix_host_pic, history_matrix_house, history_matrix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ab7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf96144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得测试集正负样本对应的house、host索引\n",
    "def get_train_input_index(path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor):\n",
    "    neg_data=pd.read_csv(path)   #负样本特征表，train_neg_comment_path\n",
    "    df_comment=pd.read_csv(comment_path)  #评论表\n",
    "    #读取负样本动态特征，房子房东id，以及对应的正样本的评论id\n",
    "    pos_comment_id=[]\n",
    "    neg_house_comment_features=[]\n",
    "    neg_host_comment_features=[]\n",
    "    neg_house_id=[]\n",
    "    neg_host_id=[]\n",
    "    grouped_neg_data = neg_data.groupby('pos_comment_id')\n",
    "    for name, group in grouped_neg_data:\n",
    "        pos_comment_id.append(name)  #正样本comment_id\n",
    "        neg_house_comment_matrix=group[house_comment_continuous_column].values[:]\n",
    "        neg_host_comment_matrix=group[host_comment_continuous_column].values[:]\n",
    "        \n",
    "        #先填缺失值，再标准化\n",
    "        neg_house_comment_matrix = np.nan_to_num(neg_house_comment_matrix, nan=-2)\n",
    "        neg_host_comment_matrix = np.nan_to_num(neg_host_comment_matrix, nan=-2)\n",
    "        \n",
    "        #标准化\n",
    "        neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        \n",
    "        neg_house_comment_features.append(neg_house_comment_matrix)  #负样本房子评论特征(6,特征数)\n",
    "        neg_host_comment_features.append(neg_host_comment_matrix)#负样本房东评论特征\n",
    "        neg_house_id.append(group['listing_id'].values[:])  #负样本房子id\n",
    "        neg_host_id.append(group['host_id'].values[:])\n",
    "        \n",
    "\n",
    "    \n",
    "    #负样本特征转换成tensor\n",
    "    neg_house_comment_features=torch.tensor(neg_house_comment_features).clone().detach()\n",
    "    neg_host_comment_features=torch.tensor(neg_host_comment_features).clone().detach()\n",
    "    \n",
    "    #根据id，获得负样本房子，房东的index\n",
    "    neg_house_id=np.array(neg_house_id)\n",
    "    neg_host_id=np.array(neg_host_id)\n",
    "    # 使用NumPy的vectorized操作获取所有id对应的索引\n",
    "    input_neg_house_index= np.vectorize(house_index.get)(neg_house_id)\n",
    "    input_neg_host_pic_index= np.vectorize(host_pic_index.get,otypes=[int])(neg_host_id,-1)\n",
    "    input_neg_host_index = np.vectorize(host_index.get, otypes=[int])(neg_host_id, -1)\n",
    "    \n",
    "    #根据正样本的评论id，获得房子，房东。comment。reviewer的index\n",
    "    #获取正样本的评论id对应的index\n",
    "    input_pos_comment_index=np.vectorize(comment_index.get)(pos_comment_id)\n",
    "    #获得房子，房东。reviewer的id\n",
    "    input_pos_house_id=comment_house[input_pos_comment_index]\n",
    "    input_pos_host_id=comment_host[input_pos_comment_index]\n",
    "    input_reviewer_id=df_comment['reviewer_id'][input_pos_comment_index].to_numpy()\n",
    "    #获得房子，房东。reviewer的index\n",
    "    input_pos_house_index=np.vectorize(house_index.get)(input_pos_house_id)\n",
    "    input_pos_host_index=np.vectorize(host_index.get)(input_pos_host_id)\n",
    "    input_pos_host_pic_index=np.vectorize(host_pic_index.get,otypes=[int])(input_pos_host_id,-1)\n",
    "    input_reviewer_index=np.vectorize(user_index.get)(input_reviewer_id)\n",
    "\n",
    "    #获得正样本的动态特征\n",
    "    pos_house_comment_features =house_comment_tensor[input_pos_comment_index]\n",
    "    pos_host_comment_features =host_comment_tensor[input_pos_comment_index]\n",
    "    \n",
    "    #合并正负样本\n",
    "    #动态特征\n",
    "    house_comment_features = torch.cat((pos_house_comment_features.unsqueeze(1), neg_house_comment_features), dim=1)\n",
    "    host_comment_features = torch.cat((pos_host_comment_features.unsqueeze(1), neg_host_comment_features), dim=1)\n",
    "    #房子，房东的index\n",
    "    input_house_index=np.concatenate((input_pos_house_index[:, np.newaxis], input_neg_house_index), axis=1)\n",
    "    input_host_index=np.concatenate((input_pos_host_index[:, np.newaxis], input_neg_host_index), axis=1)\n",
    "    input_host_pic_index=np.concatenate((input_pos_host_pic_index[:, np.newaxis], input_neg_host_pic_index), axis=1)\n",
    "    \n",
    "    label = np.zeros((len(input_house_index), 1 + npratio))\n",
    "    label[:, 0] = 1\n",
    "    label= torch.tensor(label)\n",
    "    \n",
    "    return pos_comment_id,input_house_index,input_host_index,input_host_pic_index,input_reviewer_index,label,house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851dbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据house索引，获得house的特征，返回为tensor\n",
    "def get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,input_house_index):\n",
    "    # 样本编号转换成tensor\n",
    "    input_house_index = torch.tensor(input_house_index).clone().detach()\n",
    "    # house\n",
    "    # continuous_features\n",
    "    house_continuous_features = house_continuous_tensor[input_house_index]\n",
    "    # categorical_features1\n",
    "    house_categorical_features1 = house_categorical_tensor1[input_house_index]\n",
    "    # categorical_features2\n",
    "    house_categorical_feature2 = house_categorical_tensor2[input_house_index]\n",
    "    # multi_features\n",
    "    house_multi_features = house_multi_tensor[input_house_index]\n",
    "\n",
    "    return house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c309aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,input_host_index, input_host_pic_index):\n",
    "    # host\n",
    "    # continuous_features\n",
    "    host_continuous_features = host_continuous_tensor[input_host_index]\n",
    "    # categorical_features1\n",
    "    host_categorical_features1 = host_categorical_tensor1[input_host_index]\n",
    "    # categorical_features2\n",
    "    host_categorical_features2 = host_categorical_tensor2[input_host_index]\n",
    "    # host_pic_features\n",
    "    host_pic_features = host_pic_continuous_tensor[input_host_pic_index]\n",
    "            \n",
    "    return host_continuous_features,host_categorical_features1,host_categorical_features2,host_pic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccf91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据样本中的user编号获得对应house、host编号\n",
    "def get_user_input_index(history_matrix_comment,history_matrix_host,history_matrix_host_pic,history_matrix_house,history_matrix_mask,input_reviewer_index):\n",
    "    input_history_comment = history_matrix_comment[input_reviewer_index]  # shape(样本数, max_history)\n",
    "    input_history_host = history_matrix_host[input_reviewer_index]\n",
    "    input_history_host_pic = history_matrix_host_pic[input_reviewer_index]\n",
    "    input_history_house = history_matrix_house[input_reviewer_index]\n",
    "    input_history_mask = history_matrix_mask[input_reviewer_index]\n",
    "\n",
    "    return input_history_comment,input_history_host,input_history_host_pic,input_history_house,input_history_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc53242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取user历史house、host的动态特征（只有正样本）\n",
    "def get_history_comment_input(history_comment_index,house_comment_tensor,host_comment_tensor):\n",
    "    # 样本编号转换成tensor\n",
    "    history_comment_index = torch.tensor(history_comment_index).clone().detach()\n",
    "    house_comment_features=house_comment_tensor[history_comment_index]\n",
    "    host_comment_features=host_comment_tensor[history_comment_index]\n",
    "    return house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c0729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_scaler(path,column):\n",
    "    data=pd.read_csv(path)\n",
    "    continuous_features=data[column]\n",
    "    # 补缺失值-2\n",
    "    continuous_features= continuous_features.fillna(-2)\n",
    "    continuous_matrix=np.array(continuous_features)\n",
    "    # 创建一个StandardScaler对象\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(continuous_matrix)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e6cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#dnn层,hidden_units=[[输入1，输出1],[输入2，输出2]]\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden_units, activation, l2_reg, dropout_rate, use_bn):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # 创建一个空的列表用于存储神经网络的层\n",
    "        layers = []\n",
    "\n",
    "        # 遍历每个隐藏层的设置\n",
    "        for hidden_size in hidden_units:\n",
    "            # 添加一个线性层（全连接层），输入维度为 hidden_size[0]，输出维度为 hidden_size[1]\n",
    "            layers.append(nn.Linear(hidden_size[0], hidden_size[1]))\n",
    "\n",
    "            # 如果 use_bn 为 True，则添加批归一化层\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size[1]))\n",
    "\n",
    "            # 添加激活函数层，激活函数为传入的 activation 函数\n",
    "            layers.append(activation)\n",
    "\n",
    "            # 添加 dropout 层，dropout_rate 为 dropout 的比例\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # 将所有层组合成一个顺序的神经网络\n",
    "        self.dnn_network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在前向传播过程中，输入数据 x 经过 dnn_network 这个神经网络模型\n",
    "        return self.dnn_network(x)\n",
    "\n",
    "#用户建模\n",
    "#用户建模\n",
    "class DinAttention(nn.Module):\n",
    "    def __init__(self,attetion_dnn_hidden_units):\n",
    "        super(DinAttention, self).__init__()\n",
    "        self.dnn1= DNN(attetion_dnn_hidden_units, nn.Sigmoid(), 0.0, 0.0, False)\n",
    "        self.dnn2= nn.Linear(feature_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, history_matrix, house,mask=None):\n",
    "        query= torch.unsqueeze(house, dim=-2)#(batch,npratio+1,1,400)\n",
    "        keys=torch.unsqueeze(history_matrix, dim=1)  #(batch,1,15,400)\n",
    "        query_len=query.size()[1]  #npratio+1\n",
    "#         keys = keys.repeat(1,query_len,1,1)#(batch,npratio+1,15,400)\n",
    "        keys = keys.expand(-1,query_len ,-1, -1)\n",
    "        keys_len = keys.size()[2]   #15\n",
    "#         querys = query.repeat(1,1, keys_len, 1)#(batch,npratio+1,15,400)\n",
    "        querys = query.expand(-1,-1, keys_len, -1)\n",
    "        atten_input = torch.cat([querys, keys, querys - keys, querys * keys], dim=-1)  # (batch,npratio+1,T, 4 * embed_size)\n",
    "        \n",
    "        # 经过三层全连接层\n",
    "        output1 = self.dnn1(atten_input)\n",
    "        output2 = self.dnn2(output1)  # (batch,npratio+1,T, 1)\n",
    "#         outputs = output2.transpose(2, 3)  # (batch,npratio+1, 1,T)\n",
    "#         user_att = outputs.view(-1, keys_len)  # 展平为一维向量（batch，15）\n",
    "        user_att = output2.squeeze(-1) #每个历史记录的权重(batch,npratio+1,T)\n",
    "        \n",
    "        if mask is not None:\n",
    "            #mask(batch,T),扩展成(batch,npratio+1,T)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.repeat(1,query_len,1)\n",
    "            user_att = user_att.masked_fill(mask, float('-inf'))  # 对填充向量进行mask操作(batch,npratio+1,T)\n",
    "        user_att = user_att / (keys.size()[-1] ** 0.5)\n",
    "        user_att = torch.softmax(user_att, dim=-1)  # (batch,npratio+1,T)\n",
    "        weighted_sum = torch.matmul(user_att.unsqueeze(-2), keys).squeeze(-2)  # (batch,npratio+1,400)\n",
    "        return weighted_sum\n",
    "\n",
    " # 全连接层\n",
    "def dense_layer(in_features, out_features):\n",
    "    # in_features=hidden_size,out_features=1\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features,bias=True),\n",
    "        nn.ReLU())\n",
    "\n",
    "\n",
    "def categorical1_embedding(num_categorical1_features): #输入categorical1的特征数\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical1_embeddings = []\n",
    "    for i in range(num_categorical1_features):\n",
    "        embedding_layer1 = nn.Embedding(3, feature_dim)\n",
    "        categorical1_embeddings.append(embedding_layer1)\n",
    "    return categorical1_embeddings\n",
    "\n",
    "def categorical2_embedding(num_categorical2_features,num_categories_list2): #输入categorical2的特征数，各个特征的特征值数量的列表\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical2_embeddings = []\n",
    "    # 循环遍历每个特征，为每个特征创建嵌入层并添加到列表中\n",
    "    for i in range(num_categorical2_features):\n",
    "        categorie2_embedding_dim = int(math.log2(num_categories_list2[i]+1))  # 嵌入维度为类别数的底数为2的对数\n",
    "        embedding_layer2 = nn.Embedding(num_categories_list2[i]+1, feature_dim)\n",
    "        categorical2_embeddings.append(embedding_layer2)\n",
    "    return categorical2_embeddings\n",
    "\n",
    "def continuous_embedding(num_continuous_features):\n",
    "    continuous_embedding_layers = []\n",
    "    for i in range(num_continuous_features):\n",
    "        embedding_layer = dense_layer(1, feature_dim)\n",
    "        continuous_embedding_layers.append(embedding_layer)\n",
    "    return continuous_embedding_layers\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HouseEmbedding(nn.Module):\n",
    "    def __init__(self,num_house_categorical1_features,num_house_categorical2_features,num_house_categories_list2,\n",
    "                 num_house_multi_features,multi_embedding_dim,num_host_categorical1_features,num_host_categorical2_features,\n",
    "                 num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                 num_host_pic_continuous_features,num_house_comment_features,feature_dim):\n",
    "        super(HouseEmbedding, self).__init__()\n",
    "        self.multi_embedding_dim=multi_embedding_dim\n",
    "        self.num_house_multi_features=num_house_multi_features\n",
    "        self.sum_num_features=num_house_categorical1_features+num_house_categorical2_features+num_house_continuous_features+num_house_multi_features+num_house_comment_features+num_host_categorical1_features+num_host_categorical2_features+num_host_continuous_features+num_host_pic_continuous_features\n",
    "        # house_categorical1\n",
    "        self.house_categorical1_embeddings = categorical1_embedding(num_house_categorical1_features)\n",
    "        #house_categorical2\n",
    "        self.house_categorical2_embeddings = categorical2_embedding(num_house_categorical2_features,num_house_categories_list2)\n",
    "        #house_continuous\n",
    "        self.house_continuous_embedding_layer = continuous_embedding(num_house_continuous_features)\n",
    "        #house_multi\n",
    "        self.multi_embedding_layer = categorical1_embedding(num_house_multi_features)\n",
    "        #house_comment\n",
    "        self.house_comment_embedding_layer = continuous_embedding(num_house_comment_features)\n",
    "\n",
    "        # host_categorical1\n",
    "        self.host_categorical1_embeddings = categorical1_embedding(num_host_categorical1_features)\n",
    "        # host_categorical2\n",
    "        self.host_categorical2_embeddings = categorical2_embedding(num_host_categorical2_features,num_host_categories_list2)\n",
    "        #host_continuous_features\n",
    "        self.host_continuous_embedding_layer = continuous_embedding(num_host_continuous_features)\n",
    "        #host_pic_features\n",
    "        self.host_pic_embedding_layer = continuous_embedding(num_host_pic_continuous_features)\n",
    "        \n",
    "        # MLP256-11\n",
    "        self.house_dense_layer = dense_layer(self.sum_num_features*feature_dim,feature_dim)\n",
    "\n",
    "    def forward(self, house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features):\n",
    "        # house\n",
    "        # house_continuous_features(样本数, 2, 14)\n",
    "        # house_multi_features(样本数, 2, 42->18),(样本数, 15, 42->18)\n",
    "        # 对多值特征进行嵌入\n",
    "        house_multi_embeddings = torch.cat(\n",
    "            [embedding_layer(house_multi_features[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.multi_embedding_layer)], dim=-1)  \n",
    "        # house_comment_features（样本数, 2, 18）\n",
    "        # house_categories_embeddings1,shape(样本数，2,2(2*1))\n",
    "        house_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(house_categorical_features1[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical1_embeddings)], dim=-1)\n",
    "        # house_categories_embeddings2，shape(样本数，2,(52+4)->embedding维度之和=7(5+2))\n",
    "        house_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(house_categorical_feature2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical2_embeddings)], dim=-1)\n",
    "        #连续值embedding(样本数，2,特征数,200)\n",
    "        house_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(house_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_continuous_embedding_layer)], dim=-1)\n",
    "        # house_comment_features（样本数, 2, 19,200）\n",
    "        house_comment_embeddings = torch.cat(\n",
    "            [embedding_layer(house_comment_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_comment_embedding_layer)], dim=-1)\n",
    "        \n",
    "        # concate（样本数, 2,61）\n",
    "        house_con = torch.cat(\n",
    "            [house_continuous_embeddings, house_multi_embeddings, house_comment_embeddings, house_categories_embeddings1,\n",
    "             house_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        # host_categories_embeddings1，shape(样本数，2,10(2*5))\n",
    "        host_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features1[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical1_embeddings)], dim=-1)\n",
    "        #host_categories_embeddings2，shape(样本数，2,5->2(2*1))\n",
    "        host_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical2_embeddings)], dim=-1)\n",
    "        #host_continuous_features\n",
    "        host_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(host_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_continuous_embedding_layer)], dim=-1)\n",
    "        #host_pic_features\n",
    "        host_pic_embeddings = torch.cat(\n",
    "            [embedding_layer(host_pic_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_pic_embedding_layer)], dim=-1)        \n",
    "    \n",
    "        # concate(样本数，2,195)-host_comment_features(11)=184\n",
    "        host_con = torch.cat(\n",
    "            [host_continuous_embeddings,host_pic_embeddings,host_categories_embeddings1,host_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        all_con=torch.cat(\n",
    "            [house_con,host_con], dim=-1)\n",
    "        \n",
    "        house_vec = self.house_dense_layer(all_con.float())  # shape(样本数, 2,200)\n",
    "        return house_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a77e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingModel(nn.Module):\n",
    "    def __init__(self, num_house_categorical1_features,num_host_categorical1_features,num_house_categorical2_features,num_host_categorical2_features,num_house_multi_features,multi_embedding_dim,\n",
    "                 num_house_categories_list2,num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                 num_host_pic_continuous_features,num_house_comment_features,feature_dim,num_heads,max_history_len,\n",
    "                 num_experts, num_tasks, expert_hidden_units, gate_hidden_units,user_dnn_hidden_units,house_dnn_hidden_units,attetion_dnn_hidden_units,device):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        #embedding\n",
    "        self.house_embedding=HouseEmbedding(num_house_categorical1_features,num_house_categorical2_features,num_house_categories_list2,\n",
    "                                            num_house_multi_features,multi_embedding_dim,num_host_categorical1_features,\n",
    "                                            num_host_categorical2_features,num_host_categories_list2,num_house_continuous_features,\n",
    "                                            num_host_continuous_features,num_host_pic_continuous_features,num_house_comment_features,\n",
    "                                            feature_dim)\n",
    "\n",
    "        self.interest_num = num_interests\n",
    "        self.num_heads =num_interests\n",
    "        self.device=device\n",
    "        self.seq_len=max_history_len\n",
    "        self.hidden_size=feature_dim\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.seq_len, self.hidden_size))  #（1，T,dim)\n",
    "        self.linear1 = nn.Sequential(\n",
    "                        nn.Linear(self.hidden_size, self.hidden_size * 4, bias=False),    #tanh(linear(dim,dim*4))  \n",
    "                        nn.Tanh()\n",
    "                    )\n",
    "        self.linear2 = nn.Linear(self.hidden_size * 4, self.num_heads, bias=False)  #（dim*4,兴趣数）\n",
    "        \n",
    "\n",
    "    def forward(self,house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,\n",
    "                 host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features,\n",
    "                 user_house_continuous_features,user_house_categorical_features1,user_house_categorical_feature2,user_house_multi_features,user_house_comment_features,\n",
    "                 user_host_continuous_features,user_host_categorical_features1,user_host_categorical_features2,user_host_comment_features,user_host_pic_features\n",
    "                ,mask):\n",
    "        #house\n",
    "        # shape(样本数,npratio+1,200)\n",
    "        house_vec=self.house_embedding(house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features)\n",
    "        # shape(样本数, 15,200)\n",
    "        user_house_emb=self.house_embedding(user_house_continuous_features,user_house_categorical_features1,user_house_categorical_feature2,user_house_multi_features,user_house_comment_features,user_host_continuous_features,user_host_categorical_features1,user_host_categorical_features2,user_host_comment_features,user_host_pic_features)\n",
    "\n",
    "        # 位置嵌入堆叠一个batch，然后与历史物品嵌入相加\n",
    "        user_house_emb_add_pos = user_house_emb + self.position_embedding.repeat(user_house_emb.shape[0], 1, 1)\n",
    "        # shape=(batch_size, maxlen, hidden_size*4)\n",
    "        user_house_hidden = self.linear1(user_house_emb_add_pos)\n",
    "        # shape=(batch_size, maxlen, num_heads)\n",
    "        att_w  = self.linear2(user_house_hidden)\n",
    "        # shape=(batch_size, num_heads, maxlen)\n",
    "        att_w  = torch.transpose(att_w, 2, 1).contiguous()\n",
    "        \n",
    "        #mask后再做softmax\n",
    "        atten_mask = torch.unsqueeze(mask, dim=1).repeat(1, self.num_heads, 1)  # shape=(batch_size, num_heads, maxlen)\n",
    "        paddings = torch.ones_like(atten_mask, dtype=torch.float) * (-2 ** 32 + 1) # softmax之后无限接近于0\n",
    "        \n",
    "        item_att_w = torch.where(atten_mask.bool(), paddings, att_w)  #torch.eq(atten_mask, 0)当mak中为0时，返回true，即mask为0，填充负无穷\n",
    "        item_att_w = F.softmax(item_att_w, dim=-1) # 矩阵A，shape=(batch_size, num_heads, maxlen)\n",
    "        \n",
    "        # interest_emb即论文中的Vu\n",
    "        interest_emb = torch.matmul(item_att_w, # shape=(batch_size, num_heads, maxlen)\n",
    "                                user_house_emb_add_pos # shape=(batch_size, maxlen, embedding_dim)\n",
    "                                ) # shape=(batch_size, num_heads, embedding_dim)\n",
    "        #兴趣矩阵*item=（batch，1，兴趣数，dim）*（batch，正负样本，dim，1）\n",
    "        multi_score = torch.matmul(interest_emb.unsqueeze(1),house_vec.unsqueeze(-1))  #（batch,正负样本,proposals_num.1)\n",
    "            \n",
    "        #分数最大的兴趣索引\n",
    "        user_embedding_idx = torch.argmax(multi_score, dim=2) #（batch,正负样本，1）\n",
    "        #与正样本点积最大的兴趣向量索引\n",
    "        user_embedding_idx=user_embedding_idx[:,0,0] #（batch）\n",
    "        user_embedding_idx = user_embedding_idx.unsqueeze(1)  # 将维度扩展为（batch，1）\n",
    "        user_embedding_idx = user_embedding_idx.unsqueeze(2)  # 将维度扩展为（batch，1，1）\n",
    "        #在兴趣向量矩阵中找到索引对应的兴趣向量作为用户向量（batch，1，dim）\n",
    "        user_embedding= torch.gather(interest_emb, 1, user_embedding_idx.expand(len(user_embedding_idx), 1, feature_dim))\n",
    "        pos_house_vec=house_vec[:,0,:] .unsqueeze(1) #(batch,1,dim)\n",
    "        neg_house_vec=house_vec[:,1:,:]\n",
    "        #用于loss的分数(batch,1,1,dim)*#(batch,1,dim,1)\n",
    "        pos_score= torch.matmul(user_embedding.unsqueeze(1),pos_house_vec.unsqueeze(-1)).squeeze(-2).squeeze(-1) #(batch,1)\n",
    "        neg_score= torch.matmul(user_embedding.unsqueeze(1),neg_house_vec.unsqueeze(-1)).squeeze(-2).squeeze(-1) #(batch,负样本)\n",
    "\n",
    "        #用于排序的分数 (batch,正负样本)\n",
    "        score, _=torch.max(multi_score,dim=2)\n",
    "        score=score.squeeze(-1)  #(batch,正负样本)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return score,pos_score,neg_score,item_att_w\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c82a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atten_loss(attention):\n",
    "    #attention=item_att_w(batch_size, num_heads, maxlen)\n",
    "    C_mean = torch.mean(attention, dim=2, keepdim=True) #(batch_size, num_heads)\n",
    "    C_reg = (attention - C_mean)  #(batch_size, num_heads, maxlen)\n",
    "    # C_reg = C_reg.matmul(C_reg.transpose(1,2)) / self.hidden_size\n",
    "    C_reg = torch.bmm(C_reg, C_reg.transpose(1, 2))/feature_dim  #(batch_size, num_heads, num_heads)\n",
    "    dr = torch.diagonal(C_reg, dim1=-2, dim2=-1) #(batch_size, num_heads)\n",
    "    n2 = torch.norm(dr, dim=(1)) ** 2\n",
    "    return n2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b6f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e08a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, positive_scores, negative_scores):\n",
    "        # 计算正样本和负样本之间的分数差异\n",
    "        loss = -torch.log(torch.sigmoid(positive_scores - negative_scores))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc01b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc38237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model,train_loader,val_loader,\n",
    "                   rec_criterion,optimizer,EPOCH,device):\n",
    "    # 定义早停策略的参数\n",
    "    best_val_loss = float('inf')  # 初始化最佳验证损失为正无穷\n",
    "    patience = 3  # 容忍多少个epoch没有验证性能提升\n",
    "    early_stopping_counter = 0  # 初始化计数器\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        total_classfier_loss = 0.0\n",
    "        total_rec_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        total_loss_attend = 0.0\n",
    "        total_loss_contrastive = 0.0\n",
    "        total_loss_construct = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = [data.to(device) for data in batch]\n",
    "            house_continuous_features, house_categorical_features1, house_categorical_feature2, house_multi_features, house_comment_features, \\\n",
    "            host_continuous_features, host_categorical_features1, host_categorical_features2, host_comment_features, host_pic_features, \\\n",
    "            user_house_continuous_features, user_house_categorical_features1, user_house_categorical_feature2, user_house_multi_features, user_house_comment_features, \\\n",
    "            user_host_continuous_features, user_host_categorical_features1, user_host_categorical_features2, user_host_comment_features, user_host_pic_features, \\\n",
    "            label, mask = batch\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            score,pos_score,neg_score,item_att_w= model(house_continuous_features,\n",
    "                                                                   house_categorical_features1,\n",
    "                                                                   house_categorical_feature2, house_multi_features,\n",
    "                                                                   house_comment_features,\n",
    "                                                                   host_continuous_features, host_categorical_features1,\n",
    "                                                                   host_categorical_features2, host_comment_features,\n",
    "                                                                   host_pic_features,\n",
    "                                                                   user_house_continuous_features,\n",
    "                                                                   user_house_categorical_features1,\n",
    "                                                                   user_house_categorical_feature2,\n",
    "                                                                   user_house_multi_features,\n",
    "                                                                   user_house_comment_features,\n",
    "                                                                   user_host_continuous_features,\n",
    "                                                                   user_host_categorical_features1,\n",
    "                                                                   user_host_categorical_features2,\n",
    "                                                                   user_host_comment_features, user_host_pic_features\n",
    "                                                                   , mask)\n",
    "            \n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            softmax_score=torch.softmax(score,dim=1)  #(batch,正负样本)\n",
    "            rec_loss=softmax_score[:,0]  #正样本分数\n",
    "            rec_loss = torch.mean(-torch.log(rec_loss))\n",
    "            reg_loss=calculate_atten_loss(item_att_w)\n",
    "            loss = rec_loss+λ*reg_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #计算平均loss\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "        \n",
    "\n",
    "        average_loss=total_loss/len(train_loader)\n",
    "        \n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"Epoch {epoch + 1},loss:{average_loss}\")\n",
    "            \n",
    "            # 验证集评估\n",
    "            model.eval()  # 将模型切换为评估模式\n",
    "            with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "                total_loss_val = 0.0\n",
    "                total_val_auc = 0.0\n",
    "                total_regu_loss_val=0.0\n",
    "                total_rec_loss_val = 0.0\n",
    "                total_loss_attend_val = 0.0\n",
    "                total_loss_contrastive_val = 0.0\n",
    "                total_loss_construct_val = 0.0\n",
    "                for batch_val in val_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "                    batch_val = [data.to(device) for data in batch_val]\n",
    "                    house_continuous_features_val, house_categorical_features1_val, house_categorical_feature2_val, house_multi_features_val, house_comment_features_val, \\\n",
    "                    host_continuous_features_val, host_categorical_features1_val, host_categorical_features2_val, host_comment_features_val, host_pic_features_val, \\\n",
    "                    user_house_continuous_features_val, user_house_categorical_features1_val, user_house_categorical_feature2_val, user_house_multi_features_val, user_house_comment_features_val, \\\n",
    "                    user_host_continuous_features_val, user_host_categorical_features1_val, user_host_categorical_features2_val, user_host_comment_features_val, user_host_pic_features_val, \\\n",
    "                    label_val, mask_val = batch_val\n",
    "                    #在验证集上进行前向传播\n",
    "                    score_val,pos_score_val,neg_score_val,item_att_w_val= model(house_continuous_features_val, house_categorical_features1_val,\n",
    "                                                    house_categorical_feature2_val, house_multi_features_val,\n",
    "                                                    house_comment_features_val, host_continuous_features_val,\n",
    "                                                    host_categorical_features1_val, host_categorical_features2_val,\n",
    "                                                    host_comment_features_val, host_pic_features_val,\n",
    "                                                    user_house_continuous_features_val,\n",
    "                                                    user_house_categorical_features1_val,\n",
    "                                                    user_house_categorical_feature2_val, user_house_multi_features_val,\n",
    "                                                    user_house_comment_features_val, user_host_continuous_features_val,\n",
    "                                                    user_host_categorical_features1_val, user_host_categorical_features2_val,\n",
    "                                                    user_host_comment_features_val, user_host_pic_features_val,\n",
    "                                                    mask_val)\n",
    "\n",
    "                    \n",
    "                    #推荐损失\n",
    "                    softmax_score_val=torch.softmax(score_val,dim=1)  #(batch,正负样本)\n",
    "                    rec_loss_val=softmax_score_val[:,0]\n",
    "                    rec_loss_val = torch.mean(-torch.log(rec_loss_val))\n",
    "                    reg_loss_val=calculate_atten_loss(item_att_w_val)\n",
    "                    loss_val = rec_loss_val+λ*reg_loss_val\n",
    "                    #总损失\n",
    "\n",
    "                    total_loss_val+=loss_val.item()\n",
    "\n",
    "                    # 计算验证集上的AUC\n",
    "                    positive_scores_val = score_val[:, 0]\n",
    "                    positive_scores_val = positive_scores_val.unsqueeze(-1)\n",
    "                    negative_scores_val = score_val[:, 1:]\n",
    "                    predictions_val = (positive_scores_val > negative_scores_val).float()\n",
    "                    equal_indices_val = positive_scores_val == negative_scores_val\n",
    "                    predictions_val[equal_indices_val] = 0.5\n",
    "                    correct_val = predictions_val.sum().item()\n",
    "                    total_val_auc += correct_val / (len(predictions_val)*npratio)\n",
    "                    \n",
    "                average_val_loss = total_loss_val / len(val_loader)\n",
    "                average_auc_val=total_val_auc/ len(val_loader)\n",
    "\n",
    "                print(f\"Validation Loss: {average_val_loss},AUC: {average_auc_val}\")\n",
    "                if average_val_loss<best_val_loss:\n",
    "                    best_val_loss = average_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"早停策略触发，停止训练在第 {epoch} 个epoch.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e45d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training(model,train_loader,val_loader,classfier,house_embedding,host_embedding,classfier_optimizer,classfier_criterion,att_criterion,rec_criterion,optimizer,500,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c097e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "496bb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_centered_distance(X):\n",
    "    # 计算每个样本的平方模长\n",
    "    r = torch.sum(X ** 2, dim=1, keepdim=True)\n",
    "\n",
    "    # 计算欧氏距离矩阵\n",
    "    D = torch.sqrt(torch.clamp(r - 2 * torch.mm(X, X.t()) + r.t(), min=0.0) + 1e-8)\n",
    "\n",
    "    # 计算中心化距离矩阵\n",
    "    D -= torch.mean(D, dim=0, keepdim=True)\n",
    "    D -= torch.mean(D, dim=1, keepdim=True)\n",
    "    D += torch.mean(D)\n",
    "    return D\n",
    "def create_distance_covariance(D1, D2):\n",
    "    # 计算 D1 和 D2 之间的距离协方差\n",
    "    n_samples = float(D1.size(0))\n",
    "    dcov = torch.sqrt(torch.clamp(torch.sum(D1 * D2) / (n_samples * n_samples), min=0.0) + 1e-8)\n",
    "    return dcov\n",
    "def create_distance_correlation( X1, X2):\n",
    "    X1=X1.reshape(-1,feature_dim)\n",
    "    X2=X2.reshape(-1,feature_dim)\n",
    "    D1 = create_centered_distance(X1)\n",
    "    D2 = create_centered_distance(X2)\n",
    "\n",
    "    dcov_12 = create_distance_covariance(D1, D2)\n",
    "    dcov_11 = create_distance_covariance(D1, D1)\n",
    "    dcov_22 = create_distance_covariance(D2, D2)\n",
    "\n",
    "    dcor = dcov_12 / (torch.sqrt(torch.clamp(dcov_11 * dcov_22, min=0.0)) + 1e-10)\n",
    "    return dcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        total_loss_test = 0.0\n",
    "        total_rec_loss_test = 0.0\n",
    "        total_test_auc=0.0\n",
    "        total_loss_attend_test = 0.0\n",
    "        total_loss_contrastive_test = 0.0\n",
    "        total_loss_construct_test = 0.0\n",
    "        total_distance_correlation=0.0\n",
    "        results = []  # 用于保存结果的列表\n",
    "        for batch_test in test_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "            batch_test = [data.to(device) for data in batch_test]\n",
    "            pos_comment_id,house,host,reviewer,house_continuous_features_test, house_categorical_features1_test, house_categorical_feature2_test, house_multi_features_test, house_comment_features_test, \\\n",
    "            host_continuous_features_test, host_categorical_features1_test, host_categorical_features2_test, host_comment_features_test, host_pic_features_test, \\\n",
    "            user_house_continuous_features_test, user_house_categorical_features1_test, user_house_categorical_feature2_test, user_house_multi_features_test, user_house_comment_features_test, \\\n",
    "            user_host_continuous_features_test, user_host_categorical_features1_test, user_host_categorical_features2_test, user_host_comment_features_test, user_host_pic_features_test, \\\n",
    "            label_test, mask_test = batch_test\n",
    "                    \n",
    "            score_test,pos_score_test,neg_score_test,item_att_w_test= model(house_continuous_features_test, house_categorical_features1_test,\n",
    "                                                                                                                            house_categorical_feature2_test, house_multi_features_test, \n",
    "                                                                                                                            house_comment_features_test, host_continuous_features_test, \n",
    "                                                                                                                            host_categorical_features1_test, host_categorical_features2_test, \n",
    "                                                                                                                            host_comment_features_test, host_pic_features_test,                                                                                           \n",
    "                                                                                                                            user_house_continuous_features_test, user_house_categorical_features1_test, \n",
    "                                                                                                                            user_house_categorical_feature2_test, user_house_multi_features_test,\n",
    "                                                                                                                            user_house_comment_features_test, user_host_continuous_features_test, \n",
    "                                                                                                                            user_host_categorical_features1_test, user_host_categorical_features2_test, \n",
    "                                                                                                                            user_host_comment_features_test, user_host_pic_features_test,mask_test)\n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            softmax_score_test=torch.softmax(score_test,dim=1)  #(batch,正负样本)\n",
    "            rec_loss_test=softmax_score_test[:,0]\n",
    "            rec_loss_test = torch.mean(-torch.log(rec_loss_test))\n",
    "            reg_loss_test=calculate_atten_loss(item_att_w_test)\n",
    "            loss_test = rec_loss_test+λ*reg_loss_test\n",
    "            #总损失\n",
    "\n",
    "            total_loss_test+=loss_test.item()\n",
    "            \n",
    "            \n",
    "            # 计算验证集上的AUC\n",
    "            positive_scores_test = score_test[:, 0]\n",
    "            positive_scores_test = positive_scores_test.unsqueeze(-1)\n",
    "            negative_scores_test = score_test[:, 1:]\n",
    "            predictions_test = (positive_scores_test > negative_scores_test).float()\n",
    "            equal_indices = positive_scores_test == negative_scores_test\n",
    "            predictions_test[equal_indices] = 0.5\n",
    "            correct_test = predictions_test.sum().item()\n",
    "            total_test_auc += correct_test /  (len(predictions_test)*npratio)\n",
    "\n",
    "        \n",
    "            for i in range(house.shape[0]):\n",
    "                #正样本\n",
    "                results.append({\n",
    "                    'pos_comment_id':pos_comment_id[i],\n",
    "                    'house': house[i][0],\n",
    "                    'host': host[i][0],\n",
    "                    'reviewer': reviewer[i],\n",
    "                    'score': score_test[i][0],  # 正样本分数\n",
    "\n",
    "                    'label': label_test[i][0]\n",
    "                    \n",
    "                })\n",
    "                for j in range(npratio):\n",
    "                    #负样本\n",
    "                    results.append({\n",
    "                        'pos_comment_id':pos_comment_id[i],\n",
    "                        'house': house[i][1+j],\n",
    "                        'host': host[i][1+j],\n",
    "                        'reviewer': reviewer[i],             \n",
    "                        'score': score_test[i][1+j],  # 负样本分数\n",
    "\n",
    "                        'label': label_test[i][1+j]\n",
    "                    })\n",
    "                \n",
    "        average_test_loss = total_loss_test / len(test_loader)\n",
    "        average_auc_test=total_test_auc/ len(test_loader)\n",
    "        average_rec_loss_test=total_rec_loss_test/ len(test_loader)\n",
    "\n",
    "        print(f\"Test Loss: {average_test_loss},AUC: {average_auc_test},rec_loss: {average_rec_loss_test}\")\n",
    "        \n",
    "        # 将结果列表转换为DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results['pos_comment_id'] = df_results['pos_comment_id'].apply(lambda x: x.item())\n",
    "        df_results['house'] = df_results['house'].apply(lambda x: x.item())\n",
    "        df_results['host'] = df_results['host'].apply(lambda x: x.item())\n",
    "        df_results['reviewer'] = df_results['reviewer'].apply(lambda x: x.item())\n",
    "        df_results['score'] = df_results['score'].apply(lambda x: x.item())\n",
    "\n",
    "        df_results['label'] = df_results['label'].apply(lambda x: x.item())\n",
    "        df_results['item_count'] = df_results.groupby('reviewer')['reviewer'].transform('count')\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "        return df_results,average_test_loss,average_auc_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0401b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results,average_test_loss,average_auc_test,average_att_loss_test=test_model(model, test_loader)\n",
    "# print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e458c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-n\n",
    "def top_evaluation(df_results,n):\n",
    "    df_results_shuffled = df_results.sample(frac=1).reset_index(drop=True)\n",
    "    # 初始化结果列表\n",
    "    ndcg_list = []\n",
    "    hr_list=[]\n",
    "    \n",
    "    for user_id, user_group in df_results_shuffled.groupby('pos_comment_id'):\n",
    "        user_group = user_group.sort_values(by='score', ascending=False)  # 按分数降序排序\n",
    "        top_items = user_group.head(n)['label'].tolist()  # 选取前n个的标签\n",
    "        true_labels = user_group['label'].tolist()  # 用户所有候选物品的真实标签\n",
    "                \n",
    "        # 计算NDCG\n",
    "        idcg = sorted(true_labels, reverse=True)[:n]  # 理想情况下的DCG\n",
    "        dcg = sum(rel / np.log(i + 2) for i, rel in enumerate(top_items))  # 计算DCG\n",
    "        ndcg = dcg / sum(rel / np.log(i + 2) for i, rel in enumerate(idcg))  # 计算NDCG\n",
    "        ndcg_list.append(ndcg)\n",
    "        # 计算HR@N\n",
    "        hit_rate = 1 if any(label == 1 for label in top_items) else 0  # 若Top N中有真实正样本，HR@N为1，否则为0\n",
    "        hr_list.append(hit_rate)\n",
    "\n",
    "        \n",
    "    # 计算平均值\n",
    "    average_ndcg = np.mean(ndcg_list)\n",
    "    average_hr = np.mean(hr_list)\n",
    "    \n",
    "    return average_ndcg, average_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=+1\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.6763582814414546\n",
      "Validation Loss: 3.082756280899048,AUC: 0.8472622282608695\n",
      "Epoch 10,loss:2.6151915361296454\n",
      "Validation Loss: 3.0723340096681016,AUC: 0.8521923524844719\n",
      "Epoch 15,loss:2.5681390132544175\n",
      "Validation Loss: 2.9935224367224653,AUC: 0.8556579968944098\n",
      "Epoch 20,loss:2.5501134035722264\n",
      "Validation Loss: 3.113721391429072,AUC: 0.8547457298136645\n",
      "Epoch 25,loss:2.523020820797614\n",
      "Validation Loss: 2.9795759553494663,AUC: 0.8545108695652176\n",
      "Epoch 30,loss:2.5020029679784237\n",
      "Validation Loss: 3.01599439330723,AUC: 0.8546399456521739\n",
      "Epoch 35,loss:2.49295888756806\n",
      "Validation Loss: 3.047296917956808,AUC: 0.8557822204968943\n",
      "Epoch 40,loss:2.4774825617952168\n",
      "Validation Loss: 3.021443854207578,AUC: 0.8579677795031059\n",
      "早停策略触发，停止训练在第 39 个epoch.\n",
      "Test Loss: 3.173420991897583,AUC: 0.8405363636363634,rec_loss: 0.0\n",
      "i=+2\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.643219956811869\n",
      "Validation Loss: 3.0498955042465874,AUC: 0.847710597826087\n",
      "Epoch 10,loss:2.582390110447722\n",
      "Validation Loss: 3.0092870152514912,AUC: 0.853131793478261\n",
      "Epoch 15,loss:2.526746614924017\n",
      "Validation Loss: 3.0311536374299424,AUC: 0.8540130046583849\n",
      "Epoch 20,loss:2.472195377889669\n",
      "Validation Loss: 3.0630590086397915,AUC: 0.8566013198757766\n",
      "Epoch 25,loss:2.477025279458964\n",
      "Validation Loss: 2.99752604443094,AUC: 0.8561607142857143\n",
      "Epoch 30,loss:2.4500949337797344\n",
      "Validation Loss: 2.998241072115691,AUC: 0.8545273680124225\n",
      "Epoch 35,loss:2.458918845878457\n",
      "Validation Loss: 3.007676311161207,AUC: 0.8605939440993788\n",
      "Epoch 40,loss:2.448074849146717\n",
      "Validation Loss: 2.9393778987552808,AUC: 0.8615663819875776\n",
      "Epoch 45,loss:2.4378966565402047\n",
      "Validation Loss: 2.9235320713209068,AUC: 0.8669934006211181\n",
      "Epoch 50,loss:2.425081896332075\n",
      "Validation Loss: 2.943773435509723,AUC: 0.8599369177018632\n",
      "Epoch 55,loss:2.44379689558497\n",
      "Validation Loss: 2.965805706770524,AUC: 0.8551339285714284\n",
      "Epoch 60,loss:2.3895226604533644\n",
      "Validation Loss: 2.942663814710534,AUC: 0.8605968555900623\n",
      "早停策略触发，停止训练在第 59 个epoch.\n",
      "Test Loss: 3.0815180110931397,AUC: 0.8462403409090907,rec_loss: 0.0\n",
      "i=+3\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.6721737699688606\n",
      "Validation Loss: 3.0650572362153428,AUC: 0.8462679541925466\n",
      "Epoch 10,loss:2.5956018555839107\n",
      "Validation Loss: 3.038723375486291,AUC: 0.8531968167701863\n",
      "Epoch 15,loss:2.5584104870850184\n",
      "Validation Loss: 2.9790525229080864,AUC: 0.8518483113354037\n",
      "Epoch 20,loss:2.5246966829839743\n",
      "Validation Loss: 2.971427513205487,AUC: 0.8555551242236025\n",
      "Epoch 25,loss:2.5354838191338307\n",
      "Validation Loss: 3.054342321727587,AUC: 0.8546535326086955\n",
      "Epoch 30,loss:2.5210904400303678\n",
      "Validation Loss: 3.1054883521536123,AUC: 0.8575412461180127\n",
      "Epoch 35,loss:2.535593383717087\n",
      "Validation Loss: 3.024112473363462,AUC: 0.8571200504658383\n",
      "早停策略触发，停止训练在第 34 个epoch.\n",
      "Test Loss: 3.1509596729278564,AUC: 0.8420875000000001,rec_loss: 0.0\n",
      "i=+4\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.635310370967073\n",
      "Validation Loss: 3.0087142716283384,AUC: 0.8496486801242235\n",
      "Epoch 10,loss:2.6036591439876915\n",
      "Validation Loss: 3.014725218648496,AUC: 0.8542672748447203\n",
      "Epoch 15,loss:2.5487038954248966\n",
      "Validation Loss: 3.0269567137179165,AUC: 0.8524679736024846\n",
      "Epoch 20,loss:2.4938901910242044\n",
      "Validation Loss: 3.121245404948359,AUC: 0.8522758152173914\n",
      "早停策略触发，停止训练在第 19 个epoch.\n",
      "Test Loss: 3.281481580734253,AUC: 0.8346355113636365,rec_loss: 0.0\n",
      "i=+5\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.6665669162318393\n",
      "Validation Loss: 3.0364401444144873,AUC: 0.8465828804347827\n",
      "Epoch 10,loss:2.619507420737788\n",
      "Validation Loss: 3.0703521604123325,AUC: 0.8462126358695652\n",
      "Epoch 15,loss:2.5837073551034027\n",
      "Validation Loss: 3.150837856790294,AUC: 0.8526484860248449\n",
      "Epoch 20,loss:2.590482324924109\n",
      "Validation Loss: 3.066292970076851,AUC: 0.8564450698757764\n",
      "早停策略触发，停止训练在第 19 个epoch.\n",
      "Test Loss: 3.2381733894348144,AUC: 0.8376758522727272,rec_loss: 0.0\n",
      "i=+1\n",
      "02_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.776799451920294\n",
      "Validation Loss: 3.182809133529663,AUC: 0.8267164772727271\n",
      "Epoch 10,loss:2.7261690170534196\n",
      "Validation Loss: 3.2224139022827147,AUC: 0.8294085227272727\n",
      "Epoch 15,loss:2.6991414446984567\n",
      "Validation Loss: 3.185238828659058,AUC: 0.8357778409090909\n",
      "Epoch 20,loss:2.6582123694881314\n",
      "Validation Loss: 3.158178882598877,AUC: 0.8414400568181819\n",
      "Epoch 25,loss:2.6716585082392537\n",
      "Validation Loss: 3.1519172096252444,AUC: 0.8395664772727273\n",
      "Epoch 30,loss:2.694164579914462\n",
      "Validation Loss: 3.070749464035034,AUC: 0.8427525568181818\n",
      "Epoch 35,loss:2.630319253090889\n",
      "Validation Loss: 3.1370735645294188,AUC: 0.8403153409090909\n",
      "Epoch 40,loss:2.620939547015775\n",
      "Validation Loss: 3.1856682968139647,AUC: 0.8461482954545455\n",
      "Epoch 45,loss:2.635239032007033\n",
      "Validation Loss: 3.18111234664917,AUC: 0.8383499999999999\n",
      "早停策略触发，停止训练在第 44 个epoch.\n",
      "Test Loss: 3.2976002883911133,AUC: 0.8211904069767442,rec_loss: 0.0\n",
      "i=+2\n",
      "02_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.7926977488302414\n",
      "Validation Loss: 3.223860502243042,AUC: 0.8310107954545454\n",
      "Epoch 10,loss:2.7612927998265913\n",
      "Validation Loss: 3.2394438648223876,AUC: 0.8354076704545456\n",
      "Epoch 15,loss:2.728987947587044\n",
      "Validation Loss: 3.1155091094970704,AUC: 0.8380258522727272\n",
      "Epoch 20,loss:2.759541153907776\n",
      "Validation Loss: 3.1551803016662596,AUC: 0.8326201704545454\n",
      "Epoch 25,loss:2.6898558793529386\n",
      "Validation Loss: 3.144966173171997,AUC: 0.8380991477272727\n",
      "Epoch 30,loss:2.714937383128751\n",
      "Validation Loss: 3.198988380432129,AUC: 0.8408622159090907\n",
      "早停策略触发，停止训练在第 29 个epoch.\n",
      "Test Loss: 3.30476842880249,AUC: 0.8216687500000001,rec_loss: 0.0\n",
      "i=+3\n",
      "02_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.8170658234627015\n",
      "Validation Loss: 3.1738585376739503,AUC: 0.8356755681818183\n",
      "Epoch 10,loss:2.7448321061749614\n",
      "Validation Loss: 3.299411153793335,AUC: 0.8359346590909089\n",
      "Epoch 15,loss:2.692923484310027\n",
      "Validation Loss: 3.1436312007904053,AUC: 0.8419607954545454\n",
      "Epoch 20,loss:2.7276244125058575\n",
      "Validation Loss: 3.069026918411255,AUC: 0.842865340909091\n",
      "Epoch 25,loss:2.7028043385474914\n",
      "Validation Loss: 3.1459319591522217,AUC: 0.8363232954545455\n",
      "Epoch 30,loss:2.672712975932706\n",
      "Validation Loss: 3.1033104705810546,AUC: 0.8411522727272728\n",
      "Epoch 35,loss:2.682048116960833\n",
      "Validation Loss: 3.056128158569336,AUC: 0.844480965909091\n",
      "Epoch 40,loss:2.676374070106014\n",
      "Validation Loss: 3.0544816875457763,AUC: 0.8437957386363637\n",
      "Epoch 45,loss:2.6714383248359925\n",
      "Validation Loss: 3.148386163711548,AUC: 0.8441767045454546\n",
      "Epoch 50,loss:2.6343621092457927\n",
      "Validation Loss: 3.1177173900604247,AUC: 0.8429869318181819\n",
      "Epoch 55,loss:2.6614107162721696\n",
      "Validation Loss: 3.056108808517456,AUC: 0.8486806818181819\n",
      "早停策略触发，停止训练在第 54 个epoch.\n",
      "Test Loss: 3.187458019256592,AUC: 0.8280729651162791,rec_loss: 0.0\n",
      "i=+4\n",
      "02_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.7769881948348014\n",
      "Validation Loss: 3.2227535152435305,AUC: 0.8358267045454545\n",
      "Epoch 10,loss:2.7221961482878654\n",
      "Validation Loss: 3.112046537399292,AUC: 0.8438221590909092\n",
      "Epoch 15,loss:2.6748820389470747\n",
      "Validation Loss: 3.048227796554565,AUC: 0.8483551136363636\n",
      "Epoch 20,loss:2.65922131846028\n",
      "Validation Loss: 3.154346437454224,AUC: 0.8471664772727272\n",
      "Epoch 25,loss:2.6345382236665293\n",
      "Validation Loss: 3.102172374725342,AUC: 0.8448386363636367\n",
      "Epoch 30,loss:2.637772221719065\n",
      "Validation Loss: 3.0689955615997313,AUC: 0.8486045454545453\n",
      "早停策略触发，停止训练在第 29 个epoch.\n",
      "Test Loss: 3.198723039627075,AUC: 0.8284300872093024,rec_loss: 0.0\n",
      "i=+5\n",
      "02_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.8025123919210126\n",
      "Validation Loss: 3.1475794219970705,AUC: 0.8369707386363635\n",
      "Epoch 10,loss:2.7406535110165997\n",
      "Validation Loss: 3.1036157321929934,AUC: 0.8395653409090911\n",
      "Epoch 15,loss:2.7107180703070854\n",
      "Validation Loss: 3.1052700805664064,AUC: 0.8380892045454545\n",
      "Epoch 20,loss:2.739539788615319\n",
      "Validation Loss: 3.0832169246673584,AUC: 0.842112784090909\n",
      "Epoch 25,loss:2.7154805506429365\n",
      "Validation Loss: 3.0817904472351074,AUC: 0.8419269886363636\n",
      "Epoch 30,loss:2.703870677178906\n",
      "Validation Loss: 3.0949498462677,AUC: 0.8447653409090908\n",
      "Epoch 35,loss:2.6546132756817724\n",
      "Validation Loss: 3.133804998397827,AUC: 0.8437559659090909\n",
      "Epoch 40,loss:2.655423145140371\n",
      "Validation Loss: 3.0913601112365723,AUC: 0.8471789772727272\n",
      "早停策略触发，停止训练在第 39 个epoch.\n",
      "Test Loss: 3.2188819885253905,AUC: 0.8270754360465115,rec_loss: 0.0\n",
      "i=+1\n",
      "03_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.8934386900493076\n",
      "Validation Loss: 3.299204387664795,AUC: 0.8096947674418605\n",
      "Epoch 10,loss:2.8527131489345003\n",
      "Validation Loss: 3.2447002601623534,AUC: 0.8166933139534883\n",
      "Epoch 15,loss:2.830123782157898\n",
      "Validation Loss: 3.181516017913818,AUC: 0.818292805232558\n",
      "Epoch 20,loss:2.8299989189420427\n",
      "Validation Loss: 3.1878083419799803,AUC: 0.821168168604651\n",
      "Epoch 25,loss:2.811193265233721\n",
      "Validation Loss: 3.169021711349487,AUC: 0.8217670058139533\n",
      "Epoch 30,loss:2.790858827318464\n",
      "Validation Loss: 3.1436950969696045,AUC: 0.8231156976744185\n",
      "Epoch 35,loss:2.7833311114992414\n",
      "Validation Loss: 3.134279794692993,AUC: 0.8279045058139536\n",
      "Epoch 40,loss:2.7671742609569003\n",
      "Validation Loss: 3.119758768081665,AUC: 0.8308664244186045\n",
      "Epoch 45,loss:2.7709702355521064\n",
      "Validation Loss: 3.1574893379211426,AUC: 0.8301325581395349\n",
      "Epoch 50,loss:2.7506446157182967\n",
      "Validation Loss: 3.130063228607178,AUC: 0.829124273255814\n",
      "Epoch 55,loss:2.7423801626477924\n",
      "Validation Loss: 3.1209881114959717,AUC: 0.8316508720930235\n",
      "早停策略触发，停止训练在第 54 个epoch.\n",
      "Test Loss: 3.1266166142054965,AUC: 0.8359944196428571,rec_loss: 0.0\n",
      "i=+2\n",
      "03_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.92499224117824\n",
      "Validation Loss: 3.2712604141235353,AUC: 0.8075080668604652\n",
      "Epoch 10,loss:2.856955065046038\n",
      "Validation Loss: 3.27250527381897,AUC: 0.809367078488372\n",
      "Epoch 15,loss:2.826992610522679\n",
      "Validation Loss: 3.272864589691162,AUC: 0.8158296511627907\n",
      "Epoch 20,loss:2.828610461098807\n",
      "Validation Loss: 3.171925439834595,AUC: 0.8249309593023255\n",
      "Epoch 25,loss:2.8034470455987113\n",
      "Validation Loss: 3.2174260330200197,AUC: 0.8225485465116279\n",
      "Epoch 30,loss:2.776710728236607\n",
      "Validation Loss: 3.245420274734497,AUC: 0.818220784883721\n",
      "Epoch 35,loss:2.774501589366368\n",
      "Validation Loss: 3.170586738586426,AUC: 0.8246159883720929\n",
      "Epoch 40,loss:2.7689572402409146\n",
      "Validation Loss: 3.151485424041748,AUC: 0.8260203488372092\n",
      "Epoch 45,loss:2.7650247880390713\n",
      "Validation Loss: 3.1489587020874024,AUC: 0.8275152616279067\n",
      "Epoch 50,loss:2.748836987359183\n",
      "Validation Loss: 3.2379420948028566,AUC: 0.8227771802325582\n",
      "Epoch 55,loss:2.758483873094831\n",
      "Validation Loss: 3.155791826248169,AUC: 0.8291482558139535\n",
      "Epoch 60,loss:2.739389865739005\n",
      "Validation Loss: 3.1322203731536864,AUC: 0.8274192587209303\n",
      "Epoch 65,loss:2.7314828974860057\n",
      "Validation Loss: 3.2324045658111573,AUC: 0.8308074127906976\n",
      "Epoch 70,loss:2.7371498720986502\n",
      "Validation Loss: 3.1612597560882567,AUC: 0.8271787790697673\n",
      "Epoch 75,loss:2.721183443069458\n",
      "Validation Loss: 3.15647780418396,AUC: 0.8258883720930235\n",
      "早停策略触发，停止训练在第 74 个epoch.\n",
      "Test Loss: 3.1515433873449052,AUC: 0.8305115327380952,rec_loss: 0.0\n",
      "i=+3\n",
      "03_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.897915938922337\n",
      "Validation Loss: 3.2801402378082276,AUC: 0.8109671511627907\n",
      "Epoch 10,loss:2.8801511798586166\n",
      "Validation Loss: 3.25474347114563,AUC: 0.814524273255814\n",
      "Epoch 15,loss:2.8390840428216118\n",
      "Validation Loss: 3.2603273391723633,AUC: 0.8200696220930231\n",
      "Epoch 20,loss:2.8288631166730607\n",
      "Validation Loss: 3.3613211917877197,AUC: 0.8183034883720929\n",
      "Epoch 25,loss:2.837854896272932\n",
      "Validation Loss: 3.287689323425293,AUC: 0.8142321220930232\n",
      "早停策略触发，停止训练在第 24 个epoch.\n",
      "Test Loss: 3.281066128185817,AUC: 0.8173463541666665,rec_loss: 0.0\n",
      "i=+4\n",
      "03_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.916074139731271\n",
      "Validation Loss: 3.275669870376587,AUC: 0.8123789970930231\n",
      "Epoch 10,loss:2.8655390126364573\n",
      "Validation Loss: 3.3024885177612306,AUC: 0.808049055232558\n",
      "Epoch 15,loss:2.831416766984122\n",
      "Validation Loss: 3.2334433364868165,AUC: 0.8171862645348839\n",
      "Epoch 20,loss:2.817011639050075\n",
      "Validation Loss: 3.233699207305908,AUC: 0.824314389534884\n",
      "Epoch 25,loss:2.8141107899802074\n",
      "Validation Loss: 3.2001496028900145,AUC: 0.8211563226744184\n",
      "Epoch 30,loss:2.789437522206988\n",
      "Validation Loss: 3.3434552192687987,AUC: 0.8143503633720931\n",
      "Epoch 35,loss:2.773069541794913\n",
      "Validation Loss: 3.150420866012573,AUC: 0.8306284156976745\n",
      "Epoch 40,loss:2.7820993559701104\n",
      "Validation Loss: 3.142850170135498,AUC: 0.8262024709302327\n",
      "Epoch 45,loss:2.743932100704738\n",
      "Validation Loss: 3.1851065444946287,AUC: 0.8314232558139534\n",
      "Epoch 50,loss:2.729913204056876\n",
      "Validation Loss: 3.1173025035858153,AUC: 0.8337406976744187\n",
      "Epoch 55,loss:2.750672023636954\n",
      "Validation Loss: 3.1479062938690188,AUC: 0.8302138808139534\n",
      "Epoch 60,loss:2.7493973936353413\n",
      "Validation Loss: 3.1117009925842285,AUC: 0.8304733284883722\n",
      "Epoch 65,loss:2.726096957070487\n",
      "Validation Loss: 3.1099107456207276,AUC: 0.8331228924418604\n",
      "Epoch 70,loss:2.720250725746155\n",
      "Validation Loss: 3.153060131072998,AUC: 0.8333256540697677\n",
      "Epoch 75,loss:2.7150658845901487\n",
      "Validation Loss: 3.1527482318878173,AUC: 0.8296167877906976\n",
      "Epoch 80,loss:2.6938278130122595\n",
      "Validation Loss: 3.1877888584136964,AUC: 0.8325867005813954\n",
      "早停策略触发，停止训练在第 79 个epoch.\n",
      "Test Loss: 3.194478750228882,AUC: 0.8363768601190477,rec_loss: 0.0\n",
      "i=+5\n",
      "03_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:2.8866049425942557\n",
      "Validation Loss: 3.2410173797607422,AUC: 0.8128539244186048\n",
      "Epoch 10,loss:2.8243379388536725\n",
      "Validation Loss: 3.158521614074707,AUC: 0.8264906976744184\n",
      "Epoch 15,loss:2.7775958027158465\n",
      "Validation Loss: 3.2290362548828124,AUC: 0.8200495639534884\n",
      "Epoch 20,loss:2.7706807817731587\n",
      "Validation Loss: 3.228975200653076,AUC: 0.8197491279069767\n",
      "Epoch 25,loss:2.742342162132263\n",
      "Validation Loss: 3.162000141143799,AUC: 0.8290497093023257\n",
      "早停策略触发，停止训练在第 24 个epoch.\n",
      "Test Loss: 3.1804224167551314,AUC: 0.8319869791666666,rec_loss: 0.0\n",
      "i=+1\n",
      "04_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.045406921913749\n",
      "Validation Loss: 3.2520876356533597,AUC: 0.8125732886904761\n",
      "Epoch 10,loss:2.994111020314066\n",
      "Validation Loss: 3.195761203765869,AUC: 0.8238439360119046\n",
      "Epoch 15,loss:2.9574848005646155\n",
      "Validation Loss: 3.2309586235455106,AUC: 0.8251532738095237\n",
      "Epoch 20,loss:2.925309846275731\n",
      "Validation Loss: 3.1718864696366444,AUC: 0.8269575892857144\n",
      "Epoch 25,loss:2.9040452241897583\n",
      "Validation Loss: 3.176879346370697,AUC: 0.8257535342261905\n",
      "Epoch 30,loss:2.9064545192216573\n",
      "Validation Loss: 3.1446774772235324,AUC: 0.8324233630952381\n",
      "Epoch 35,loss:2.904209193430449\n",
      "Validation Loss: 3.162321090698242,AUC: 0.8350859375\n",
      "Epoch 40,loss:2.8852735569602563\n",
      "Validation Loss: 3.2850251453263417,AUC: 0.834859375\n",
      "Epoch 45,loss:2.8756125349747506\n",
      "Validation Loss: 3.154619404247829,AUC: 0.8358113839285716\n",
      "早停策略触发，停止训练在第 44 个epoch.\n",
      "Test Loss: 3.1357444255582747,AUC: 0.8395219069630213,rec_loss: 0.0\n",
      "i=+2\n",
      "04_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.0101698009591353\n",
      "Validation Loss: 3.2316554273877824,AUC: 0.8163013392857142\n",
      "Epoch 10,loss:2.9839811827007092\n",
      "Validation Loss: 3.2274763924734935,AUC: 0.8167061011904763\n",
      "Epoch 15,loss:2.9310819851724723\n",
      "Validation Loss: 3.2161780255181447,AUC: 0.8179683779761905\n",
      "Epoch 20,loss:2.9181998936753524\n",
      "Validation Loss: 3.1367153269904002,AUC: 0.8315446428571428\n",
      "Epoch 25,loss:2.8973023169919063\n",
      "Validation Loss: 3.2227586678096225,AUC: 0.825272693452381\n",
      "Epoch 30,loss:2.8856195713344372\n",
      "Validation Loss: 3.1334604961531505,AUC: 0.8343947172619047\n",
      "Epoch 35,loss:2.883509181047741\n",
      "Validation Loss: 3.2253729615892683,AUC: 0.8244720982142858\n",
      "Epoch 40,loss:2.859028624860864\n",
      "Validation Loss: 3.170728768621172,AUC: 0.8350293898809525\n",
      "Epoch 45,loss:2.868972310894414\n",
      "Validation Loss: 3.1943585872650146,AUC: 0.8367626488095237\n",
      "早停策略触发，停止训练在第 44 个epoch.\n",
      "Test Loss: 3.1937272779403196,AUC: 0.8372430664830841,rec_loss: 0.0\n",
      "i=+3\n",
      "04_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.038114020698949\n",
      "Validation Loss: 3.321194205965315,AUC: 0.8094646577380953\n",
      "Epoch 10,loss:2.9708067429693124\n",
      "Validation Loss: 3.302707178252084,AUC: 0.8143511904761904\n",
      "Epoch 15,loss:2.9293856369821647\n",
      "Validation Loss: 3.2109582083565846,AUC: 0.8272235863095239\n",
      "Epoch 20,loss:2.918587254850488\n",
      "Validation Loss: 3.153375838484083,AUC: 0.828122581845238\n",
      "Epoch 25,loss:2.8918650840458118\n",
      "Validation Loss: 3.1599856189319064,AUC: 0.8262853422619049\n",
      "Epoch 30,loss:2.9043020260961434\n",
      "Validation Loss: 3.2053249308041165,AUC: 0.8293076636904761\n",
      "Epoch 35,loss:2.87529508691085\n",
      "Validation Loss: 3.297444232872554,AUC: 0.8335364583333333\n",
      "早停策略触发，停止训练在第 34 个epoch.\n",
      "Test Loss: 3.295077516186622,AUC: 0.8333486182140047,rec_loss: 0.0\n",
      "i=+4\n",
      "04_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.025456255988071\n",
      "Validation Loss: 3.2862159184047153,AUC: 0.8106285342261905\n",
      "Epoch 10,loss:2.9534191928411784\n",
      "Validation Loss: 3.187002752508436,AUC: 0.8260736607142858\n",
      "Epoch 15,loss:2.9310667859880546\n",
      "Validation Loss: 3.16474255493709,AUC: 0.8296209077380953\n",
      "Epoch 20,loss:2.914259013376738\n",
      "Validation Loss: 3.177073802266802,AUC: 0.8302834821428572\n",
      "Epoch 25,loss:2.909716003819516\n",
      "Validation Loss: 3.1444146718297685,AUC: 0.833364211309524\n",
      "Epoch 30,loss:2.8833478281372473\n",
      "Validation Loss: 3.112284881728036,AUC: 0.8370279017857142\n",
      "Epoch 35,loss:2.8784020072535466\n",
      "Validation Loss: 3.1281421184539795,AUC: 0.8388798363095237\n",
      "Epoch 40,loss:2.8515325502345434\n",
      "Validation Loss: 3.145032380308424,AUC: 0.8361450892857142\n",
      "Epoch 45,loss:2.8408374441297433\n",
      "Validation Loss: 3.134052336215973,AUC: 0.8375613839285714\n",
      "早停策略触发，停止训练在第 44 个epoch.\n",
      "Test Loss: 3.1309810684573267,AUC: 0.8389314516129033,rec_loss: 0.0\n",
      "i=+5\n",
      "04_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.0075449629833826\n",
      "Validation Loss: 3.265153867857797,AUC: 0.8204579613095238\n",
      "Epoch 10,loss:2.9250910595843664\n",
      "Validation Loss: 3.229139106614249,AUC: 0.8249553571428573\n",
      "Epoch 15,loss:2.9042974898689673\n",
      "Validation Loss: 3.159693045275552,AUC: 0.8296674107142857\n",
      "Epoch 20,loss:2.898384953800001\n",
      "Validation Loss: 3.1731098294258118,AUC: 0.8272600446428572\n",
      "Epoch 25,loss:2.8582883069389746\n",
      "Validation Loss: 3.224305876663753,AUC: 0.8198240327380953\n",
      "Epoch 30,loss:2.8227767222806026\n",
      "Validation Loss: 3.1300632783344815,AUC: 0.8378381696428568\n",
      "Epoch 35,loss:2.8549509926846155\n",
      "Validation Loss: 3.1610882026808604,AUC: 0.8350606398809522\n",
      "Epoch 40,loss:2.801686117523595\n",
      "Validation Loss: 3.103421211242676,AUC: 0.8377020089285713\n",
      "Epoch 45,loss:2.7995530711977104\n",
      "Validation Loss: 3.121837147644588,AUC: 0.8345926339285715\n",
      "Epoch 50,loss:2.8105498709176717\n",
      "Validation Loss: 3.126047798565456,AUC: 0.8351841517857143\n",
      "Epoch 55,loss:2.753431771930895\n",
      "Validation Loss: 3.1036692772592818,AUC: 0.839834449404762\n",
      "早停策略触发，停止训练在第 54 个epoch.\n",
      "Test Loss: 3.118221229122531,AUC: 0.840071117722266,rec_loss: 0.0\n",
      "i=+1\n",
      "05_08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.1417333033026718\n",
      "Validation Loss: 3.2583069186056814,AUC: 0.8141105674665617\n",
      "Epoch 10,loss:3.0941470716057755\n",
      "Validation Loss: 3.189238602115262,AUC: 0.8259548337922895\n",
      "Epoch 15,loss:3.07060515299076\n",
      "Validation Loss: 3.1738301323306177,AUC: 0.8276083054681355\n",
      "Epoch 20,loss:3.073069807959766\n",
      "Validation Loss: 3.2096784114837646,AUC: 0.8232039240755312\n",
      "Epoch 25,loss:3.048951681067304\n",
      "Validation Loss: 3.165704596427179,AUC: 0.8309233625098348\n",
      "Epoch 30,loss:3.053507822315867\n",
      "Validation Loss: 3.1333617625697965,AUC: 0.8347395013768686\n",
      "Epoch 35,loss:3.0147304563987545\n",
      "Validation Loss: 3.1086747877059446,AUC: 0.8372156520456332\n",
      "Epoch 40,loss:3.0109884186488824\n",
      "Validation Loss: 3.109907296396071,AUC: 0.8406332366247049\n",
      "Epoch 45,loss:3.024487350045181\n",
      "Validation Loss: 3.11683294850011,AUC: 0.8354953038945714\n",
      "Epoch 50,loss:2.9999197808707634\n",
      "Validation Loss: 3.1490306623520388,AUC: 0.836429976396538\n",
      "早停策略触发，停止训练在第 49 个epoch.\n",
      "Test Loss: 3.140097407733693,AUC: 0.8372063929738564,rec_loss: 0.0\n",
      "i=+2\n",
      "05_08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.148188015309776\n",
      "Validation Loss: 3.2443288987682712,AUC: 0.8155530832022031\n",
      "Epoch 10,loss:3.0580765677661432\n",
      "Validation Loss: 3.136263139786259,AUC: 0.8347004081431946\n",
      "Epoch 15,loss:3.003202208658544\n",
      "Validation Loss: 3.110561386231453,AUC: 0.8371150914634146\n",
      "Epoch 20,loss:2.9716398745048336\n",
      "Validation Loss: 3.1951117284836306,AUC: 0.8263955546813534\n",
      "Epoch 25,loss:2.968943837212353\n",
      "Validation Loss: 3.0571143011892996,AUC: 0.8453206136900079\n",
      "Epoch 30,loss:2.929134790490313\n",
      "Validation Loss: 3.143672458587154,AUC: 0.8363038453973248\n",
      "Epoch 35,loss:2.9255694121849247\n",
      "Validation Loss: 3.0733630503377607,AUC: 0.8437383212037766\n",
      "Epoch 40,loss:2.9187533680985616\n",
      "Validation Loss: 3.1252893324821227,AUC: 0.8429310090479938\n",
      "早停策略触发，停止训练在第 39 个epoch.\n",
      "Test Loss: 3.1145404437009026,AUC: 0.844250335550887,rec_loss: 0.0\n",
      "i=+3\n",
      "05_08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.1338259214308204\n",
      "Validation Loss: 3.2275994746915755,AUC: 0.8190950776947287\n",
      "Epoch 10,loss:3.0667185405405557\n",
      "Validation Loss: 3.2201342351974978,AUC: 0.8276950973642802\n",
      "Epoch 15,loss:3.0190217611266346\n",
      "Validation Loss: 3.150061345869495,AUC: 0.8326851396538159\n",
      "Epoch 20,loss:2.99222022149621\n",
      "Validation Loss: 3.12899887177252,AUC: 0.8383407503933911\n",
      "Epoch 25,loss:2.9759772492618097\n",
      "Validation Loss: 3.100742224724062,AUC: 0.8422383949645948\n",
      "Epoch 30,loss:2.9529674460248247\n",
      "Validation Loss: 3.1155083487110753,AUC: 0.8387653668371361\n",
      "Epoch 35,loss:2.9215504803308625\n",
      "Validation Loss: 3.1040511208195842,AUC: 0.844398480527144\n",
      "Epoch 40,loss:2.9140134381084906\n",
      "Validation Loss: 3.0556158404196463,AUC: 0.8460659667584581\n",
      "Epoch 45,loss:2.898041597226771\n",
      "Validation Loss: 3.04823717763347,AUC: 0.8492660798583792\n",
      "Epoch 50,loss:2.905477805835445\n",
      "Validation Loss: 3.100134426547635,AUC: 0.8435365853658537\n",
      "Epoch 55,loss:2.8939465197121224\n",
      "Validation Loss: 3.0538921202382734,AUC: 0.8504195761211645\n",
      "Epoch 60,loss:2.8886105636271036\n",
      "Validation Loss: 3.102026924010246,AUC: 0.8412699154209282\n",
      "早停策略触发，停止训练在第 59 个epoch.\n",
      "Test Loss: 3.079109149820664,AUC: 0.8427158467553688,rec_loss: 0.0\n",
      "i=+4\n",
      "05_08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.2022629481990164\n",
      "Validation Loss: 3.309274834971274,AUC: 0.8032143735247838\n",
      "Epoch 10,loss:3.1022058899809672\n",
      "Validation Loss: 3.351784844552317,AUC: 0.8108955792682926\n",
      "Epoch 15,loss:3.089089350002568\n",
      "Validation Loss: 3.213211928644488,AUC: 0.823159421715185\n",
      "Epoch 20,loss:3.0726603531255954\n",
      "Validation Loss: 3.1529581546783447,AUC: 0.831661646833202\n",
      "Epoch 25,loss:3.0546892968619743\n",
      "Validation Loss: 3.1367325090592906,AUC: 0.8353607518686074\n",
      "Epoch 30,loss:3.0320656532194556\n",
      "Validation Loss: 3.134968465374362,AUC: 0.8372037273800157\n",
      "Epoch 35,loss:3.04291175632942\n",
      "Validation Loss: 3.137362572454637,AUC: 0.8345050649095201\n",
      "Epoch 40,loss:3.0155556201934814\n",
      "Validation Loss: 3.150039365214686,AUC: 0.8330773013375297\n",
      "Epoch 45,loss:3.0139776875333086\n",
      "Validation Loss: 3.1214681517693306,AUC: 0.8382083497246264\n",
      "Epoch 50,loss:3.003217999528094\n",
      "Validation Loss: 3.1871772735349593,AUC: 0.8368064024390244\n",
      "Epoch 55,loss:2.994724131212002\n",
      "Validation Loss: 3.09471111143789,AUC: 0.8399438188434304\n",
      "Epoch 60,loss:2.9840125950371346\n",
      "Validation Loss: 3.1384767870749197,AUC: 0.8364270874311567\n",
      "Epoch 65,loss:3.000743822353642\n",
      "Validation Loss: 3.1428696109402563,AUC: 0.8363596577498031\n",
      "Epoch 70,loss:2.967222530667375\n",
      "Validation Loss: 3.123464545895976,AUC: 0.8372956825334384\n",
      "早停策略触发，停止训练在第 69 个epoch.\n",
      "Test Loss: 3.1080378434237312,AUC: 0.8402836499183006,rec_loss: 0.0\n",
      "i=+5\n",
      "05_08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,loss:3.1130118457282463\n",
      "Validation Loss: 3.1960640645796254,AUC: 0.8249055861526357\n",
      "Epoch 10,loss:3.041116295791254\n",
      "Validation Loss: 3.12878502568891,AUC: 0.8363181058221874\n",
      "Epoch 15,loss:3.0001608162391475\n",
      "Validation Loss: 3.1472859459538616,AUC: 0.8336591512588511\n",
      "Epoch 20,loss:2.981671696755944\n",
      "Validation Loss: 3.1225612009725263,AUC: 0.8348491591266719\n",
      "Epoch 25,loss:2.96081656944461\n",
      "Validation Loss: 3.0842100420305805,AUC: 0.842974036191975\n"
     ]
    }
   ],
   "source": [
    "import cloudpickle\n",
    "# 创建一个空的DataFrame来存储结果\n",
    "test_auc_df = pd.DataFrame(columns=['时间窗','实验数', '测试集总损失', 'AUC','NDCG@5','HR@5','NDCG@10','HR@10'])\n",
    "for train_user_history_path,val_user_history_path,test_user_history_path,train_neg_comment_path,val_neg_comment_path,test_neg_comment_path,sclar_path in zip(train_user_history_paths,val_user_history_paths,test_user_history_paths,train_neg_comment_paths,val_neg_comment_paths,test_neg_comment_paths,sclar_paths):\n",
    "    \n",
    "    for n in range(5):\n",
    "        print(f'i=+{n+1}')\n",
    "\n",
    "        # 假设文件名总是有相同的结构，可以根据固定的位置进行切片\n",
    "        start_index = train_user_history_path.find('2018') + 4\n",
    "        end_index = start_index + 5\n",
    "        substring = train_user_history_path[start_index:end_index]\n",
    "        print(substring)\n",
    "\n",
    "        # 设置 CSV 文件路径\n",
    "        csv_path = 'new_result//new_REMI-allfeature.csv'\n",
    "\n",
    "        # 检查文件是否存在，如果存在则不写入表头\n",
    "        try:\n",
    "            with open(csv_path, 'r') as f:\n",
    "                header_exists = True\n",
    "        except FileNotFoundError:\n",
    "            header_exists = False\n",
    "\n",
    "\n",
    "        house_index,house_continuous_matrix,house_categorical_matrix1,house_categorical_matrix2,house_categorical_encodings,house_multi_matrix,num_house_categories_list2=read_house_host(house_path,house_id_column,'house',house_continuous_column,house_categorical_column1,house_categorical_column2)\n",
    "        host_index,host_continuous_matrix,host_categorical_matrix1,host_categorical_matrix2,host_categorical_encodings,num_host_categories_list2=read_house_host(host_path,host_id_column,'host',host_continuous_column,host_categorical_column1,host_categorical_column2)\n",
    "\n",
    "        #读取host图片特征\n",
    "        host_pic_index, host_pic_continuous_matrix, host_pic_continuous_column = read_host_pic_features(host_pic_path)\n",
    "\n",
    "        #读取正样本house和host动态特征\n",
    "        comment_index,house_comment_matrix,host_comment_matrix,comment_house,comment_host=read_comment_features(comment_path,comment_id_column,house_comment_continuous_column,host_comment_continuous_column)\n",
    "\n",
    "\n",
    "        #获取所有用户历史记录对应的house、host索引\n",
    "        train_user_index, train_history_matrix_comment, train_history_matrix_host, train_history_matrix_host_pic, train_history_matrix_house, train_history_matrix_mask = user_history(train_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        val_user_index, val_history_matrix_comment, val_history_matrix_host, val_history_matrix_host_pic, val_history_matrix_house, val_history_matrix_mask = user_history(val_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        test_user_index, test_history_matrix_comment, test_history_matrix_host, test_history_matrix_host_pic, test_history_matrix_house, test_history_matrix_mask = user_history(test_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "\n",
    "\n",
    "        house_scaler=creat_scaler(sclar_path,house_continuous_column)\n",
    "        host_scaler=creat_scaler(sclar_path,host_continuous_column)\n",
    "        house_comment_scaler=creat_scaler(sclar_path,house_comment_continuous_column)\n",
    "        host_comment_scaler=creat_scaler(sclar_path,host_comment_continuous_column)\n",
    "        host_pic_scaler=creat_scaler(sclar_path,host_pic_continuous_column)\n",
    "\n",
    "        #连续特征标准化\n",
    "        house_continuous_matrix = house_scaler.transform(house_continuous_matrix)\n",
    "        host_continuous_matrix = host_scaler.transform(host_continuous_matrix)\n",
    "        house_comment_matrix = house_comment_scaler.transform(house_comment_matrix)\n",
    "        #     neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        host_comment_matrix = host_comment_scaler.transform(host_comment_matrix)\n",
    "        #     neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        host_pic_continuous_matrix = host_pic_scaler.transform(host_pic_continuous_matrix)\n",
    "\n",
    "        #特征矩阵转换成tensor\n",
    "        house_continuous_tensor=torch.tensor(house_continuous_matrix).detach()\n",
    "        house_categorical_tensor1=torch.tensor(house_categorical_matrix1).detach()\n",
    "        house_categorical_tensor2=torch.tensor(house_categorical_matrix2).detach()\n",
    "        house_multi_tensor=torch.tensor(house_multi_matrix).detach()\n",
    "        house_comment_tensor=torch.tensor(house_comment_matrix).detach()\n",
    "        #     neg_house_comment_tensor=torch.tensor(neg_house_comment_matrix).clone().detach()\n",
    "\n",
    "        host_continuous_tensor=torch.tensor(host_continuous_matrix).detach()\n",
    "        host_categorical_tensor1=torch.tensor(host_categorical_matrix1).detach()\n",
    "        host_categorical_tensor2=torch.tensor(host_categorical_matrix2).detach()\n",
    "        host_comment_tensor=torch.tensor(host_comment_matrix).detach()\n",
    "        #     neg_host_comment_tensor=torch.tensor(neg_host_comment_matrix).clone().detach()\n",
    "        host_pic_continuous_tensor=torch.tensor(host_pic_continuous_matrix).detach()\n",
    "\n",
    "        #训练集输入特征编号、label.负样本动态特征（标准化后的tensor）\n",
    "        train_pos_comment_id,train_house,train_host,train_host_pic,train_reviewer,train_label,train_house_comment_features,train_host_comment_features=get_train_input_index(train_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,train_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        val_pos_comment_id,val_house,val_host,val_host_pic,val_reviewer,val_label,val_house_comment_features,val_host_comment_features=get_train_input_index(val_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,val_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        test_pos_comment_id,test_house,test_host,test_host_pic,test_reviewer,test_label,test_house_comment_features,test_host_comment_features=get_train_input_index(test_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,test_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "\n",
    "        #获得训练集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_house)\n",
    "        #获得验证集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        val_house_continuous_features,val_house_categorical_features1,val_house_categorical_feature2,val_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_house)\n",
    "        #获得测试集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        test_house_continuous_features,test_house_categorical_features1,test_house_categorical_feature2,test_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_house)\n",
    "\n",
    "\n",
    "        #获得训练集输入的host特征\n",
    "        train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_host,train_host_pic)\n",
    "        #获得验证集输入的host特征\n",
    "        val_host_continuous_features,val_host_categorical_features1,val_host_categorical_features2,val_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_host,val_host_pic)\n",
    "        #获得测试集输入的host特征\n",
    "        test_host_continuous_features,test_host_categorical_features1,test_host_categorical_features2,test_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_host,test_host_pic)\n",
    "\n",
    "        #user历史记录矩阵转换成tensor\n",
    "        train_history_matrix_comment=torch.tensor(train_history_matrix_comment).detach()\n",
    "        train_history_matrix_mask=torch.tensor(train_history_matrix_mask).detach()\n",
    "        train_history_matrix_host=torch.tensor(train_history_matrix_host).detach()\n",
    "        train_history_matrix_host_pic=torch.tensor(train_history_matrix_host_pic).detach()\n",
    "        train_history_matrix_house=torch.tensor(train_history_matrix_house).detach()\n",
    "\n",
    "        val_history_matrix_comment=torch.tensor(val_history_matrix_comment).detach()\n",
    "        val_history_matrix_mask=torch.tensor(val_history_matrix_mask).detach()\n",
    "        val_history_matrix_host=torch.tensor(val_history_matrix_host).detach()\n",
    "        val_history_matrix_host_pic=torch.tensor(val_history_matrix_host_pic).detach()\n",
    "        val_history_matrix_house=torch.tensor(val_history_matrix_house).detach()\n",
    "\n",
    "        test_history_matrix_comment=torch.tensor(test_history_matrix_comment).detach()\n",
    "        test_history_matrix_mask=torch.tensor(test_history_matrix_mask).detach()\n",
    "        test_history_matrix_host=torch.tensor(test_history_matrix_host).detach()\n",
    "        test_history_matrix_host_pic=torch.tensor(test_history_matrix_host_pic).detach()\n",
    "        test_history_matrix_house=torch.tensor(test_history_matrix_house).detach()\n",
    "\n",
    "        #得到训练集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        train_history_comment,train_history_host,train_history_host_pic,train_history_house,train_history_mask=get_user_input_index(train_history_matrix_comment,train_history_matrix_host,train_history_matrix_host_pic,train_history_matrix_house,train_history_matrix_mask,train_reviewer)\n",
    "        #得到验证集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        val_history_comment,val_history_host,val_history_host_pic,val_history_house,val_history_mask=get_user_input_index(val_history_matrix_comment,val_history_matrix_host,val_history_matrix_host_pic,val_history_matrix_house,val_history_matrix_mask,val_reviewer)\n",
    "        #得到测试集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        test_history_comment,test_history_host,test_history_host_pic,test_history_house,test_history_mask=get_user_input_index(test_history_matrix_comment,test_history_matrix_host,test_history_matrix_host_pic,test_history_matrix_house,test_history_matrix_mask,test_reviewer)\n",
    "\n",
    "        #获得训练集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_history_house)\n",
    "        #获得验证集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_house_continuous_features,val_user_house_categorical_features1,val_user_house_categorical_feature2,val_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_history_house)\n",
    "        #获得测试集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_house_continuous_features,test_user_house_categorical_features1,test_user_house_categorical_feature2,test_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_history_house)\n",
    "\n",
    "        #获得训练集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_history_host,train_history_host_pic)\n",
    "        #获得验证集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_host_continuous_features,val_user_host_categorical_features1,val_user_host_categorical_features2,val_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_history_host,val_history_host_pic)\n",
    "        #获得测试集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_host_continuous_features,test_user_host_categorical_features1,test_user_host_categorical_features2,test_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_history_host,test_history_host_pic)\n",
    "\n",
    "        #获得训练集输入user历史记录的comment特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_comment_features,train_user_host_comment_features=get_history_comment_input(train_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        val_user_house_comment_features,val_user_host_comment_features=get_history_comment_input(val_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        test_user_house_comment_features,test_user_host_comment_features=get_history_comment_input(test_history_comment,house_comment_tensor,host_comment_tensor)    \n",
    "\n",
    "\n",
    "        num_house_categorical2_features=house_categorical_matrix2.shape[1]\n",
    "        num_host_categorical2_features=host_categorical_matrix2.shape[1]\n",
    "        num_house_categorical1_features=house_categorical_matrix1.shape[1]\n",
    "        num_host_categorical1_features=host_categorical_matrix1.shape[1]\n",
    "        num_house_multi_features=house_multi_matrix.shape[1]\n",
    "        num_house_continuous_features=house_continuous_matrix.shape[1]\n",
    "        num_host_continuous_features=host_continuous_matrix.shape[1]   \n",
    "        num_house_comment_features=house_comment_matrix.shape[1]\n",
    "        num_host_comment_features=host_comment_matrix.shape[1]\n",
    "        num_host_pic_continuous_features=host_pic_continuous_matrix.shape[1]\n",
    "\n",
    "\n",
    "        train_modals= f'model//new_REMI-allfeature+{substring}+{n}.pth'\n",
    "        #训练集\n",
    "        train_dataset = TensorDataset(train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features,train_house_comment_features,\n",
    "            train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_comment_features,train_host_pic_features,\n",
    "            train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features,train_user_house_comment_features,\n",
    "            train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_comment_features,train_user_host_pic_features,\n",
    "            train_label,train_history_mask)\n",
    "        val_dataset = TensorDataset(val_house_continuous_features, val_house_categorical_features1, val_house_categorical_feature2, val_house_multi_features, val_house_comment_features,\n",
    "            val_host_continuous_features, val_host_categorical_features1, val_host_categorical_features2, val_host_comment_features, val_host_pic_features,\n",
    "            val_user_house_continuous_features, val_user_house_categorical_features1, val_user_house_categorical_feature2, val_user_house_multi_features, val_user_house_comment_features,\n",
    "            val_user_host_continuous_features, val_user_host_categorical_features1, val_user_host_categorical_features2, val_user_host_comment_features, val_user_host_pic_features,\n",
    "            val_label, val_history_mask)\n",
    "\n",
    "\n",
    "        # 创建数据加载器\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        # 确保您的计算机上有CUDA支持的GPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # 创建大模型的实例\n",
    "        model = MatchingModel(num_house_categorical1_features,num_host_categorical1_features,num_house_categorical2_features,num_host_categorical2_features,num_house_multi_features,multi_embedding_dim,\n",
    "                     num_house_categories_list2,num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                     num_host_pic_continuous_features,num_house_comment_features,feature_dim,num_heads,max_history_len,\n",
    "                     num_experts, num_tasks, expert_hidden_units, gate_hidden_units,user_dnn_hidden_units,house_dnn_hidden_units,attetion_dnn_hidden_units,device)\n",
    "\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        #进一步处理 列表转移到GPU\n",
    "        for i in range(len(model.house_embedding.house_categorical1_embeddings)):\n",
    "            model.house_embedding.house_categorical1_embeddings[i] = model.house_embedding.house_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_categorical2_embeddings)):\n",
    "            model.house_embedding.house_categorical2_embeddings[i] = model.house_embedding.house_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical1_embeddings)):\n",
    "            model.house_embedding.host_categorical1_embeddings[i] = model.house_embedding.host_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical2_embeddings)):\n",
    "            model.house_embedding.host_categorical2_embeddings[i] = model.house_embedding.host_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.multi_embedding_layer)):\n",
    "            model.house_embedding.multi_embedding_layer[i] = model.house_embedding.multi_embedding_layer[i].to(device)\n",
    "\n",
    "        for i in range(len(model.house_embedding.house_continuous_embedding_layer)):\n",
    "            model.house_embedding.house_continuous_embedding_layer[i] = model.house_embedding.house_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_comment_embedding_layer)):\n",
    "            model.house_embedding.house_comment_embedding_layer[i] = model.house_embedding.house_comment_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_continuous_embedding_layer)):\n",
    "            model.house_embedding.host_continuous_embedding_layer[i] = model.house_embedding.host_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_pic_embedding_layer)):\n",
    "            model.house_embedding.host_pic_embedding_layer[i] = model.house_embedding.host_pic_embedding_layer[i].to(device)\n",
    "\n",
    "\n",
    "        rec_criterion = nn.BCELoss(size_average=True, reduce=True)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        #训练\n",
    "        model_training(model,train_loader,val_loader,rec_criterion,optimizer,500,device)\n",
    "        with open(train_modals, 'wb') as f:\n",
    "            cloudpickle.dump(model, f)     \n",
    "        #测试\n",
    "        test_reviewer=torch.tensor(test_reviewer).detach()\n",
    "        test_host=torch.tensor(test_host).detach()\n",
    "        test_house=torch.tensor(test_house).detach()\n",
    "        test_pos_comment_id=torch.tensor(test_pos_comment_id, dtype=torch.int64).detach()\n",
    "        test_dataset = TensorDataset(test_pos_comment_id,test_house,test_host,test_reviewer,test_house_continuous_features, test_house_categorical_features1, test_house_categorical_feature2, test_house_multi_features, test_house_comment_features,\n",
    "            test_host_continuous_features, test_host_categorical_features1, test_host_categorical_features2, test_host_comment_features, test_host_pic_features,\n",
    "            test_user_house_continuous_features, test_user_house_categorical_features1, test_user_house_categorical_feature2, test_user_house_multi_features, test_user_house_comment_features,\n",
    "            test_user_host_continuous_features, test_user_host_categorical_features1, test_user_host_categorical_features2, test_user_host_comment_features, test_user_host_pic_features,\n",
    "            test_label, test_history_mask)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        df_results,average_test_loss,average_auc_test=test_model(model, test_loader)\n",
    "        #测试的每个样本结果保存到csv\n",
    "    #     file_name = f\"result/REMI{n}.csv\"\n",
    "    #     df_results.to_csv(file_name, index=False)\n",
    "\n",
    "        top_5_ndcg,top_5_hr=top_evaluation(df_results,5)\n",
    "        top_10_ndcg,top_10_hr=top_evaluation(df_results,10)   \n",
    "\n",
    "        new_row = {\n",
    "            '实验数': n, \n",
    "            '测试集总损失': average_test_loss, \n",
    "            'AUC': average_auc_test, \n",
    "            'NDCG@5': top_5_ndcg, \n",
    "            'HR@5': top_5_hr, \n",
    "            'NDCG@10': top_10_ndcg, \n",
    "            'HR@10': top_10_hr\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        # 将新的行追加到 CSV 文件中\n",
    "        pd.DataFrame([new_row]).to_csv(csv_path, mode='a', index=False, header=not header_exists)\n",
    "        header_exists = True  # 确保后续追加时不再写入表头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f534554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47047c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db239c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0adbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
