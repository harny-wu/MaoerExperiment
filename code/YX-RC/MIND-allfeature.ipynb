{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4475629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:29.213856Z",
     "start_time": "2024-09-04T06:31:27.984530Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "house_path='dataset//selected_feature_num_can_house_distinct__201901_202103.csv'\n",
    "host_path='dataset//selected_feature_num_can_host_distinct_201901_202103.csv'\n",
    "host_pic_path='dataset//feature_img_can_host_distinct_201901_202103.csv'\n",
    "comment_path='new_dataset//selected_feature_can_comment_basic_201901_202103.csv'\n",
    "\n",
    "\n",
    "house_id_column='HOUSE_ID'\n",
    "host_id_column='host_id'\n",
    "host_pic_id_column='host_id'\n",
    "comment_id_column='comment_id'\n",
    "neg_comment_id_column='pos_comment_id'\n",
    "house_continuous_column=['HOUSE_PRICE_DAY','HOUSE_BATHROOM_NUM', 'HOUSE_BEDROOM_NUM', 'HOUSE_BED_NUM', 'HOUSE_DESCRIPTION_LEN', 'HOUSE_ACCOMMODATES',\n",
    "         'HOUSE_NAME_LEN', 'HOUSE_NEIGHBOUR_OVERVIEW_LEN','HOUSE_REVIEW_SCORE_CLEAN',\n",
    "         'HOUSE_REVIEW_SCORE_ACCURACY', 'HOUSE_REVIEW_SCORE_CHECKIN', 'HOUSE_REVIEW_SCORE_COMMUNICATION',\n",
    "         'HOUSE_REVIEW_SCORE_VALUE', 'HOUSE_REVIEW_SCORE_LOCATION','HOUSE_REVIEW_SCORE_RATE']\n",
    "\n",
    "host_continuous_column=['HOST_ABOUT_LEN', 'HOST_ACCEPTANCE_RATE', 'HOST_HOUSE_NUM', 'HOST_NAME_LEN', 'HOST_RESPONSE_RATE',\n",
    "         'HOST_VERIFICATION_NUM', 'info_amount', 'readability', 'user_about_t1', 'user_about_t3', 'user_about_t4',\n",
    "         'user_about_t2','HOST_HOUSE_REVIEW_SCORE_COMMUNICAT_AVG']\n",
    "\n",
    "\n",
    "\n",
    "house_comment_continuous_column=['house_tran_review_num','house_tran_reviewer_num', 'house_tran_review_1y_num',\n",
    "                                 'house_tran_review_30d_num',  'house_tran_comment_subjectivity_score_avg',\n",
    "                                 'house_tran_comment_plarity_score_avg', 'house_tran_review_theme_RoomService_avg',\n",
    "                                 'house_tran_review_theme_IndoorEnvironment_avg', 'house_tran_review_theme_NeighborFacilities_avg',\n",
    "                                 'house_tran_review_Transportation_avg', 'house_tran_review_BookingRelus_avg',\n",
    "                                 'house_tran_review_TouristScenery_avg','house_tran_review_HostServices_avg',\n",
    "                                 'house_tran_review_theme_HouseFacilities_avg', 'house_tran_review_theme_AccommodExperience_avg',\n",
    "                                 'house_tran_review_pos_num', 'house_tran_review_neg_num',\n",
    "                                 'interval_house_first_review', 'interval_house_previous_review']\n",
    "\n",
    "host_comment_continuous_column=['host_tran_review_num']\n",
    "\n",
    "house_categorical_column1=['HOUSE_IS_INSTANT_BOOKABLE']\n",
    "host_categorical_column1=['HOST_HAS_PROFILE_PIC', 'HOST_IS_IDENTITY_VERIFIED', 'HOST_IS_SUPERHOST', 'HOST_NAME_HAS_DIG',\n",
    "         'HOST_NAME_HAS_ENGLISH']\n",
    "house_categorical_column2=['HOUSE_PROPERTY_TYPE', 'HOUSE_ROOM_TYPE']\n",
    "host_categorical_column2=['HOST_RESPONSE_TIME']\n",
    "\n",
    "\n",
    "\n",
    "npratio=50 #负样本数\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "num_heads=8\n",
    "\n",
    "feature_dim=128\n",
    "max_history_len=15\n",
    "num_experts=3\n",
    "num_tasks=2\n",
    "multi_embedding_dim = 18  # 设置嵌入维度\n",
    "expert_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "gate_hidden_units=[[feature_dim,feature_dim//2],[feature_dim//2,num_experts]]\n",
    "classifier_dnn_hidden_units=[[feature_dim,feature_dim*2],[feature_dim*2,feature_dim]]\n",
    "weight_dnn_hidden_units=[[feature_dim*3,feature_dim],[feature_dim,2]]\n",
    "user_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "house_dnn_hidden_units=[[feature_dim*2,feature_dim]]\n",
    "attetion_dnn_hidden_units=[[4*feature_dim,2*feature_dim],[2*feature_dim,feature_dim]]\n",
    "lr=0.0001\n",
    "batch_size = 128\n",
    "k_max=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f4189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:29.229234Z",
     "start_time": "2024-09-04T06:31:29.221762Z"
    }
   },
   "outputs": [],
   "source": [
    "train_user_history_path1 = 'new_dataset2//can_history_201801.csv'\n",
    "val_user_history_path1 = 'new_dataset2//can_history_201805.csv'\n",
    "test_user_history_path1 = 'new_dataset2//can_history_201806.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path1='new_dataset2//new_can_comment_neg_201801_04_50neg.csv'\n",
    "val_neg_comment_path1='new_dataset2//new_can_comment_neg_201805_50neg.csv'\n",
    "test_neg_comment_path1='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "\n",
    "sclar_path1='new_dataset/all_feature+label_201801_04_50neg.csv'\n",
    "\n",
    "\n",
    "train_user_history_path2 = 'new_dataset2//can_history_201802_05.csv'\n",
    "val_user_history_path2 = 'new_dataset2//can_history_201806.csv'\n",
    "test_user_history_path2 = 'new_dataset2//can_history_201807.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path2='new_dataset2//new_can_comment_neg_201802_05_50neg.csv'\n",
    "val_neg_comment_path2='new_dataset2//new_can_comment_neg_201806_50neg.csv'\n",
    "test_neg_comment_path2='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "\n",
    "sclar_path2='new_dataset2/all_feature+label_201802_05_50neg.csv'\n",
    "\n",
    "train_user_history_path3 = 'new_dataset2//can_history_201803_06.csv'\n",
    "val_user_history_path3 = 'new_dataset2//can_history_201807.csv'\n",
    "test_user_history_path3 = 'new_dataset2//can_history_201808.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path3='new_dataset2//new_can_comment_neg_201803_06_50neg.csv'\n",
    "val_neg_comment_path3='new_dataset2//new_can_comment_neg_201807_50neg.csv'\n",
    "test_neg_comment_path3='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "\n",
    "sclar_path3='new_dataset2/all_feature+label_201803_06_50neg.csv'\n",
    "\n",
    "train_user_history_path4 = 'new_dataset2//can_history_201804_07.csv'\n",
    "val_user_history_path4 = 'new_dataset2//can_history_201808.csv'\n",
    "test_user_history_path4 = 'new_dataset2//can_history_201809.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path4='new_dataset2//new_can_comment_neg_201804_07_50neg.csv'\n",
    "val_neg_comment_path4='new_dataset2//new_can_comment_neg_201808_50neg.csv'\n",
    "test_neg_comment_path4='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "\n",
    "sclar_path4='new_dataset2/all_feature+label_201804_07_50neg.csv'\n",
    "\n",
    "train_user_history_path5 = 'new_dataset2//can_history_201805_08.csv'\n",
    "val_user_history_path5 = 'new_dataset2//can_history_201809.csv'\n",
    "test_user_history_path5 = 'new_dataset2//can_history_201810.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path5='new_dataset2//new_can_comment_neg_201805_08_50neg.csv'\n",
    "val_neg_comment_path5='new_dataset2//new_can_comment_neg_201809_50neg.csv'\n",
    "test_neg_comment_path5='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "\n",
    "sclar_path5='new_dataset2/all_feature+label_201805_08_50neg.csv'\n",
    "\n",
    "train_user_history_path6 = 'new_dataset2//can_history_201806_09.csv'\n",
    "val_user_history_path6 = 'new_dataset2//can_history_201810.csv'\n",
    "test_user_history_path6 = 'new_dataset2//can_history_201811.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path6='new_dataset2//new_can_comment_neg_201806_09_50neg.csv'\n",
    "val_neg_comment_path6='new_dataset2//new_can_comment_neg_201810_50neg.csv'\n",
    "test_neg_comment_path6='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "\n",
    "sclar_path6='new_dataset2/all_feature+label_201806_09_50neg.csv'\n",
    "\n",
    "train_user_history_path7 = 'new_dataset2//can_history_201807_10.csv'\n",
    "val_user_history_path7 = 'new_dataset2//can_history_201811.csv'\n",
    "test_user_history_path7 = 'new_dataset2//can_history_201812.csv'\n",
    "\n",
    "\n",
    "train_neg_comment_path7='new_dataset2//new_can_comment_neg_201807_10_50neg.csv'\n",
    "val_neg_comment_path7='new_dataset2//new_can_comment_neg_201811_50neg.csv'\n",
    "test_neg_comment_path7='new_dataset2//new_can_comment_neg_201812_50neg.csv'\n",
    "\n",
    "sclar_path7='new_dataset2/all_feature+label_201807_10_50neg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5c16c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:29.636936Z",
     "start_time": "2024-09-04T06:31:29.619649Z"
    }
   },
   "outputs": [],
   "source": [
    "train_user_history_paths=[train_user_history_path1,train_user_history_path2,train_user_history_path3,train_user_history_path4,train_user_history_path5,train_user_history_path6,train_user_history_path7]\n",
    "val_user_history_paths=[val_user_history_path1,val_user_history_path2,val_user_history_path3,val_user_history_path4,val_user_history_path5,val_user_history_path6,val_user_history_path7]\n",
    "test_user_history_paths=[test_user_history_path1,test_user_history_path2,test_user_history_path3,test_user_history_path4,test_user_history_path5,test_user_history_path6,test_user_history_path7]\n",
    "\n",
    "train_neg_comment_paths=[train_neg_comment_path1,train_neg_comment_path2,train_neg_comment_path3,train_neg_comment_path4,train_neg_comment_path5,train_neg_comment_path6,train_neg_comment_path7]\n",
    "val_neg_comment_paths=[val_neg_comment_path1,val_neg_comment_path2,val_neg_comment_path3,val_neg_comment_path4,val_neg_comment_path5,val_neg_comment_path6,val_neg_comment_path7]\n",
    "test_neg_comment_paths=[test_neg_comment_path1,test_neg_comment_path2,test_neg_comment_path3,test_neg_comment_path4,test_neg_comment_path5,test_neg_comment_path6,test_neg_comment_path7]\n",
    "\n",
    "sclar_paths=[sclar_path1,sclar_path2,sclar_path3,sclar_path4,sclar_path5,sclar_path6,sclar_path7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f6e9d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:29.842108Z",
     "start_time": "2024-09-04T06:31:29.823431Z"
    }
   },
   "outputs": [],
   "source": [
    "#读取house或host的静态特征\n",
    "def read_house_host(path, id_column, object, continuous_column, categorical_column1, categorical_column2):\n",
    "    # 从CSV文件中读取房屋信息\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "\n",
    "    # 提取连续特征和类别特征,multi-hot特征\n",
    "    continuous_features = data[continuous_column]\n",
    "    categorical_features1 = data[categorical_column1]  # 类别特征列\n",
    "    categorical_features2 = data[categorical_column2]\n",
    "\n",
    "    # 使用LabelEncoder将类别特征转换为编号\n",
    "    label_encoders = {}  # 存储特征列对应的编码器对象\n",
    "    categorical_encodings = {}  # 存储特征值对应的编号\n",
    "    for col in categorical_features2.columns:\n",
    "        le = LabelEncoder()\n",
    "        categorical_features2.loc[:, col] = le.fit_transform(categorical_features2[col])\n",
    "        label_encoders[col] = le  #储存到编码器对象字典中\n",
    "        categorical_encodings[col] = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    # 得到每个分类特征的类别数\n",
    "    num_categories_list2 = [len(le.classes_) for le in label_encoders.values()]\n",
    "      \n",
    "    \n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        multi_features = data.iloc[:, 10:52]\n",
    "        #补缺失值，-2\n",
    "        multi_features=multi_features.fillna(2)\n",
    "        house_multi_matrix = np.array(multi_features)\n",
    "        # 创建一个与最后一行维度相同的 NaN 数组\n",
    "        nan_row = np.full((1, house_multi_matrix.shape[1]),2)\n",
    "        # 将 NaN 数组插入到 house_multi_matrix 的最后一行\n",
    "        house_multi_matrix = np.concatenate((house_multi_matrix, nan_row), axis=0)\n",
    "    else:\n",
    "        # 将连续特征中的百分比特征转换成数值类型\n",
    "        # 找到包含百分数的列\n",
    "        percent_columns = ['HOST_ACCEPTANCE_RATE','HOST_RESPONSE_RATE']  \n",
    "        # 将百分数转换为浮点数\n",
    "        for column in percent_columns:\n",
    "            continuous_features[column] = continuous_features[column].str.rstrip('%').astype(float) / 100.0\n",
    "\n",
    "                        \n",
    "    #补缺失值，连续值补-2，分类1特征补2，分类2特征不用补\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "    categorical_features1=categorical_features1.fillna(2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "    categorical_matrix1 = np.array(categorical_features1).astype(int)\n",
    "    categorical_matrix2 = np.array(categorical_features2)\n",
    "\n",
    "    # 添加一行-2\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    continuous_matrix = np.vstack((continuous_matrix, nan_row))\n",
    "    # 计算每一列的最大值加1\n",
    "    new_row1 = np.full(categorical_matrix1.shape[1], 2)\n",
    "    max_values2 = np.max(categorical_matrix2, axis=0) + 1\n",
    "    # 将最大值加1的行添加到categorical_matrix1\n",
    "    categorical_matrix1 = np.vstack((categorical_matrix1,new_row1))\n",
    "    categorical_matrix2 = np.vstack((categorical_matrix2, max_values2))\n",
    "\n",
    "    \n",
    "\n",
    "    if object == 'house':  # house处理多热特征，host处理百分比连续特征\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, house_multi_matrix, num_categories_list2\n",
    "    else:\n",
    "        return index, continuous_matrix, categorical_matrix1, categorical_matrix2, categorical_encodings, num_categories_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8d8d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:29.992595Z",
     "start_time": "2024-09-04T06:31:29.973082Z"
    }
   },
   "outputs": [],
   "source": [
    "#读取host图片特征\n",
    "def read_host_pic_features(path):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data['host_id'].unique())}\n",
    "    # 提取连续特征\n",
    "    continuous_features =  data.iloc[:, 5:]\n",
    "    host_pic_continuous_column = data.iloc[:, 5:].columns.tolist()\n",
    "    \n",
    "    #补缺失值-2\n",
    "    continuous_features=continuous_features.fillna(-2)\n",
    "        \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    continuous_matrix = np.array(continuous_features)\n",
    "\n",
    "    #当host没有pic特征时，取最后一行，特征为-2\n",
    "    # 创建全为nan的一行\n",
    "    nan_row = np.full((1, continuous_matrix.shape[1]),-2)\n",
    "    # 在continuous_matrix最后添加zero_row\n",
    "    continuous_matrix = np.vstack((continuous_matrix,nan_row))\n",
    "\n",
    "    return index,continuous_matrix,host_pic_continuous_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6058803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.127001Z",
     "start_time": "2024-09-04T06:31:30.111381Z"
    }
   },
   "outputs": [],
   "source": [
    "#读取house和host以及负样本动态特征\n",
    "def read_comment_features(path,id_column,house_continuous_column,host_continuous_column):\n",
    "    data = pd.read_csv(path)\n",
    "    # 创建重新编号的字典\n",
    "    index = {id: new_id for new_id, id in enumerate(data[id_column].unique())}\n",
    "    # 提取连续特征\n",
    "    house_continuous_features = data[house_continuous_column]\n",
    "    host_continuous_features = data[host_continuous_column]\n",
    "    #补缺失值-2\n",
    "    house_continuous_features=house_continuous_features.fillna(-2)\n",
    "    host_continuous_features=host_continuous_features.fillna(-2)\n",
    "    \n",
    "    # 将特征存放到矩阵中，每一列代表一个特征，特征值存放在对应的house编号索引中\n",
    "    house_continuous_matrix = np.array(house_continuous_features)\n",
    "    host_continuous_matrix = np.array(host_continuous_features)\n",
    "    \n",
    "    # 创建一个全为 -2 的行\n",
    "    nan_row1 = np.full((1, house_continuous_matrix.shape[1]), -2)\n",
    "    nan_row2 = np.full((1, host_continuous_matrix.shape[1]), -2)\n",
    "    # 将 nan_row 插入到 host_continuous_matrix 的最后一行\n",
    "    house_continuous_matrix = np.vstack((house_continuous_matrix, nan_row1))\n",
    "    host_continuous_matrix = np.vstack((host_continuous_matrix, nan_row2))\n",
    "\n",
    "    #提取对应的house_id和host_id\n",
    "    comment_house=np.array(data['listing_id'])\n",
    "    comment_host = np.array(data['host_id'])\n",
    "    #最后一行为-1，comment_id不存在时，house_id,host_id为-1\n",
    "    comment_house = np.append(comment_house, -1)\n",
    "    comment_host = np.append(comment_host, -1)\n",
    "\n",
    "    return index, house_continuous_matrix, host_continuous_matrix, comment_house,comment_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ab4239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.274020Z",
     "start_time": "2024-09-04T06:31:30.250857Z"
    }
   },
   "outputs": [],
   "source": [
    "# 转换 history 列为长度为15的数组\n",
    "def process_history(history,max_history_len):\n",
    "    if len(history) >= max_history_len:\n",
    "        processed_history = history[-max_history_len:]\n",
    "    else:\n",
    "        processed_history = [-1] * (max_history_len - len(history)) + history\n",
    "    return processed_history\n",
    "\n",
    "\n",
    "# 将填充-1的位置标记为True\n",
    "def create_mask(history):\n",
    "    mask = [True if item == -1 else False for item in history]\n",
    "    return mask\n",
    "\n",
    "#获取所有用户历史记录对应的house、host索引\n",
    "def user_history(path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len):\n",
    "    data = pd.read_csv(path)\n",
    "    # 将history列的数据转换为列表格式\n",
    "    data['history'] = data['history'].apply(lambda x: [int(item) for item in x.strip('[]').split()])\n",
    "    # 获取唯一的 reviewer_id\n",
    "    unique_reviewer_ids = data['reviewer_id'].unique()\n",
    "\n",
    "    # 创建 reviewer_id 到编号的映射字典\n",
    "    user_index = {reviewer_id: idx for idx, reviewer_id in enumerate(unique_reviewer_ids)}\n",
    "    # 应用处理函数到每个 history\n",
    "    data['processed_history'] = data['history'].apply(lambda x: process_history(x, max_history_len))  # shape（user_num,history_num）,comment_id,填充为-1\n",
    "    # 生成整个 history 的矩阵\n",
    "    history_matrix = np.array(data['processed_history'].tolist())\n",
    "\n",
    "    # 将 history_matrix 中的 comment_id 转换为 host_id，不存在的设为 -1\n",
    "    history_matrix_host = []\n",
    "    history_matrix_house = []\n",
    "    history_matrix_host_pic = []\n",
    "    history_matrix_mask = []\n",
    "    history_matrix_comment = []\n",
    "    for user_history in history_matrix:\n",
    "        user_history_host = []\n",
    "        user_history_host_pic = []\n",
    "        user_history_house = []\n",
    "        user_history_mask = []\n",
    "        user_history_comment = []\n",
    "\n",
    "        for comment_id in user_history:\n",
    "            user_history_comment.append(comment_index.get(comment_id, -1))  # comment编号，填充的comment编号为-1\n",
    "            user_history_host_id = comment_host[comment_index.get(comment_id, -1)]  # host_id\n",
    "            user_history_house_id = comment_house[comment_index.get(comment_id, -1)]  # house_id\n",
    "            user_history_host.append(host_index.get(user_history_host_id, -1))  # host表编号\n",
    "            user_history_host_pic.append(host_pic_index.get(user_history_host_id, -1))  # host_pic表编号\n",
    "            user_history_house.append(house_index.get(user_history_house_id, -1))  # house表编号\n",
    "\n",
    "        history_matrix_comment.append(user_history_comment)  # shape（user_num,history_num）,comment编号\n",
    "        history_matrix_mask.append(create_mask(user_history))# 在填充的位置标记true\n",
    "        history_matrix_host.append(user_history_host)  # host表编号,shape（user_num,history_num）\n",
    "        history_matrix_host_pic.append(user_history_host_pic)  # host_pic表编号\n",
    "        history_matrix_house.append(user_history_house)  # house表编号  shape（user_num,history_num）\n",
    "\n",
    "    return user_index, history_matrix_comment, history_matrix_host, history_matrix_host_pic, history_matrix_house, history_matrix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce227f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.400088Z",
     "start_time": "2024-09-04T06:31:30.387978Z"
    }
   },
   "outputs": [],
   "source": [
    "#获得测试集正负样本对应的house、host索引\n",
    "def get_train_input_index(path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor):\n",
    "    neg_data=pd.read_csv(path)   #负样本特征表，train_neg_comment_path\n",
    "    df_comment=pd.read_csv(comment_path)  #评论表\n",
    "    #读取负样本动态特征，房子房东id，以及对应的正样本的评论id\n",
    "    pos_comment_id=[]\n",
    "    neg_house_comment_features=[]\n",
    "    neg_host_comment_features=[]\n",
    "    neg_house_id=[]\n",
    "    neg_host_id=[]\n",
    "    grouped_neg_data = neg_data.groupby('pos_comment_id')\n",
    "    for name, group in grouped_neg_data:\n",
    "        pos_comment_id.append(name)  #正样本comment_id\n",
    "        neg_house_comment_matrix=group[house_comment_continuous_column].values[:]\n",
    "        neg_host_comment_matrix=group[host_comment_continuous_column].values[:]\n",
    "        \n",
    "        #先填缺失值，再标准化\n",
    "        neg_house_comment_matrix = np.nan_to_num(neg_house_comment_matrix, nan=-2)\n",
    "        neg_host_comment_matrix = np.nan_to_num(neg_host_comment_matrix, nan=-2)\n",
    "        \n",
    "        #标准化\n",
    "        neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        \n",
    "        neg_house_comment_features.append(neg_house_comment_matrix)  #负样本房子评论特征(6,特征数)\n",
    "        neg_host_comment_features.append(neg_host_comment_matrix)#负样本房东评论特征\n",
    "        neg_house_id.append(group['listing_id'].values[:])  #负样本房子id\n",
    "        neg_host_id.append(group['host_id'].values[:])\n",
    "        \n",
    "\n",
    "    \n",
    "    #负样本特征转换成tensor\n",
    "    neg_house_comment_features=torch.tensor(neg_house_comment_features).clone().detach()\n",
    "    neg_host_comment_features=torch.tensor(neg_host_comment_features).clone().detach()\n",
    "    \n",
    "    #根据id，获得负样本房子，房东的index\n",
    "    neg_house_id=np.array(neg_house_id)\n",
    "    neg_host_id=np.array(neg_host_id)\n",
    "    # 使用NumPy的vectorized操作获取所有id对应的索引\n",
    "    input_neg_house_index= np.vectorize(house_index.get)(neg_house_id)\n",
    "    input_neg_host_pic_index= np.vectorize(host_pic_index.get,otypes=[int])(neg_host_id,-1)\n",
    "    input_neg_host_index = np.vectorize(host_index.get, otypes=[int])(neg_host_id, -1)\n",
    "    \n",
    "    #根据正样本的评论id，获得房子，房东。comment。reviewer的index\n",
    "    #获取正样本的评论id对应的index\n",
    "    input_pos_comment_index=np.vectorize(comment_index.get)(pos_comment_id)\n",
    "    #获得房子，房东。reviewer的id\n",
    "    input_pos_house_id=comment_house[input_pos_comment_index]\n",
    "    input_pos_host_id=comment_host[input_pos_comment_index]\n",
    "    input_reviewer_id=df_comment['reviewer_id'][input_pos_comment_index].to_numpy()\n",
    "    #获得房子，房东。reviewer的index\n",
    "    input_pos_house_index=np.vectorize(house_index.get)(input_pos_house_id)\n",
    "    input_pos_host_index=np.vectorize(host_index.get)(input_pos_host_id)\n",
    "    input_pos_host_pic_index=np.vectorize(host_pic_index.get,otypes=[int])(input_pos_host_id,-1)\n",
    "    input_reviewer_index=np.vectorize(user_index.get)(input_reviewer_id)\n",
    "\n",
    "    #获得正样本的动态特征\n",
    "    pos_house_comment_features =house_comment_tensor[input_pos_comment_index]\n",
    "    pos_host_comment_features =host_comment_tensor[input_pos_comment_index]\n",
    "    \n",
    "    #合并正负样本\n",
    "    #动态特征\n",
    "    house_comment_features = torch.cat((pos_house_comment_features.unsqueeze(1), neg_house_comment_features), dim=1)\n",
    "    host_comment_features = torch.cat((pos_host_comment_features.unsqueeze(1), neg_host_comment_features), dim=1)\n",
    "    #房子，房东的index\n",
    "    input_house_index=np.concatenate((input_pos_house_index[:, np.newaxis], input_neg_house_index), axis=1)\n",
    "    input_host_index=np.concatenate((input_pos_host_index[:, np.newaxis], input_neg_host_index), axis=1)\n",
    "    input_host_pic_index=np.concatenate((input_pos_host_pic_index[:, np.newaxis], input_neg_host_pic_index), axis=1)\n",
    "    \n",
    "    label = np.zeros((len(input_house_index), 1 + npratio))\n",
    "    label[:, 0] = 1\n",
    "    label= torch.tensor(label)\n",
    "    \n",
    "    return  pos_comment_id,input_house_index,input_host_index,input_host_pic_index,input_reviewer_index,label,house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0aa0b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.522599Z",
     "start_time": "2024-09-04T06:31:30.487782Z"
    }
   },
   "outputs": [],
   "source": [
    "#根据house索引，获得house的特征，返回为tensor\n",
    "def get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,input_house_index):\n",
    "    # 样本编号转换成tensor\n",
    "    input_house_index = torch.tensor(input_house_index).clone().detach()\n",
    "    # house\n",
    "    # continuous_features\n",
    "    house_continuous_features = house_continuous_tensor[input_house_index]\n",
    "    # categorical_features1\n",
    "    house_categorical_features1 = house_categorical_tensor1[input_house_index]\n",
    "    # categorical_features2\n",
    "    house_categorical_feature2 = house_categorical_tensor2[input_house_index]\n",
    "    # multi_features\n",
    "    house_multi_features = house_multi_tensor[input_house_index]\n",
    "\n",
    "    return house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2d0d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.637612Z",
     "start_time": "2024-09-04T06:31:30.594919Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,input_host_index, input_host_pic_index):\n",
    "    # host\n",
    "    # continuous_features\n",
    "    host_continuous_features = host_continuous_tensor[input_host_index]\n",
    "    # categorical_features1\n",
    "    host_categorical_features1 = host_categorical_tensor1[input_host_index]\n",
    "    # categorical_features2\n",
    "    host_categorical_features2 = host_categorical_tensor2[input_host_index]\n",
    "    # host_pic_features\n",
    "    host_pic_features = host_pic_continuous_tensor[input_host_pic_index]\n",
    "            \n",
    "    return host_continuous_features,host_categorical_features1,host_categorical_features2,host_pic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32a37f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.742129Z",
     "start_time": "2024-09-04T06:31:30.723267Z"
    }
   },
   "outputs": [],
   "source": [
    "#根据样本中的user编号获得对应house、host编号\n",
    "def get_user_input_index(history_matrix_comment,history_matrix_host,history_matrix_host_pic,history_matrix_house,history_matrix_mask,input_reviewer_index):\n",
    "    input_history_comment = history_matrix_comment[input_reviewer_index]  # shape(comment_num, max_history)\n",
    "    input_history_host = history_matrix_host[input_reviewer_index]\n",
    "    input_history_host_pic = history_matrix_host_pic[input_reviewer_index]\n",
    "    input_history_house = history_matrix_house[input_reviewer_index]\n",
    "    input_history_mask = history_matrix_mask[input_reviewer_index]\n",
    "\n",
    "    return input_history_comment,input_history_host,input_history_host_pic,input_history_house,input_history_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763fc0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:30.834714Z",
     "start_time": "2024-09-04T06:31:30.811532Z"
    }
   },
   "outputs": [],
   "source": [
    "#获取user历史house、host的动态特征（只有正样本）\n",
    "def get_history_comment_input(history_comment_index,house_comment_tensor,host_comment_tensor):\n",
    "    # 样本编号转换成tensor\n",
    "    history_comment_index = torch.tensor(history_comment_index).clone().detach()\n",
    "    house_comment_features=house_comment_tensor[history_comment_index]\n",
    "    host_comment_features=host_comment_tensor[history_comment_index]\n",
    "    return house_comment_features,host_comment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6ffd67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:31.125660Z",
     "start_time": "2024-09-04T06:31:31.102329Z"
    }
   },
   "outputs": [],
   "source": [
    "def creat_scaler(path,column):\n",
    "    data=pd.read_csv(path)\n",
    "    continuous_features=data[column]\n",
    "    # 补缺失值-2\n",
    "    continuous_features= continuous_features.fillna(-2)\n",
    "    continuous_matrix=np.array(continuous_features)\n",
    "    # 创建一个StandardScaler对象\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(continuous_matrix)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcda80de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:32.567235Z",
     "start_time": "2024-09-04T06:31:31.232225Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#dnn层,hidden_units=[[输入1，输出1],[输入2，输出2]]\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden_units, activation, l2_reg, dropout_rate, use_bn):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # 创建一个空的列表用于存储神经网络的层\n",
    "        layers = []\n",
    "\n",
    "        # 遍历每个隐藏层的设置\n",
    "        for hidden_size in hidden_units:\n",
    "            # 添加一个线性层（全连接层），输入维度为 hidden_size[0]，输出维度为 hidden_size[1]\n",
    "            layers.append(nn.Linear(hidden_size[0], hidden_size[1]))\n",
    "\n",
    "            # 如果 use_bn 为 True，则添加批归一化层\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size[1]))\n",
    "\n",
    "            # 添加激活函数层，激活函数为传入的 activation 函数\n",
    "            layers.append(activation)\n",
    "\n",
    "            # 添加 dropout 层，dropout_rate 为 dropout 的比例\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # 将所有层组合成一个顺序的神经网络\n",
    "        self.dnn_network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在前向传播过程中，输入数据 x 经过 dnn_network 这个神经网络模型\n",
    "        return self.dnn_network(x)\n",
    "\n",
    "    #用户建模\n",
    "class DinAttention(nn.Module):\n",
    "    def __init__(self,attetion_dnn_hidden_units):\n",
    "        super(DinAttention, self).__init__()\n",
    "        self.dnn1= DNN(attetion_dnn_hidden_units, nn.Sigmoid(), 0.0, 0.0, False)\n",
    "        self.dnn2= nn.Linear(feature_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, history_matrix, house,mask=None):\n",
    "        query= torch.unsqueeze(house, dim=-2)#(batch,npratio+1,1,200)\n",
    "        keys=torch.unsqueeze(history_matrix, dim=1)  #(batch,1,15,200)\n",
    "        query_len=query.size()[1]  #npratio+1\n",
    "#       keys = keys.repeat(1,query_len,1,1)#(batch,npratio+1,15,200)\n",
    "        keys = keys.expand(-1,query_len ,-1, -1)  #(batch,npration+1,15,200)\n",
    "        keys_len = keys.size()[2]   #15\n",
    "#         querys = query.repeat(1,1, keys_len, 1)#(batch,npratio+1,15,200)\n",
    "        querys = query.expand(-1,-1, keys_len, -1) #(batch,npration+1,15,200)\n",
    "        atten_input = torch.cat([querys, keys, querys - keys, querys * keys], dim=-1)  # (batch,npratio+1,T, 4 * embed_size)\n",
    "        \n",
    "        # 经过三层全连接层\n",
    "        output1 = self.dnn1(atten_input)\n",
    "        output2 = self.dnn2(output1)  # (batch,npratio+1,T, 1)\n",
    "#         outputs = output2.transpose(2, 3)  # (batch,npratio+1, 1,T)\n",
    "#         user_att = outputs.view(-1, keys_len)  # 展平为一维向量（batch，15）\n",
    "        user_att = output2.squeeze(-1) #每个历史记录的权重(batch,npratio+1,T)\n",
    "        \n",
    "        if mask is not None:\n",
    "            #mask(batch,T),扩展成(batch,npratio+1,T)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.repeat(1,query_len,1)\n",
    "            user_att = user_att.masked_fill(mask, float('-inf'))  # 对填充向量进行mask操作(batch,npratio+1,T)\n",
    "        user_att = user_att / (keys.size()[-1] ** 0.5)\n",
    "        user_att = torch.softmax(user_att, dim=-1)  # (batch,npratio+1,T)\n",
    "        weighted_sum = torch.matmul(user_att.unsqueeze(-2), keys).squeeze(-2)  # (batch,npratio+1,200)\n",
    "        return weighted_sum\n",
    "    \n",
    "# 全连接层\n",
    "def dense_layer(in_features, out_features):\n",
    "    # in_features=hidden_size,out_features=1\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features,bias=True),\n",
    "        nn.ReLU())\n",
    "\n",
    "\n",
    "def categorical1_embedding(num_categorical1_features): #输入categorical1的特征数\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical1_embeddings = []\n",
    "    for i in range(num_categorical1_features):\n",
    "        embedding_layer1 = nn.Embedding(3, feature_dim)\n",
    "        categorical1_embeddings.append(embedding_layer1)\n",
    "    return categorical1_embeddings\n",
    "\n",
    "def categorical2_embedding(num_categorical2_features,num_categories_list2): #输入categorical2的特征数，各个特征的特征值数量的列表\n",
    "    # 创建一个列表来存储每个嵌入层\n",
    "    categorical2_embeddings = []\n",
    "    # 循环遍历每个特征，为每个特征创建嵌入层并添加到列表中\n",
    "    for i in range(num_categorical2_features):\n",
    "        categorie2_embedding_dim = int(math.log2(num_categories_list2[i]+1))  # 嵌入维度为类别数的底数为2的对数\n",
    "        embedding_layer2 = nn.Embedding(num_categories_list2[i]+1, feature_dim)\n",
    "        categorical2_embeddings.append(embedding_layer2)\n",
    "    return categorical2_embeddings\n",
    "\n",
    "def continuous_embedding(num_continuous_features):\n",
    "    continuous_embedding_layers = []\n",
    "    for i in range(num_continuous_features):\n",
    "        embedding_layer = dense_layer(1, feature_dim)\n",
    "        continuous_embedding_layers.append(embedding_layer)\n",
    "    return continuous_embedding_layers\n",
    "\n",
    "\n",
    "    \n",
    "class Dice(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dice, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.zeros((1,)))\n",
    "        self.epsilon = 1e-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        norm_x = (x - x.mean(dim=0)) / torch.sqrt(x.var(dim=0) + self.epsilon)\n",
    "        p = torch.sigmoid(norm_x)\n",
    "        x = self.alpha * x.mul(1-p) + x.mul(p)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728460d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:32.589110Z",
     "start_time": "2024-09-04T06:31:32.576592Z"
    }
   },
   "outputs": [],
   "source": [
    "class HouseEmbedding(nn.Module):\n",
    "    def __init__(self,num_house_categorical1_features,num_house_categorical2_features,num_house_categories_list2,\n",
    "                 num_house_multi_features,multi_embedding_dim,num_host_categorical1_features,num_host_categorical2_features,\n",
    "                 num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                 num_host_pic_continuous_features,num_house_comment_features,feature_dim):\n",
    "        super(HouseEmbedding, self).__init__()\n",
    "        self.multi_embedding_dim=multi_embedding_dim\n",
    "        self.num_house_multi_features=num_house_multi_features\n",
    "        self.sum_num_features=num_house_categorical1_features+num_house_categorical2_features+num_house_continuous_features+num_house_multi_features+num_house_comment_features+num_host_categorical1_features+num_host_categorical2_features+num_host_continuous_features+num_host_pic_continuous_features\n",
    "        # house_categorical1\n",
    "        self.house_categorical1_embeddings = categorical1_embedding(num_house_categorical1_features)\n",
    "        #house_categorical2\n",
    "        self.house_categorical2_embeddings = categorical2_embedding(num_house_categorical2_features,num_house_categories_list2)\n",
    "        #house_continuous\n",
    "        self.house_continuous_embedding_layer = continuous_embedding(num_house_continuous_features)\n",
    "        #house_multi\n",
    "        self.multi_embedding_layer = categorical1_embedding(num_house_multi_features)\n",
    "        #house_comment\n",
    "        self.house_comment_embedding_layer = continuous_embedding(num_house_comment_features)\n",
    "\n",
    "        # host_categorical1\n",
    "        self.host_categorical1_embeddings = categorical1_embedding(num_host_categorical1_features)\n",
    "        # host_categorical2\n",
    "        self.host_categorical2_embeddings = categorical2_embedding(num_host_categorical2_features,num_host_categories_list2)\n",
    "        #host_continuous_features\n",
    "        self.host_continuous_embedding_layer = continuous_embedding(num_host_continuous_features)\n",
    "        #host_pic_features\n",
    "        self.host_pic_embedding_layer = continuous_embedding(num_host_pic_continuous_features)\n",
    "        \n",
    "        # MLP256-11\n",
    "        self.house_dense_layer = dense_layer(self.sum_num_features*feature_dim,feature_dim)\n",
    "\n",
    "    def forward(self, house_continuous_features,house_categorical_features1,house_categorical_feature2,house_multi_features,house_comment_features,host_continuous_features, host_categorical_features1, host_categorical_features2,host_comment_features, host_pic_features):\n",
    "        # house\n",
    "        # house_continuous_features(样本数, 2, 14)\n",
    "        # house_multi_features(样本数, 2, 42->18),(样本数, 15, 42->18)\n",
    "        # 对多值特征进行嵌入\n",
    "        house_multi_embeddings = torch.cat(\n",
    "            [embedding_layer(house_multi_features[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.multi_embedding_layer)], dim=-1)\n",
    "        # house_comment_features（样本数, 2, 18）\n",
    "        # house_categories_embeddings1,shape(样本数，2,2(2*1))\n",
    "        house_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(house_categorical_features1[:,:, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical1_embeddings)], dim=-1)\n",
    "        # house_categories_embeddings2，shape(样本数，2,(52+4)->embedding维度之和=7(5+2))\n",
    "        house_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(house_categorical_feature2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.house_categorical2_embeddings)], dim=-1)\n",
    "        #连续值embedding(样本数，2,特征数,200)\n",
    "        house_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(house_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_continuous_embedding_layer)], dim=-1)\n",
    "        # house_comment_features（样本数, 2, 19,200）\n",
    "        house_comment_embeddings = torch.cat(\n",
    "            [embedding_layer(house_comment_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.house_comment_embedding_layer)], dim=-1)\n",
    "        \n",
    "        # concate（样本数, 2,61）\n",
    "        house_con = torch.cat(\n",
    "            [house_continuous_embeddings, house_multi_embeddings, house_comment_embeddings, house_categories_embeddings1,\n",
    "             house_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        # host_categories_embeddings1，shape(样本数，2,10(2*5))\n",
    "        host_categories_embeddings1 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features1[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical1_embeddings)], dim=-1)\n",
    "        #host_categories_embeddings2，shape(样本数，2,5->2(2*1))\n",
    "        host_categories_embeddings2 = torch.cat(\n",
    "            [embedding_layer(host_categorical_features2[:, :, i]) for i, embedding_layer in\n",
    "             enumerate(self.host_categorical2_embeddings)], dim=-1)\n",
    "        #host_continuous_features\n",
    "        host_continuous_embeddings = torch.cat(\n",
    "            [embedding_layer(host_continuous_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_continuous_embedding_layer)], dim=-1)\n",
    "        #host_pic_features\n",
    "        host_pic_embeddings = torch.cat(\n",
    "            [embedding_layer(host_pic_features[:,:, i].unsqueeze(2).float()) for i, embedding_layer in\n",
    "             enumerate(self.host_pic_embedding_layer)], dim=-1)        \n",
    "    \n",
    "        # concate(样本数，2,195)-host_comment_features(11)=184\n",
    "        host_con = torch.cat(\n",
    "            [host_continuous_embeddings,host_pic_embeddings,host_categories_embeddings1,host_categories_embeddings2], dim=-1)\n",
    "        \n",
    "        all_con=torch.cat(\n",
    "            [house_con,host_con], dim=-1)\n",
    "        \n",
    "        house_vec = self.house_dense_layer(all_con.float())  # shape(样本数, 2,200)\n",
    "        return house_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb3b58e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:32.632124Z",
     "start_time": "2024-09-04T06:31:32.594567Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, input_units, out_units, max_len, k_max, iteration_times=3, init_std=1.0):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.out_units = out_units\n",
    "        self.max_len = max_len\n",
    "        self.k_max = k_max\n",
    "        self.iteration_times = iteration_times\n",
    "        self.init_std = init_std\n",
    "        self.device = device\n",
    "        \n",
    "        #b_ij (batch,k_max,max_history_len)\n",
    "#         self.routing_logits=torch.nn.init.normal_(torch.empty(self.k_max, self.max_len), mean=0.0, std=1.0)\n",
    "        # bilinear_mapping_matrix shape: (feature_dim, feature_dim)\n",
    "        self.bilinear_mapping_matrix = nn.Parameter(torch.randn(input_units, out_units) * init_std)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        routing_logits=torch.nn.init.normal_(torch.empty(self.k_max, self.max_len), mean=0.0, std=1.0)\n",
    "        # behavior_embeddings shape:(batch_size, max_history_len，feature_dim)\n",
    "        # mask shape:(batch_size, max_history_len)\n",
    "        behavior_embeddings, mask = inputs\n",
    "        batch = behavior_embeddings.size(0)\n",
    "        reshape_mask = mask.unsqueeze(1).expand(batch,self.k_max,max_history_len)\n",
    "        routing_logits = routing_logits.detach().to(self.device)\n",
    "        routing_logits = torch.tile(routing_logits, (batch, 1, 1)) #(batch,k_max,max_history_len)\n",
    "        for i in range(self.iteration_times):\n",
    "            reshape_mask = reshape_mask.bool()  # 将 mask 张量转换为布尔型张量\n",
    "            #b_ij (batch, 输出胶囊数，初始胶囊数)  (batch_size,  k_max, max_history_len)和(batch_size, k_max, max_history_len)\n",
    "            pad = torch.zeros_like(reshape_mask, dtype=torch.float32)\n",
    "            routing_logits = F.softmax(routing_logits, dim=1)\n",
    "            weight =  torch.where(reshape_mask,pad, routing_logits)\n",
    "            # e_i* S behavior_embeddings (batch_size, max_history_len，feature_dim)\n",
    "            # S bilinear_mapping_matrix shape: (feature_dim, feature_dim)\n",
    "            # behavior_embedding_mapping (batch_size, max_history_len，feature_dim)*(feature_dim, feature_dim)=\n",
    "            #                          (batch_size, max_history_len，feature_dim)\n",
    "            behavior_embedding_mapping = torch.matmul(behavior_embeddings, self.bilinear_mapping_matrix)\n",
    "            # 候选路由\n",
    "            # 执行批量矩阵乘法 (batch_size, k_max, max_len)*(batch_size, max_len， feature_dim) = (batch_size, k_max, feature_dim)\n",
    "            Z = torch.matmul(weight, behavior_embedding_mapping)\n",
    "            # squash操作 shape:(batch_size,  k_max, feature_dim)\n",
    "            interest_capsules = squash(Z)\n",
    "            # 执行矩阵乘法操作\n",
    "            # capsules shape:((batch_size,  k_max, feature_dim)*(batch_size,feature_dim, max_history_len)=(batch_size, k_max, max_len)\n",
    "            behavior_embedding_mapping_transpose = torch.transpose(behavior_embedding_mapping,1,2)\n",
    "            #matmul_result shape:(batch_size, k_max, max_len)\n",
    "            matmul_result = torch.matmul(interest_capsules, behavior_embedding_mapping_transpose)\n",
    "            # 沿着指定维度求和\n",
    "            #delta_routing_logits = torch.sum(matmul_result, dim=0, keepdim=True)  # 沿着第一个维度求和\n",
    "            #print(matmul_result.shape)\n",
    "            routing_logits = routing_logits + matmul_result\n",
    "            #print(routing_logits.shape)\n",
    "            # 使用 torch.add 方法创建新的张量保存加法结果 routing_logits = (batch, k_max, max_len)+\n",
    "            #routing_logits = nn.Parameter(routing_logits + delta_routing_logits)\n",
    "        # 输出（batch_size , k_max, feature_dim）\n",
    "        return interest_capsules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def squash(inputs):\n",
    "    inputs = inputs.to(inputs.device)\n",
    "    norm = torch.norm(inputs, dim=-1, keepdim=True)\n",
    "    nSquare = torch.pow(norm, 2)\n",
    "    return (nSquare / ((1 + nSquare) * norm + 1e-9)) * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e99138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:32.633781Z",
     "start_time": "2024-09-04T06:31:32.605482Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# capsule_output shape: (batch_size, k_max, feature_dim)\n",
    "# house_vec shape：(batch_size, npratio+1, feature_dim)\n",
    "class LabelAwareAttention(nn.Module):\n",
    "    def __init__(self, k_max,pow_p = float('inf')):\n",
    "        super(LabelAwareAttention, self).__init__()\n",
    "        self.k_max = k_max\n",
    "        self.pow_p = pow_p\n",
    " \n",
    "    def forward(self, capsule_output, house, hist_len=None):\n",
    "        #(batch_size,feature_dim,npratio+1)\n",
    "        house = house.transpose(1, 2) \n",
    "        # inner shape:(batch_size, k_max, npratio+1)\n",
    "        inner =  torch.matmul(capsule_output,house)\n",
    "        # weight shape:(batch_size, k_max, npratio+1)\n",
    "        weight = torch.pow(inner, self.pow_p) \n",
    "        # weight shape:(batch_size, k_max, npratio+1)\n",
    "        weight = F.softmax(inner, dim=1)\n",
    "        weight = weight.transpose(1, 2) \n",
    "        # (batch_size, npratio+1, k_max)* (batch_size, k_max, feature_dim) = (batch_size, npratio+1, feature_dim)\n",
    "        mul = torch.matmul(weight,capsule_output)\n",
    "        # output shape: (batch_size, npratio+1, feature_dim)\n",
    "        return mul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f7e75c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:33.124052Z",
     "start_time": "2024-09-04T06:31:33.093053Z"
    }
   },
   "outputs": [],
   "source": [
    "class MatchingModel(nn.Module):\n",
    "    def __init__(self, num_house_categorical1_features, num_host_categorical1_features, num_house_categorical2_features,\n",
    "                 num_host_categorical2_features, num_house_multi_features, multi_embedding_dim,\n",
    "                 num_house_categories_list2, num_host_categories_list2, num_house_continuous_features,\n",
    "                 num_host_continuous_features, num_host_pic_continuous_features, num_house_comment_features,\n",
    "                 feature_dim, num_heads, max_history_len,\n",
    "                 num_experts, num_tasks, expert_hidden_units, gate_hidden_units, user_dnn_hidden_units,\n",
    "                 house_dnn_hidden_units, attention_dnn_hidden_units):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.house_embedding = HouseEmbedding(num_house_categorical1_features, num_house_categorical2_features,\n",
    "                                               num_house_categories_list2, num_house_multi_features,\n",
    "                                               multi_embedding_dim, num_host_categorical1_features,\n",
    "                                               num_host_categorical2_features, num_host_categories_list2,\n",
    "                                               num_house_continuous_features, num_host_continuous_features,\n",
    "                                               num_host_pic_continuous_features, num_house_comment_features,\n",
    "                                               feature_dim)\n",
    "        # LabelAwareAttention\n",
    "        self.user_house_encoder = LabelAwareAttention(attetion_dnn_hidden_units)\n",
    "        # Capsule layer\n",
    "        self.capsule_layer = CapsuleLayer(input_units=feature_dim, out_units=feature_dim, max_len=max_history_len,\n",
    "                                           k_max=k_max, iteration_times=3)\n",
    "        \n",
    "        # DNN layers\n",
    "        self.dnn1 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.dnn2 = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, house_continuous_features, house_categorical_features1, house_categorical_feature2,\n",
    "                house_multi_features, house_comment_features,\n",
    "                host_continuous_features, host_categorical_features1, host_categorical_features2,\n",
    "                host_comment_features, host_pic_features,\n",
    "                user_house_continuous_features, user_house_categorical_features1, user_house_categorical_feature2,\n",
    "                user_house_multi_features, user_house_comment_features,\n",
    "                user_host_continuous_features, user_host_categorical_features1, user_host_categorical_features2,\n",
    "                user_host_comment_features, user_host_pic_features,\n",
    "                mask):\n",
    "\n",
    "        # House embedding (batch_size, npratio+1, feature_dim)候选house的embedding\n",
    "        house_vec = self.house_embedding(house_continuous_features, house_categorical_features1,\n",
    "                                          house_categorical_feature2, house_multi_features, house_comment_features,\n",
    "                                          host_continuous_features, host_categorical_features1,\n",
    "                                          host_categorical_features2, host_comment_features, host_pic_features)\n",
    "        # User-house embedding (batch_size, max_history_len, feature_dim)#user历史house的embedding第二维度不是max_history_len吗\n",
    "        user_house_emb = self.house_embedding(user_house_continuous_features, user_house_categorical_features1,\n",
    "                                              user_house_categorical_feature2, user_house_multi_features,\n",
    "                                              user_house_comment_features, user_host_continuous_features,\n",
    "                                              user_host_categorical_features1, user_host_categorical_features2,\n",
    "                                              user_host_comment_features, user_host_pic_features)\n",
    "\n",
    "        # mask shape(batch_size, max_history_len)\n",
    "        # input(batch_size, max_history_len, feature_dim),(batch_size, max_history_len))\n",
    "        # output shape(batch_size, k_max, feature_dim)\n",
    "        capsule_output = self.capsule_layer((user_house_emb, mask))\n",
    "        # ReLU layers\n",
    "        # shape(batch_size, k_max, feature_dim)\n",
    "        capsule_output = self.dnn2(torch.relu(self.dnn1(capsule_output)))\n",
    "        # capsule_output shape: (batch_size, k_max, feature_dim)\n",
    "        # house_vec shape：(batch_size, npratio+1, feature_dim)\n",
    "        # attention out_put shape: (npratio+1, batch_size, k_max, feature_dim)\n",
    "        user_house_vec=self.user_house_encoder(capsule_output,house_vec)  #(batch_size, npratio+1, feature_dim)\n",
    "        \n",
    " \n",
    "    \n",
    "        #匹配分数(batch_size, npratio+1, 1,feature_dim)*(batch,正负样本，feature_dim,1)=(batch,正负样本,1,1)\n",
    "        #(batch_size, npratio+1, feature_dim)*(batch_size, npratio+1, feature_dim)\n",
    "        score=torch.matmul(house_vec.unsqueeze(-2), user_house_vec.unsqueeze(-1))\n",
    "        score=score.squeeze(-1).squeeze(-1)\n",
    "        #(batch,正负样本)\n",
    "#         score = torch.softmax(score, dim=-1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca1ba231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:33.593859Z",
     "start_time": "2024-09-04T06:31:33.562696Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, positive_scores, negative_scores):\n",
    "        # 计算正样本和负样本之间的分数差异\n",
    "        scores_diff = positive_scores - negative_scores\n",
    "        loss = -torch.log(torch.sigmoid(scores_diff))\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6776503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:33.981224Z",
     "start_time": "2024-09-04T06:31:33.971927Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_training(model,train_loader,val_loader,\n",
    "                   rec_criterion,optimizer,EPOCH,device):\n",
    "    # 定义早停策略的参数\n",
    "    best_val_loss = float('inf')  # 初始化最佳验证损失为正无穷\n",
    "    patience = 3  # 容忍多少个epoch没有验证性能提升\n",
    "    early_stopping_counter = 0  # 初始化计数器\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        total_classfier_loss = 0.0\n",
    "        total_rec_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        total_regu_loss = 0.0\n",
    "        total_regu_loss1 = 0.0\n",
    "        total_regu_loss2 = 0.0\n",
    "        total_regu_loss3 = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = [data.to(device) for data in batch]\n",
    "            house_continuous_features, house_categorical_features1, house_categorical_feature2, house_multi_features, house_comment_features, \\\n",
    "            host_continuous_features, host_categorical_features1, host_categorical_features2, host_comment_features, host_pic_features, \\\n",
    "            user_house_continuous_features, user_house_categorical_features1, user_house_categorical_feature2, user_house_multi_features, user_house_comment_features, \\\n",
    "            user_host_continuous_features, user_host_categorical_features1, user_host_categorical_features2, user_host_comment_features, user_host_pic_features, \\\n",
    "            label, mask = batch\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            sum_scores = model(house_continuous_features,\n",
    "                                                                   house_categorical_features1,\n",
    "                                                                   house_categorical_feature2, house_multi_features,\n",
    "                                                                   house_comment_features,\n",
    "                                                                   host_continuous_features, host_categorical_features1,\n",
    "                                                                   host_categorical_features2, host_comment_features,\n",
    "                                                                   host_pic_features,\n",
    "                                                                   user_house_continuous_features,\n",
    "                                                                   user_house_categorical_features1,\n",
    "                                                                   user_house_categorical_feature2,\n",
    "                                                                   user_house_multi_features,\n",
    "                                                                   user_house_comment_features,\n",
    "                                                                   user_host_continuous_features,\n",
    "                                                                   user_host_categorical_features1,\n",
    "                                                                   user_host_categorical_features2,\n",
    "                                                                   user_host_comment_features, user_host_pic_features\n",
    "                                                                   , mask)\n",
    "            \n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            positive_scores = sum_scores[:, 0].unsqueeze(-1)\n",
    "            negative_scores = sum_scores[:, 1:]\n",
    "            rec_loss =  torch.exp(positive_scores ) / (\n",
    "                    torch.sum(torch.exp(negative_scores), dim=1, keepdim=True) + torch.exp(positive_scores))\n",
    "            rec_loss = torch.mean(-torch.log(rec_loss))\n",
    "            loss = rec_loss \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #计算平均loss\n",
    "\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "\n",
    "        average_loss=total_loss/len(train_loader)\n",
    "        \n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"Epoch {epoch + 1}loss:{average_loss}\")\n",
    "            \n",
    "            # 验证集评估\n",
    "            model.eval()  # 将模型切换为评估模式\n",
    "            with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "                total_val_loss = 0.0\n",
    "                total_val_auc = 0.0\n",
    "                total_regu_loss_val=0.0\n",
    "                total_rec_loss_val = 0.0\n",
    "                for batch_val in val_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "                    batch_val = [data.to(device) for data in batch_val]\n",
    "                    house_continuous_features_val, house_categorical_features1_val, house_categorical_feature2_val, house_multi_features_val, house_comment_features_val, \\\n",
    "                    host_continuous_features_val, host_categorical_features1_val, host_categorical_features2_val, host_comment_features_val, host_pic_features_val, \\\n",
    "                    user_house_continuous_features_val, user_house_categorical_features1_val, user_house_categorical_feature2_val, user_house_multi_features_val, user_house_comment_features_val, \\\n",
    "                    user_host_continuous_features_val, user_host_categorical_features1_val, user_host_categorical_features2_val, user_host_comment_features_val, user_host_pic_features_val, \\\n",
    "                    label_val, mask_val = batch_val\n",
    "                    #在验证集上进行前向传播\n",
    "                    sum_scores_val = model(house_continuous_features_val, house_categorical_features1_val,\n",
    "                                                    house_categorical_feature2_val, house_multi_features_val,\n",
    "                                                    house_comment_features_val, host_continuous_features_val,\n",
    "                                                    host_categorical_features1_val, host_categorical_features2_val,\n",
    "                                                    host_comment_features_val, host_pic_features_val,\n",
    "                                                    user_house_continuous_features_val,\n",
    "                                                    user_house_categorical_features1_val,\n",
    "                                                    user_house_categorical_feature2_val, user_house_multi_features_val,\n",
    "                                                    user_house_comment_features_val, user_host_continuous_features_val,\n",
    "                                                    user_host_categorical_features1_val, user_host_categorical_features2_val,\n",
    "                                                    user_host_comment_features_val, user_host_pic_features_val,\n",
    "                                                    mask_val)\n",
    "\n",
    "                    \n",
    "                    #推荐损失\n",
    "                    positive_scores_val = sum_scores_val[:, 0].unsqueeze(-1)\n",
    "                    negative_scores_val = sum_scores_val[:, 1:]\n",
    "                    rec_loss_val =  torch.exp(positive_scores_val ) / (\n",
    "                            torch.sum(torch.exp(negative_scores_val), dim=1, keepdim=True) + torch.exp(negative_scores_val))\n",
    "                    rec_loss_val = torch.mean(-torch.log(rec_loss_val))\n",
    "                    #总损失\n",
    "                    val_loss = rec_loss_val \n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    # 计算验证集上的AUC\n",
    "                    predictions_val = (positive_scores_val > negative_scores_val).float()\n",
    "                    correct_val = predictions_val.sum().item()\n",
    "                    total_val_auc += correct_val / (len(predictions_val)*npratio)\n",
    "                    \n",
    "                average_val_loss = total_val_loss / len(val_loader)\n",
    "                average_auc_val=total_val_auc/ len(val_loader)\n",
    "                average_regu_loss_val=total_regu_loss_val/ len(val_loader)\n",
    "                average_rec_loss_val=total_rec_loss_val/ len(val_loader)\n",
    "                print(f\"Validation Loss: {average_val_loss},AUC: {average_auc_val}\")\n",
    "                if average_val_loss<best_val_loss:\n",
    "                    best_val_loss = average_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"早停策略触发，停止训练在第 {epoch} 个epoch.\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55dccb0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:34.465446Z",
     "start_time": "2024-09-04T06:31:34.445818Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_centered_distance(X):\n",
    "    # 计算每个样本的平方模长\n",
    "    r = torch.sum(X ** 2, dim=1, keepdim=True)\n",
    "\n",
    "    # 计算欧氏距离矩阵\n",
    "    D = torch.sqrt(torch.clamp(r - 2 * torch.mm(X, X.t()) + r.t(), min=0.0) + 1e-8)\n",
    "\n",
    "    # 计算中心化距离矩阵\n",
    "    D -= torch.mean(D, dim=0, keepdim=True)\n",
    "    D -= torch.mean(D, dim=1, keepdim=True)\n",
    "    D += torch.mean(D)\n",
    "    return D\n",
    "def create_distance_covariance(D1, D2):\n",
    "    # 计算 D1 和 D2 之间的距离协方差\n",
    "    n_samples = float(D1.size(0))\n",
    "    dcov = torch.sqrt(torch.clamp(torch.sum(D1 * D2) / (n_samples * n_samples), min=0.0) + 1e-8)\n",
    "    return dcov\n",
    "def create_distance_correlation( X1, X2):\n",
    "    X1=X1.reshape(-1,feature_dim)\n",
    "    X2=X2.reshape(-1,feature_dim)\n",
    "    D1 = create_centered_distance(X1)\n",
    "    D2 = create_centered_distance(X2)\n",
    "\n",
    "    dcov_12 = create_distance_covariance(D1, D2)\n",
    "    dcov_11 = create_distance_covariance(D1, D1)\n",
    "    dcov_22 = create_distance_covariance(D2, D2)\n",
    "\n",
    "    dcor = dcov_12 / (torch.sqrt(torch.clamp(dcov_11 * dcov_22, min=0.0)) + 1e-10)\n",
    "    return dcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0011a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:34.960736Z",
     "start_time": "2024-09-04T06:31:34.911394Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        total_test_loss = 0.0\n",
    "        total_rec_loss = 0.0\n",
    "        total_regu_loss_test=0.0\n",
    "        total_test_auc=0.0\n",
    "        total_distance_correlation=0.0\n",
    "        results = []  # 用于保存结果的列表\n",
    "        for batch_test in test_loader:  # 假设你有一个名为 val_loader 的验证集数据加载器\n",
    "            batch_test = [data.to(device) for data in batch_test]\n",
    "            pos_comment_id,house,host,reviewer,house_continuous_features_test, house_categorical_features1_test, house_categorical_feature2_test, house_multi_features_test, house_comment_features_test, \\\n",
    "            host_continuous_features_test, host_categorical_features1_test, host_categorical_features2_test, host_comment_features_test, host_pic_features_test, \\\n",
    "            user_house_continuous_features_test, user_house_categorical_features1_test, user_house_categorical_feature2_test, user_house_multi_features_test, user_house_comment_features_test, \\\n",
    "            user_host_continuous_features_test, user_host_categorical_features1_test, user_host_categorical_features2_test, user_host_comment_features_test, user_host_pic_features_test, \\\n",
    "            label_test, mask_test = batch_test\n",
    "                    \n",
    "            sum_scores_test= model(house_continuous_features_test, house_categorical_features1_test,\n",
    "                                                                                                                            house_categorical_feature2_test, house_multi_features_test, \n",
    "                                                                                                                            house_comment_features_test, host_continuous_features_test, \n",
    "                                                                                                                            host_categorical_features1_test, host_categorical_features2_test, \n",
    "                                                                                                                            host_comment_features_test, host_pic_features_test,                                                                                           \n",
    "                                                                                                                            user_house_continuous_features_test, user_house_categorical_features1_test, \n",
    "                                                                                                                            user_house_categorical_feature2_test, user_house_multi_features_test,\n",
    "                                                                                                                            user_house_comment_features_test, user_host_continuous_features_test, \n",
    "                                                                                                                            user_host_categorical_features1_test, user_host_categorical_features2_test, \n",
    "                                                                                                                            user_host_comment_features_test, user_host_pic_features_test,mask_test)\n",
    "\n",
    "            \n",
    "            #推荐损失\n",
    "            positive_scores_test = sum_scores_test[:, 0].unsqueeze(-1)\n",
    "            negative_scores_test = sum_scores_test[:, 1:]\n",
    "            rec_loss_test =  torch.exp(positive_scores_test) / (\n",
    "                                        torch.sum(torch.exp(negative_scores_test), dim=1, keepdim=True) + torch.exp(positive_scores_test))\n",
    "            rec_loss_test = torch.mean(-torch.log(rec_loss_test))\n",
    "            #总损失\n",
    "            test_loss = rec_loss_test \n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            \n",
    "            # 计算验证集上的AUC\n",
    "            predictions_test = (positive_scores_test > negative_scores_test).float()\n",
    "            correct_test = predictions_test.sum().item()\n",
    "            total_test_auc += correct_test /  (len(predictions_test)*npratio)\n",
    "\n",
    "\n",
    "        \n",
    "            for i in range(house.shape[0]):\n",
    "                #正样本\n",
    "                results.append({\n",
    "                    'pos_comment_id':pos_comment_id[i],\n",
    "                    'house': house[i][0],\n",
    "                    'host': host[i][0],\n",
    "                    'reviewer': reviewer[i],\n",
    "                    'sum_score': sum_scores_test[i][0],  # 正样本分数\n",
    "\n",
    "                    'label': label_test[i][0]\n",
    "                    \n",
    "                })\n",
    "                for j in range(npratio):\n",
    "                    #负样本\n",
    "                    results.append({\n",
    "                        'pos_comment_id':pos_comment_id[i],\n",
    "                        'house': house[i][1+j],\n",
    "                        'host': host[i][1+j],\n",
    "                        'reviewer': reviewer[i],             \n",
    "                        'sum_score': sum_scores_test[i][1+j],  # 负样本分数\n",
    "\n",
    "                        'label': label_test[i][1+j]\n",
    "                    })\n",
    "                \n",
    "        average_test_loss = total_test_loss / len(test_loader)\n",
    "        average_auc_test=total_test_auc/ len(test_loader)\n",
    "#         average_distance_correlation=total_distance_correlation/ len(test_loader)\n",
    "        \n",
    "\n",
    "        print(f\"Test Loss: {average_test_loss},AUC: {average_auc_test}\")\n",
    "        \n",
    "#         print(f\"average_distance_correlation: {average_distance_correlation}\")\n",
    "        \n",
    "        \n",
    "        # 将结果列表转换为DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results['pos_comment_id'] = df_results['pos_comment_id'].apply(lambda x: x.item())\n",
    "        df_results['house'] = df_results['house'].apply(lambda x: x.item())\n",
    "        df_results['host'] = df_results['host'].apply(lambda x: x.item())\n",
    "        df_results['reviewer'] = df_results['reviewer'].apply(lambda x: x.item())\n",
    "        df_results['sum_score'] = df_results['sum_score'].apply(lambda x: x.item())\n",
    "\n",
    "        df_results['label'] = df_results['label'].apply(lambda x: x.item())\n",
    "        df_results['item_count'] = df_results.groupby('reviewer')['reviewer'].transform('count')\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "        return df_results,average_test_loss,average_auc_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd5e1874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:35.476183Z",
     "start_time": "2024-09-04T06:31:35.451870Z"
    }
   },
   "outputs": [],
   "source": [
    "#top-n\n",
    "def top_evaluation(df_results,n):\n",
    "    df_results_shuffled = df_results.sample(frac=1).reset_index(drop=True)\n",
    "    # 初始化结果列表\n",
    "    ndcg_list = []\n",
    "    hr_list=[]\n",
    "    \n",
    "    for user_id, user_group in df_results_shuffled.groupby('pos_comment_id'):\n",
    "        user_group = user_group.sort_values(by='sum_score', ascending=False)  # 按分数降序排序\n",
    "        top_items = user_group.head(n)['label'].tolist()  # 选取前n个的标签\n",
    "        true_labels = user_group['label'].tolist()  # 用户所有候选物品的真实标签\n",
    "                \n",
    "        # 计算NDCG\n",
    "        idcg = sorted(true_labels, reverse=True)[:n]  # 理想情况下的DCG\n",
    "        dcg = sum(rel / np.log(i + 2) for i, rel in enumerate(top_items))  # 计算DCG\n",
    "        ndcg = dcg / sum(rel / np.log(i + 2) for i, rel in enumerate(idcg))  # 计算NDCG\n",
    "        ndcg_list.append(ndcg)\n",
    "        # 计算HR@N\n",
    "        hit_rate = 1 if any(label == 1 for label in top_items) else 0  # 若Top N中有真实正样本，HR@N为1，否则为0\n",
    "        hr_list.append(hit_rate)\n",
    "\n",
    "        \n",
    "    # 计算平均值\n",
    "    average_ndcg = np.mean(ndcg_list)\n",
    "    average_hr = np.mean(hr_list)\n",
    "    \n",
    "    return average_ndcg, average_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48dca035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T06:31:46.412268Z",
     "start_time": "2024-09-04T06:31:43.942067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=+1\n",
      "01.cs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n",
      "/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset//feature_img_can_host_distinct_201901_202103.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zx/6cmjrfvx00b1fp30mrv9phkc0000gn/T/ipykernel_89992/4010260434.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0;31m#读取host图片特征\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m         \u001B[0mhost_pic_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhost_pic_continuous_matrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhost_pic_continuous_column\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread_host_pic_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhost_pic_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0;31m#读取正样本house和host动态特征\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zx/6cmjrfvx00b1fp30mrv9phkc0000gn/T/ipykernel_89992/1984448091.py\u001B[0m in \u001B[0;36mread_host_pic_features\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#读取host图片特征\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mread_host_pic_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0;31m# 创建重新编号的字典\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnew_id\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mnew_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mid\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'host_id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0mmemory_map\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m             \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"encoding_errors\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"strict\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m         )\n\u001B[1;32m    231\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/maoerDL/lib/python3.7/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    705\u001B[0m                 \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    706\u001B[0m                 \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 707\u001B[0;31m                 \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    708\u001B[0m             )\n\u001B[1;32m    709\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'dataset//feature_img_can_host_distinct_201901_202103.csv'"
     ]
    }
   ],
   "source": [
    "# 创建一个空的DataFrame来存储结果\n",
    "test_auc_df = pd.DataFrame(columns=['时间窗','实验数', '测试集总损失', 'AUC','NDCG@5','HR@5','NDCG@10','HR@10'])\n",
    "for train_user_history_path,val_user_history_path,test_user_history_path,train_neg_comment_path,val_neg_comment_path,test_neg_comment_path,sclar_path in zip(train_user_history_paths,val_user_history_paths,test_user_history_paths,train_neg_comment_paths,val_neg_comment_paths,test_neg_comment_paths,sclar_paths):\n",
    "    \n",
    "    for n in range(5):\n",
    "        print(f'i=+{n+1}')\n",
    "\n",
    "        # 假设文件名总是有相同的结构，可以根据固定的位置进行切片\n",
    "        start_index = train_user_history_path.find('2018') + 4\n",
    "        end_index = start_index + 5\n",
    "        substring = train_user_history_path[start_index:end_index]\n",
    "        print(substring)\n",
    "\n",
    "        # 设置 CSV 文件路径\n",
    "        csv_path = 'new_result//new_MIND-allfeature.csv'\n",
    "\n",
    "        # 检查文件是否存在，如果存在则不写入表头\n",
    "        try:\n",
    "            with open(csv_path, 'r') as f:\n",
    "                header_exists = True\n",
    "        except FileNotFoundError:\n",
    "            header_exists = False\n",
    "            \n",
    "        house_index,house_continuous_matrix,house_categorical_matrix1,house_categorical_matrix2,house_categorical_encodings,house_multi_matrix,num_house_categories_list2=read_house_host(house_path,house_id_column,'house',house_continuous_column,house_categorical_column1,house_categorical_column2)\n",
    "        host_index,host_continuous_matrix,host_categorical_matrix1,host_categorical_matrix2,host_categorical_encodings,num_host_categories_list2=read_house_host(host_path,host_id_column,'host',host_continuous_column,host_categorical_column1,host_categorical_column2)\n",
    "\n",
    "        #读取host图片特征\n",
    "        host_pic_index, host_pic_continuous_matrix, host_pic_continuous_column = read_host_pic_features(host_pic_path)\n",
    "\n",
    "        #读取正样本house和host动态特征\n",
    "        comment_index,house_comment_matrix,host_comment_matrix,comment_house,comment_host=read_comment_features(comment_path,comment_id_column,house_comment_continuous_column,host_comment_continuous_column)\n",
    "        #获取所有用户历史记录对应的house、host索引\n",
    "        train_user_index, train_history_matrix_comment, train_history_matrix_host, train_history_matrix_host_pic, train_history_matrix_house, train_history_matrix_mask = user_history(train_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        val_user_index, val_history_matrix_comment, val_history_matrix_host, val_history_matrix_host_pic, val_history_matrix_house, val_history_matrix_mask = user_history(val_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "        test_user_index, test_history_matrix_comment, test_history_matrix_host, test_history_matrix_host_pic, test_history_matrix_house, test_history_matrix_mask = user_history(test_user_history_path, comment_house, comment_host, comment_index, house_index, host_index, host_pic_index,max_history_len)\n",
    "\n",
    "\n",
    "        house_scaler=creat_scaler(sclar_path,house_continuous_column)\n",
    "        host_scaler=creat_scaler(sclar_path,host_continuous_column)\n",
    "        house_comment_scaler=creat_scaler(sclar_path,house_comment_continuous_column)\n",
    "        host_comment_scaler=creat_scaler(sclar_path,host_comment_continuous_column)\n",
    "        host_pic_scaler=creat_scaler(sclar_path,host_pic_continuous_column)\n",
    "\n",
    "        #连续特征标准化\n",
    "        house_continuous_matrix = house_scaler.transform(house_continuous_matrix)\n",
    "        host_continuous_matrix = host_scaler.transform(host_continuous_matrix)\n",
    "        house_comment_matrix = house_comment_scaler.transform(house_comment_matrix)\n",
    "    #     neg_house_comment_matrix = house_comment_scaler.transform(neg_house_comment_matrix)\n",
    "        host_comment_matrix = host_comment_scaler.transform(host_comment_matrix)\n",
    "    #     neg_host_comment_matrix = host_comment_scaler.transform(neg_host_comment_matrix)\n",
    "        host_pic_continuous_matrix = host_pic_scaler.transform(host_pic_continuous_matrix)\n",
    "\n",
    "        #特征矩阵转换成tensor\n",
    "        house_continuous_tensor=torch.tensor(house_continuous_matrix).clone().detach()\n",
    "        house_categorical_tensor1=torch.tensor(house_categorical_matrix1).clone().detach()\n",
    "        house_categorical_tensor2=torch.tensor(house_categorical_matrix2).clone().detach()\n",
    "        house_multi_tensor=torch.tensor(house_multi_matrix).clone().detach()\n",
    "        house_comment_tensor=torch.tensor(house_comment_matrix).clone().detach()\n",
    "    #     neg_house_comment_tensor=torch.tensor(neg_house_comment_matrix).clone().detach()\n",
    "\n",
    "        host_continuous_tensor=torch.tensor(host_continuous_matrix).clone().detach()\n",
    "        host_categorical_tensor1=torch.tensor(host_categorical_matrix1).clone().detach()\n",
    "        host_categorical_tensor2=torch.tensor(host_categorical_matrix2).clone().detach()\n",
    "        host_comment_tensor=torch.tensor(host_comment_matrix).clone().detach()\n",
    "    #     neg_host_comment_tensor=torch.tensor(neg_host_comment_matrix).clone().detach()\n",
    "        host_pic_continuous_tensor=torch.tensor(host_pic_continuous_matrix).clone().detach()\n",
    "\n",
    "        #训练集输入特征编号、label.负样本动态特征（标准化后的tensor）\n",
    "        train_pos_comment_id,train_house,train_host,train_host_pic,train_reviewer,train_label,train_house_comment_features,train_host_comment_features=get_train_input_index(train_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,train_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        val_pos_comment_id,val_house,val_host,val_host_pic,val_reviewer,val_label,val_house_comment_features,val_host_comment_features=get_train_input_index(val_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,val_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "        test_pos_comment_id,test_house,test_host,test_host_pic,test_reviewer,test_label,test_house_comment_features,test_host_comment_features=get_train_input_index(test_neg_comment_path,comment_path,npratio,house_comment_scaler,host_comment_scaler,comment_index, house_index, host_index, host_pic_index,test_user_index,comment_house,comment_host,house_comment_tensor,host_comment_tensor)\n",
    "\n",
    "        #获得训练集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_house)\n",
    "        #获得验证集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        val_house_continuous_features,val_house_categorical_features1,val_house_categorical_feature2,val_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_house)\n",
    "        #获得测试集输入的house特征，shape(正样本数，6，特征维度)\n",
    "        test_house_continuous_features,test_house_categorical_features1,test_house_categorical_feature2,test_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_house)\n",
    "\n",
    "\n",
    "        #获得训练集输入的host特征\n",
    "        train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_host,train_host_pic)\n",
    "        #获得验证集输入的host特征\n",
    "        val_host_continuous_features,val_host_categorical_features1,val_host_categorical_features2,val_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_host,val_host_pic)\n",
    "        #获得测试集输入的host特征\n",
    "        test_host_continuous_features,test_host_categorical_features1,test_host_categorical_features2,test_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_host,test_host_pic)\n",
    "\n",
    "        #user历史记录矩阵转换成tensor\n",
    "        train_history_matrix_comment=torch.tensor(train_history_matrix_comment).detach()\n",
    "        train_history_matrix_mask=torch.tensor(train_history_matrix_mask).detach()\n",
    "        train_history_matrix_host=torch.tensor(train_history_matrix_host).detach()\n",
    "        train_history_matrix_host_pic=torch.tensor(train_history_matrix_host_pic).detach()\n",
    "        train_history_matrix_house=torch.tensor(train_history_matrix_house).detach()\n",
    "\n",
    "        val_history_matrix_comment=torch.tensor(val_history_matrix_comment).detach()\n",
    "        val_history_matrix_mask=torch.tensor(val_history_matrix_mask).detach()\n",
    "        val_history_matrix_host=torch.tensor(val_history_matrix_host).detach()\n",
    "        val_history_matrix_host_pic=torch.tensor(val_history_matrix_host_pic).detach()\n",
    "        val_history_matrix_house=torch.tensor(val_history_matrix_house).detach()\n",
    "\n",
    "        test_history_matrix_comment=torch.tensor(test_history_matrix_comment).detach()\n",
    "        test_history_matrix_mask=torch.tensor(test_history_matrix_mask).detach()\n",
    "        test_history_matrix_host=torch.tensor(test_history_matrix_host).detach()\n",
    "        test_history_matrix_host_pic=torch.tensor(test_history_matrix_host_pic).detach()\n",
    "        test_history_matrix_house=torch.tensor(test_history_matrix_house).detach()\n",
    "\n",
    "        #得到训练集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        train_history_comment,train_history_host,train_history_host_pic,train_history_house,train_history_mask=get_user_input_index(train_history_matrix_comment,train_history_matrix_host,train_history_matrix_host_pic,train_history_matrix_house,train_history_matrix_mask,train_reviewer)\n",
    "        #得到验证集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        val_history_comment,val_history_host,val_history_host_pic,val_history_house,val_history_mask=get_user_input_index(val_history_matrix_comment,val_history_matrix_host,val_history_matrix_host_pic,val_history_matrix_house,val_history_matrix_mask,val_reviewer)\n",
    "        #得到测试集中user历史house、host对应编号，shape（正样本数，最大历史记录数）\n",
    "        test_history_comment,test_history_host,test_history_host_pic,test_history_house,test_history_mask=get_user_input_index(test_history_matrix_comment,test_history_matrix_host,test_history_matrix_host_pic,test_history_matrix_house,test_history_matrix_mask,test_reviewer)\n",
    "\n",
    "        #获得训练集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,train_history_house)\n",
    "        #获得验证集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_house_continuous_features,val_user_house_categorical_features1,val_user_house_categorical_feature2,val_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,val_history_house)\n",
    "        #获得测试集输入user历史记录的house特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_house_continuous_features,test_user_house_categorical_features1,test_user_house_categorical_feature2,test_user_house_multi_features=get_house_input(house_continuous_tensor,house_categorical_tensor1,house_categorical_tensor2,house_multi_tensor,test_history_house)\n",
    "\n",
    "        #获得训练集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,train_history_host,train_history_host_pic)\n",
    "        #获得验证集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        val_user_host_continuous_features,val_user_host_categorical_features1,val_user_host_categorical_features2,val_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,val_history_host,val_history_host_pic)\n",
    "        #获得测试集输入user历史记录的host特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        test_user_host_continuous_features,test_user_host_categorical_features1,test_user_host_categorical_features2,test_user_host_pic_features=get_host_input(host_continuous_tensor,host_categorical_tensor1,host_categorical_tensor2,host_pic_continuous_tensor,test_history_host,test_history_host_pic)\n",
    "\n",
    "        #获得训练集输入user历史记录的comment特征，shape(正样本数，最大历史记录数，特征维度)\n",
    "        train_user_house_comment_features,train_user_host_comment_features=get_history_comment_input(train_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        val_user_house_comment_features,val_user_host_comment_features=get_history_comment_input(val_history_comment,house_comment_tensor,host_comment_tensor)\n",
    "        test_user_house_comment_features,test_user_host_comment_features=get_history_comment_input(test_history_comment,house_comment_tensor,host_comment_tensor)    \n",
    "\n",
    "\n",
    "        num_house_categorical2_features=house_categorical_matrix2.shape[1]\n",
    "        num_host_categorical2_features=host_categorical_matrix2.shape[1]\n",
    "        num_house_categorical1_features=house_categorical_matrix1.shape[1]\n",
    "        num_host_categorical1_features=host_categorical_matrix1.shape[1]\n",
    "        num_house_multi_features=house_multi_matrix.shape[1]\n",
    "        num_house_continuous_features=house_continuous_matrix.shape[1]\n",
    "        num_host_continuous_features=host_continuous_matrix.shape[1]   \n",
    "        num_house_comment_features=house_comment_matrix.shape[1]\n",
    "        num_host_comment_features=host_comment_matrix.shape[1]\n",
    "        num_host_pic_continuous_features=host_pic_continuous_matrix.shape[1]\n",
    "        \n",
    "        train_modals= f'model///new_MIND-allfeature+{substring}+{n}.pth'\n",
    "\n",
    "        #训练集\n",
    "        train_dataset = TensorDataset(train_house_continuous_features,train_house_categorical_features1,train_house_categorical_feature2,train_house_multi_features,train_house_comment_features,\n",
    "            train_host_continuous_features,train_host_categorical_features1,train_host_categorical_features2,train_host_comment_features,train_host_pic_features,\n",
    "            train_user_house_continuous_features,train_user_house_categorical_features1,train_user_house_categorical_feature2,train_user_house_multi_features,train_user_house_comment_features,\n",
    "            train_user_host_continuous_features,train_user_host_categorical_features1,train_user_host_categorical_features2,train_user_host_comment_features,train_user_host_pic_features,\n",
    "            train_label,train_history_mask)\n",
    "        val_dataset = TensorDataset(val_house_continuous_features, val_house_categorical_features1, val_house_categorical_feature2, val_house_multi_features, val_house_comment_features,\n",
    "            val_host_continuous_features, val_host_categorical_features1, val_host_categorical_features2, val_host_comment_features, val_host_pic_features,\n",
    "            val_user_house_continuous_features, val_user_house_categorical_features1, val_user_house_categorical_feature2, val_user_house_multi_features, val_user_house_comment_features,\n",
    "            val_user_host_continuous_features, val_user_host_categorical_features1, val_user_host_categorical_features2, val_user_host_comment_features, val_user_host_pic_features,\n",
    "            val_label, val_history_mask)\n",
    "\n",
    "\n",
    "        # 创建数据加载器\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,drop_last=False)\n",
    "\n",
    "        # 确保您的计算机上有CUDA支持的GPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # 创建大模型的实例 /(embedding+attention)\n",
    "        model = MatchingModel(num_house_categorical1_features,num_host_categorical1_features,num_house_categorical2_features,num_host_categorical2_features,num_house_multi_features,multi_embedding_dim,\n",
    "                     num_house_categories_list2,num_host_categories_list2,num_house_continuous_features,num_host_continuous_features,\n",
    "                     num_host_pic_continuous_features,num_house_comment_features,feature_dim,num_heads,max_history_len,\n",
    "                     num_experts, num_tasks, expert_hidden_units, gate_hidden_units,user_dnn_hidden_units,house_dnn_hidden_units,attetion_dnn_hidden_units)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # 将 CapsuleLayer 中的参数手动移动到 GPU 上\n",
    "\n",
    "        model.capsule_layer.bilinear_mapping_matrix = model.capsule_layer.bilinear_mapping_matrix.to(device)\n",
    "\n",
    "        #进一步处理 列表转移到GPU\n",
    "        for i in range(len(model.house_embedding.house_categorical1_embeddings)):\n",
    "            model.house_embedding.house_categorical1_embeddings[i] = model.house_embedding.house_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_categorical2_embeddings)):\n",
    "            model.house_embedding.house_categorical2_embeddings[i] = model.house_embedding.house_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical1_embeddings)):\n",
    "            model.house_embedding.host_categorical1_embeddings[i] = model.house_embedding.host_categorical1_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_categorical2_embeddings)):\n",
    "            model.house_embedding.host_categorical2_embeddings[i] = model.house_embedding.host_categorical2_embeddings[i].to(device)\n",
    "        for i in range(len(model.house_embedding.multi_embedding_layer)):\n",
    "            model.house_embedding.multi_embedding_layer[i] = model.house_embedding.multi_embedding_layer[i].to(device)\n",
    "\n",
    "        for i in range(len(model.house_embedding.house_continuous_embedding_layer)):\n",
    "            model.house_embedding.house_continuous_embedding_layer[i] = model.house_embedding.house_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.house_comment_embedding_layer)):\n",
    "            model.house_embedding.house_comment_embedding_layer[i] = model.house_embedding.house_comment_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_continuous_embedding_layer)):\n",
    "            model.house_embedding.host_continuous_embedding_layer[i] = model.house_embedding.host_continuous_embedding_layer[i].to(device)\n",
    "        for i in range(len(model.house_embedding.host_pic_embedding_layer)):\n",
    "            model.house_embedding.host_pic_embedding_layer[i] = model.house_embedding.host_pic_embedding_layer[i].to(device)\n",
    "\n",
    "        rec_criterion =torch.nn.BCELoss()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        #训练\n",
    "        model_training(model,train_loader,val_loader,rec_criterion,optimizer,500,device)\n",
    "        #测试\n",
    "        test_reviewer=torch.tensor(test_reviewer).clone().detach()\n",
    "        test_host=torch.tensor(test_host).clone().detach()\n",
    "        test_house=torch.tensor(test_house).clone().detach()\n",
    "        test_pos_comment_id = torch.tensor(test_pos_comment_id).detach()\n",
    "        test_dataset = TensorDataset(test_pos_comment_id,test_house,test_host,test_reviewer,test_house_continuous_features, test_house_categorical_features1, test_house_categorical_feature2, test_house_multi_features, test_house_comment_features,\n",
    "            test_host_continuous_features, test_host_categorical_features1, test_host_categorical_features2, test_host_comment_features, test_host_pic_features,\n",
    "            test_user_house_continuous_features, test_user_house_categorical_features1, test_user_house_categorical_feature2, test_user_house_multi_features, test_user_house_comment_features,\n",
    "            test_user_host_continuous_features, test_user_host_categorical_features1, test_user_host_categorical_features2, test_user_host_comment_features, test_user_host_pic_features,\n",
    "            test_label, test_history_mask)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        df_results,average_test_loss,average_auc_test=test_model(model, test_loader)\n",
    "        #测试的每个样本结果保存到csv\n",
    "    #     file_name = f\"result//result1.0_{i}.csv\"\n",
    "    #     df_results.to_csv(file_name, index=False)\n",
    "    \n",
    "    \n",
    "        top_5_ndcg,top_5_hr=top_evaluation(df_results,5)\n",
    "        top_10_ndcg,top_10_hr=top_evaluation(df_results,10) \n",
    "\n",
    "        new_row = {\n",
    "            '时间窗':substring,\n",
    "            '实验数': n, \n",
    "            '测试集总损失': average_test_loss, \n",
    "            'AUC': average_auc_test, \n",
    "            'NDCG@5': top_5_ndcg, \n",
    "            'HR@5': top_5_hr, \n",
    "            'NDCG@10': top_10_ndcg, \n",
    "            'HR@10': top_10_hr\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        # 将新的行追加到 CSV 文件中\n",
    "        pd.DataFrame([new_row]).to_csv(csv_path, mode='a', index=False, header=not header_exists)\n",
    "        header_exists = True  # 确保后续追加时不再写入表头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
